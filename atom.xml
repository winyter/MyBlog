<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>花不醉的小花园</title>
  
  <subtitle>携一生所爱，看遍世间美好</subtitle>
  <link href="/MyBlog/atom.xml" rel="self"/>
  
  <link href="https://winyter.github.io/MyBlog/"/>
  <updated>2020-05-21T11:48:40.662Z</updated>
  <id>https://winyter.github.io/MyBlog/</id>
  
  <author>
    <name>winyter</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基于 Ambari 的大数据集群安装指导——篇三：Ambari 及基础组件部署</title>
    <link href="https://winyter.github.io/MyBlog/2020/05/21/ambari-install-guides-3-install-ambari-and-base-components/"/>
    <id>https://winyter.github.io/MyBlog/2020/05/21/ambari-install-guides-3-install-ambari-and-base-components/</id>
    <published>2020-05-21T11:37:12.000Z</published>
    <updated>2020-05-21T11:48:40.662Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="1、准备工作"><a href="#1、准备工作" class="headerlink" title="1、准备工作"></a>1、准备工作</h2><hr><h3 id="1-1-Ambari-版本包解析"><a href="#1-1-Ambari-版本包解析" class="headerlink" title="1.1 Ambari 版本包解析"></a>1.1 Ambari 版本包解析</h3><p>Ambari版本包较多，下面会对所有版本包进行简单的说明，本文档以 Ambari 2.7.3 版本为例：</p><ol><li><p>Ambari 安装包<br>即 Ambari 自身的安装包</p><blockquote><p>ambari-2.7.3.0-centos7.tar.gz</p></blockquote></li><li><p>HDP 集群安装包<br>即大数据平台的安装包，包含了各类组件</p><blockquote><p>HDP-2.6.3.0-centos7-rpm.tar.gz<br>HDP-UTILS-1.1.0.21-centos7.tar.gz</p></blockquote></li><li><p>组件包<br>Ambari 支持针对单个组件进行不同的组件版本定制，并非只能使用 HDP 版本包中的组件版本，你只需要上传你想要的组件版本包，并进行一些简单的操作，即可自使用自己定制的版本进行安装，本文档后面会指导如何定制组件版本进行安装，下面列出几个包名称，作为示例：</p><blockquote><p>elasticsearch-6.8.1.tar.gz<br>kafka-1.1.0.tar.gz</p></blockquote></li><li><p>组件服务包<br>在定制组件版本时，我们不仅需要上传组件版本包，部分组件还需要上传组件服务包，用来帮助 Ambari 指定针对该版本的一些配置，本文档后面会指导这些服务包如何使用，下面列出几个服务包名称，作为示例：</p><blockquote><p>ELASTICSEARCH_6.8.1.tar.gz<br>HBASE_1.4.9.tar.gz</p></blockquote></li></ol><hr><h3 id="1-2-准备工作【在-Ambari-Server-节点的-root-用户下执行】"><a href="#1-2-准备工作【在-Ambari-Server-节点的-root-用户下执行】" class="headerlink" title="1.2 准备工作【在 Ambari Server 节点的 root 用户下执行】"></a>1.2 准备工作【在 Ambari Server 节点的 root 用户下执行】</h3><p>将上一节所说的所有安装包上传到 <code>/home/package/ambari</code> 目录下，并建议根据不同种类的安装包分文件夹存放</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">|_ &#x2F;home&#x2F;package&#x2F;ambari</span><br><span class="line">   |_ versionPackage     # 存放 Ambari 和 HDP 版本包，共 3 个包</span><br><span class="line">   |_ dependentPackage   # 存放独立组件版本包，按需上传即可</span><br><span class="line">   |_ servicePackage     # 存放组件服务包，按需上传即可</span><br></pre></td></tr></table></figure><hr><h3 id="1-3-配置-Ambari-Server-本地源【在-Ambari-Server-节点的-root-用户下操作】"><a href="#1-3-配置-Ambari-Server-本地源【在-Ambari-Server-节点的-root-用户下操作】" class="headerlink" title="1.3 配置 Ambari Server 本地源【在 Ambari Server 节点的 root 用户下操作】"></a>1.3 配置 Ambari Server 本地源【在 Ambari Server 节点的 root 用户下操作】</h3><p>创建本地目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# mkdir -p &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari</span><br></pre></td></tr></table></figure><p>拷贝 <code>ambari-2.7.3.0-centos7.tar.gz</code> 到 <code>/var/www/html/ambari</code> 目录下，并解压。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# cp &#x2F;home&#x2F;package&#x2F;ambari&#x2F;versionPackage&#x2F;ambari-2.7.3.0-centos7.tar.gz &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari</span><br><span class="line">[root@node1 ~]# cd &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari</span><br><span class="line">[root@node1 ~]# tar -zxvf ambari-2.7.3.0-centos7.tar.gz</span><br></pre></td></tr></table></figure><p>修改ambari.repo文件，将该文件拷贝到 /etc/yum.repos.d目录下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# cp &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari&#x2F;ambari&#x2F;centos7&#x2F;2.7.3.0-139&#x2F;ambari.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;</span><br></pre></td></tr></table></figure><p>编辑ambari.repo</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# cd &#x2F;etc&#x2F;yum.repos.d&#x2F;</span><br><span class="line">[root@node1 ~]# vim ambari.repo</span><br></pre></td></tr></table></figure><p>参照以下内容修改：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># VERSION_NUMBER&#x3D;2.6.0.0-267</span><br><span class="line">[ambari-2.7.3.0]</span><br><span class="line">name&#x3D;ambari Version - ambari-2.7.3.0</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;172.168.1.1&#x2F;ambari&#x2F;ambari&#x2F;centos7&#x2F;2.7.3.0-139</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;http:&#x2F;&#x2F;172.168.1.1&#x2F;ambari&#x2F;ambari&#x2F;centos7&#x2F;2.7.3.0-139&#x2F;RPM-GPG-KEY&#x2F;RPM-GPG-KEY-Jenkins</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">priority&#x3D;1</span><br></pre></td></tr></table></figure><blockquote><p>ambari.repo 中的 baseurl 与 gpgkey 都需根据实际的路径填写。<br>正常情况下只需修改IP地址即可。</p></blockquote><hr><h3 id="1-4-准备-HDP-集群本地源文件【在-Ambari-Server-节点的-root-用户下操作】"><a href="#1-4-准备-HDP-集群本地源文件【在-Ambari-Server-节点的-root-用户下操作】" class="headerlink" title="1.4 准备 HDP 集群本地源文件【在 Ambari Server 节点的 root 用户下操作】"></a>1.4 准备 HDP 集群本地源文件【在 Ambari Server 节点的 root 用户下操作】</h3><p>创建目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# mkdir -p &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari&#x2F;hdp</span><br></pre></td></tr></table></figure><p>拷贝 <code>HDP-2.6.3.0-centos7-rpm.tar.gz</code>、<code>HDP-UTILS-1.1.0.21-centos7.tar.gz</code> 到 <code>/var/www/html/ambari/hdp</code> 目录下，并解压。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# cp &#x2F;home&#x2F;package&#x2F;ambari&#x2F;versionPackage&#x2F;HDP* &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari&#x2F;hdp</span><br><span class="line">[root@node1 ~]# cd &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari&#x2F;hdp</span><br><span class="line">[root@node1 ~]# tar -zxvf HDP-2.6.3.0-centos7-rpm.tar.gz</span><br><span class="line">[root@node1 ~]# tar -zxvf HDP-UTILS-1.1.0.21-centos7.tar.gz</span><br></pre></td></tr></table></figure><hr><h3 id="1-5-准备组件本地源文件【在-Ambari-Server-节点的-root-用户下操作】"><a href="#1-5-准备组件本地源文件【在-Ambari-Server-节点的-root-用户下操作】" class="headerlink" title="1.5 准备组件本地源文件【在 Ambari Server 节点的 root 用户下操作】"></a>1.5 准备组件本地源文件【在 Ambari Server 节点的 root 用户下操作】</h3><p>创建目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# mkdir -p &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari&#x2F;extend</span><br></pre></td></tr></table></figure><p>根据组件版本清单，拷贝相应版本包，下面举两个例子，作为示例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# cp &#x2F;home&#x2F;package&#x2F;ambari&#x2F;dependentPackage&#x2F;elasticsearch-6.3.0.tar.gz &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari&#x2F;extend</span><br><span class="line">[root@node1 ~]# cp &#x2F;home&#x2F;package&#x2F;ambari&#x2F;dependentPackage&#x2F;kafka-1.1.0.tar.gz &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari&#x2F;extend</span><br></pre></td></tr></table></figure><blockquote><p>一般情况下，不建议单个组件上传多个版本，这通常会导致版本冲突，可能会使你想要的那个组件版本不会在 Ambari 中出现，导致无法安装</p></blockquote><hr><h2 id="2、安装-amp-配置-Ambari-Server"><a href="#2、安装-amp-配置-Ambari-Server" class="headerlink" title="2、安装&amp;配置 Ambari Server"></a>2、安装&amp;配置 Ambari Server</h2><hr><h3 id="2-1-安装-Ambari-Server【在-Ambari-Server-节点的-root-用户下执行】"><a href="#2-1-安装-Ambari-Server【在-Ambari-Server-节点的-root-用户下执行】" class="headerlink" title="2.1 安装 Ambari Server【在 Ambari Server 节点的 root 用户下执行】"></a>2.1 安装 Ambari Server【在 Ambari Server 节点的 root 用户下执行】</h3><p>输入以下命令安装Ambari Server</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# yum install ambari-server</span><br></pre></td></tr></table></figure><p>每次出现以下交互信息时，输入y继续</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# Is this ok [y&#x2F;d&#x2F;N]: y</span><br></pre></td></tr></table></figure><hr><h3 id="2-2-在-MySQL-中新建-Ambari-用户【在-MySQL-主节点的-rhino-用户下执行】"><a href="#2-2-在-MySQL-中新建-Ambari-用户【在-MySQL-主节点的-rhino-用户下执行】" class="headerlink" title="2.2 在 MySQL 中新建 Ambari 用户【在 MySQL 主节点的 rhino 用户下执行】"></a>2.2 在 MySQL 中新建 Ambari 用户【在 MySQL 主节点的 rhino 用户下执行】</h3><blockquote><p>Ambari 默认安装 PostgreSQL 数据库，使用的数据库为 ambari，默认使用的用户名和密码为 ambari/bigdata。<br>本次安装使用之前安装的 MySQL 数据库，数据库所在的节点为 172.168.1.1，端口号 3306。若不想使用默认的 PostgreSQL 数据库，需要在安装 Ambari Server 前安装数据库，并为 Ambari 创建用户和数据库。下面在 rhino 用户的 MySQL 中为 Ambari 创建使用的用户及数据库，Ambari 使用 MySQL 的用户名为 ambari，密码为 xxxxxxxx，数据库名称为 ambari：</p></blockquote><p>登录 MySQL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64</span><br><span class="line">[rhino@node1 ~]$ bin&#x2F;mysql -h0 -urhino -pxxxxxxxx</span><br></pre></td></tr></table></figure><p>执行如下语句：<br>172.168.1.1 表示 Ambari Server 所在节点的 IP，按照实际情况修改</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CREATE USER &#39;ambari&#39;@&#39;%&#39; IDENTIFIED BY &#39;xxxxxxxx&#39;;</span><br><span class="line">GRANT ALL PRIVILEGES ON . TO &#39;ambari&#39;@&#39;%&#39;;</span><br><span class="line">CREATE USER &#39;ambari&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;xxxxxxxx&#39;;</span><br><span class="line">GRANT ALL PRIVILEGES ON . TO &#39;ambari&#39;@&#39;localhost&#39;;</span><br><span class="line">CREATE USER &#39;ambari&#39;@&#39;172.168.1.1&#39; IDENTIFIED BY &#39;xxxxxxxx&#39;;</span><br><span class="line">GRANT ALL PRIVILEGES ON . TO &#39;ambari&#39;@&#39;172.168.1.1&#39;;</span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><hr><h3 id="2-3-拷贝建库脚本到-MySQL-所在节点【在-Ambari-Server-节点的-root-用户下执行】"><a href="#2-3-拷贝建库脚本到-MySQL-所在节点【在-Ambari-Server-节点的-root-用户下执行】" class="headerlink" title="2.3 拷贝建库脚本到 MySQL 所在节点【在 Ambari Server 节点的 root 用户下执行】"></a>2.3 拷贝建库脚本到 MySQL 所在节点【在 Ambari Server 节点的 root 用户下执行】</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1]# cd &#x2F;var&#x2F;lib&#x2F;ambari-server&#x2F;resources</span><br><span class="line">[root@node1]# scp Ambari-DDL-MySQL-CREATE.sql rhino@192.168.50.213:~</span><br></pre></td></tr></table></figure><hr><h3 id="2-4-新建-Ambari-数据库【在-MySQL-主节点的-rhino-用户下执行】"><a href="#2-4-新建-Ambari-数据库【在-MySQL-主节点的-rhino-用户下执行】" class="headerlink" title="2.4 新建 Ambari 数据库【在 MySQL 主节点的 rhino 用户下执行】"></a>2.4 新建 Ambari 数据库【在 MySQL 主节点的 rhino 用户下执行】</h3><p>修改刚才拷贝的文件的用户属组（<strong>在 MySQL 主节点的 root 用户下执行</strong>）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1]# cd &#x2F;home&#x2F;rhino</span><br><span class="line">[root@node1]# chown -R rhino:root Ambari-DDL-MySQL-CREATE.sql</span><br></pre></td></tr></table></figure><p>使用 ambari 用户登录 MySQL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64</span><br><span class="line">[rhino@node1 mysql-5.6.25-linux-x86_64]$ bin&#x2F;mysql -h0 -uambari -pxxxxxxxx</span><br></pre></td></tr></table></figure><p>执行以下命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; CREATE DATABASE ambari;</span><br><span class="line">mysql&gt; USE ambari;</span><br><span class="line">mysql&gt; source &#x2F;home&#x2F;rhino&#x2F;Ambari-DDL-MySQL-CREATE.sql;</span><br></pre></td></tr></table></figure><p>安装完成后可以在metainfo表中查看版本号（版本号2.7.3）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from metainfo;</span><br><span class="line">+--------------------+-----------------------+</span><br><span class="line">|    metainfo_key    |    metainfo_value     |</span><br><span class="line">+--------------------+-----------------------+</span><br><span class="line">|      version       |        2.7.3          |</span><br><span class="line">+--------------------+-----------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line">mysql&gt; exit;</span><br></pre></td></tr></table></figure><hr><h3 id="2-5-拷贝-MySQL-JDBC-驱动包【在-MySQL-主节点的-root-用户下执行】"><a href="#2-5-拷贝-MySQL-JDBC-驱动包【在-MySQL-主节点的-root-用户下执行】" class="headerlink" title="2.5 拷贝 MySQL JDBC 驱动包【在 MySQL 主节点的 root 用户下执行】"></a>2.5 拷贝 MySQL JDBC 驱动包【在 MySQL 主节点的 root 用户下执行】</h3><p>将连接mySQL的JDBC驱动jar包拷贝到/usr/share/java目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# cd &#x2F;home&#x2F;rhino&#x2F;version&#x2F;dependentPackage</span><br><span class="line">[root@node1 ~]# cp mysql-connector-java-5.1.40-bin.jar &#x2F;usr&#x2F;share&#x2F;java</span><br></pre></td></tr></table></figure><hr><h3 id="2-6-配置-Ambari-Server【在-Ambari-Server-节点的-root-用户下执行】"><a href="#2-6-配置-Ambari-Server【在-Ambari-Server-节点的-root-用户下执行】" class="headerlink" title="2.6 配置 Ambari Server【在 Ambari Server 节点的 root 用户下执行】"></a>2.6 配置 Ambari Server【在 Ambari Server 节点的 root 用户下执行】</h3><p>使用以下命令配置Ambari Server</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ambari-server setup</span><br><span class="line"></span><br><span class="line">Using python &#x2F;usr&#x2F;bin&#x2F;python</span><br><span class="line">Setup ambari-server</span><br><span class="line">Checking SELinux…​</span><br><span class="line">SELinux status is &#39;enabled&#39;</span><br><span class="line">SELinux mode is &#39;permissive&#39;</span><br><span class="line">WARNING: SELinux is set to &#39;permissive&#39; mode and temporarily disabled.</span><br><span class="line">OK to continue [y&#x2F;n] (y)?y</span><br></pre></td></tr></table></figure><p>配置过程中，会出现各种提示，最后()中给出的值为默认配置，如果所有配置都按默认配置，只需要按提示进行即可，如果要指定配置，需要按提示给出相应的配置信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Customize user account for ambari-server daemon [y&#x2F;n] (n)? n</span><br></pre></td></tr></table></figure><p>是否使用默认的用户root，选择n使用root用户。  </p><p>继续，提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Checking JDK…​</span><br><span class="line">[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8</span><br><span class="line">[2] Custom JDK</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Enter choice (1): 2</span><br></pre></td></tr></table></figure><p>默认选择为1，如果选择2，则需要手动安装JDK（包括Hadoop集群中所有机器）。这里选择2，出现提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">WARNING:JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.</span><br><span class="line">WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.</span><br><span class="line">Path to JA home: &#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181</span><br><span class="line"></span><br><span class="line">Validating JDK on Ambari Server…​done.</span><br><span class="line">Check JDK version for Ambari Server…​</span><br><span class="line">JDK version found: 8</span><br><span class="line">Minimum JDK version is 8 for Ambari. Skipping to setup different JDK for Ambari Server.</span><br><span class="line">Checking GPL software agreement…​</span><br><span class="line">GPL License for LZO: https:&#x2F;&#x2F;www.gnu.org&#x2F;licenses&#x2F;old-licenses&#x2F;gpl-2.0.en.html</span><br><span class="line">Enable Ambari Server to download and install GPL Licensed LZO packages [y&#x2F;n] (n)?n</span><br></pre></td></tr></table></figure><p>输入有效的 Java Home 路径 /home/rhino/jdk1.8.0_181。继续，出现提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Enter advanced database configuration [y&#x2F;n] (n)?y</span><br></pre></td></tr></table></figure><p>选择y，出现提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Choose one of the following options:</span><br><span class="line">[1] - PostgreSQL (Embedded)</span><br><span class="line">[2] - Oracle</span><br><span class="line">[3] - MySQL &#x2F; MariaDB</span><br><span class="line">[4] - PostgreSQL</span><br><span class="line">[5] - Microsoft SQL Server (Tech Preview)</span><br><span class="line">[6] - SQL Anywhere</span><br><span class="line">[7] - BDB</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Enter choice (1): 3</span><br></pre></td></tr></table></figure><p>选择3，依次输入mysql所在的节点名称，IP地址，ip是mysql安装机器真实的ip，不要配置虚拟ip、mysql端口号、数据库名称、用户名、用户密码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Hostname (localhost): 172.168.1.1</span><br><span class="line">Port (3306): 3306</span><br><span class="line">Database name (ambari): ambari</span><br><span class="line">Username (ambari): ambari</span><br><span class="line">Enter Database Password (bigdata): xxxxxxxx</span><br><span class="line">Enter full path to custom jdbc driver: &#x2F;usr&#x2F;share&#x2F;java&#x2F;mysql-connector-java-5.1.40-bin.jar</span><br></pre></td></tr></table></figure><p>继续，提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WARNING:Before starting Ambari Server, you must run the following DDL against the database to create the schema: &#x2F;var&#x2F;lib&#x2F;ambari-server&#x2F;resources&#x2F;Ambari-DDL-MySQL-CREATE.sql Proceed with configuring remote database connection properties [y&#x2F;n] (y)? y</span><br></pre></td></tr></table></figure><p>按提示输入y执行，继续，最后出现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ambari Server &#39;setup&#39; completed successfully.</span><br></pre></td></tr></table></figure><p>表示配置成功。<br>| |<br>|—|<br>| 如果在配置的过程中被中断，可以再输入ambari-server setup命令重新配置，此时已经完成的配置会以默认值出现，如： |<br>| Configuring database…============================================================= <br>Choose one of the following options: <br>[1] - PostgreSQL (Embedded) <br>[2] - Oracle <br>[3] - MySQL / MariaDB <br>[4] - PostgreSQL <br>[5] - Microsoft SQL Server (Tech Preview) <br>[6] - SQL Anywhere <br>[7] - BDB <br>============================================================= <br>Enter choice (3): 3 Hostname (172.168.1.1):  |<br>|这里配置数据库的hostname，默认值显示的是上一次配置的172.168.1.1。|</p><p>使用以下命令启动Ambari Server</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# ambari-server start</span><br></pre></td></tr></table></figure><p>最后出现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ambari Server &#39;start&#39; completed successfully.</span><br></pre></td></tr></table></figure><p>说明启动成功。</p><p>查看Ambari Server的运行状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# ambari-server status</span><br><span class="line"></span><br><span class="line">Using python &#x2F;usr&#x2F;bin&#x2F;python</span><br><span class="line">Ambari-server status</span><br><span class="line">Ambari Server running</span><br><span class="line">Found Ambari Server PID: 26512 at: &#x2F;var&#x2F;run&#x2F;ambari-server&#x2F;ambari-server.pid</span><br></pre></td></tr></table></figure><hr><h2 id="3、部署-HDP-集群"><a href="#3、部署-HDP-集群" class="headerlink" title="3、部署 HDP 集群"></a>3、部署 HDP 集群</h2><hr><h3 id="3-1-登录Ambari界面"><a href="#3-1-登录Ambari界面" class="headerlink" title="3.1 登录Ambari界面"></a>3.1 登录Ambari界面</h3><p>输入<code>172.168.1.1:8080</code>登录 ambari 界面，默认用户/密码为：<code>admin/admin</code>。<br>点击<code>Launch Install Wizard</code>按钮，进入安装界面<br><img src="http://cdn.winyter.cn/ambari-install-guide_image9.png" alt="image-9.png"></p><p><img src="http://cdn.winyter.cn/ambari-install-guide_image10.png" alt="image-10.png"></p><hr><h3 id="3-2-集群命名"><a href="#3-2-集群命名" class="headerlink" title="3.2 集群命名"></a>3.2 集群命名</h3><p>第一步为要创建的集群设置名称，每个集群的名称应该是唯一的。这里设置为 $rhino$，点击下一步。<br><img src="http://cdn.winyter.cn/ambari-install-guide_image11.png" alt="image-11.png"></p><h3 id="3-3-版本与安装方式"><a href="#3-3-版本与安装方式" class="headerlink" title="3.3 版本与安装方式"></a>3.3 版本与安装方式</h3><p>在联网的情况下，Ambari会列出可使用的版本信息，如果有版本定义文件（Version Definition File），也可以点击Add Version添加自定义版本信息。</p><p><img src="http://cdn.winyter.cn/ambari-install-guide_image12.png" alt="image-12.png"><br>选择Add Version添加自定义版本信息。</p><p><img src="http://cdn.winyter.cn/ambari-install-guide_image13.png" alt="image-13.png"><br>输入<code>http://172.168.1.1/ambari/hdp/HDP/centos7/2.6.3.0-235/HDP-2.6.3.0-235.xml</code></p><p><img src="http://cdn.winyter.cn/ambari-install-guide_image14.png" alt="image-14.png"><br>点击ok，选择stack为2.6版本。</p><p>根据实际使用的操作系统填写URL信息。如果可以联网，可以选择公共源安装，如果不能联网选择本地源安装，需要手动配置本地源的URL。不需要的操作系统信息可以点击Remove删除。点击下一步时会检测填写的URL是否有效。<br>本次安装选择本地源<br>通过浏览器填写hdp的URL，结果如下：<br><img src="http://cdn.winyter.cn/ambari-install-guide_image15.png" alt="image-15.png"></p><p>其中，hdp.repo 的 URL 为 <code>http://172.168.1.1/ambari/hdp/HDP/centos7/2.6.3.0-235/</code>，<br>hdp-util.repo 的 URL 为 <code>http://172.168.1.1/ambari/hdp/HDP-UTILS-1.1.0.21/repos/centos7/</code>。  </p><p>URL 中 172.168.1.1 为安装 ambari-server 的机器 IP，版本号和操作系统可能会有所不同，请根据实际情况填写。</p><hr><h3 id="3-4-安装节点设置"><a href="#3-4-安装节点设置" class="headerlink" title="3.4 安装节点设置"></a>3.4 安装节点设置</h3><p>填写集群的所有节点，提示要输入的是各节点的完全限定域名，没有设置的话填写主机名称：<br><img src="http://cdn.winyter.cn/ambari-install-guide_image16.png" alt="image-16.png"></p><p>填写 ambari server 机器私钥  </p><p>确保 ambari-server 能自动安装 ambari-agent 到各目标机器上，将 ambari-server 机器上生成的私钥在 <code>/root/.ssh/id_rsa</code> 文件中，将 id_rsa 文件的内容全部复制。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# cat &#x2F;root&#x2F;.ssh&#x2F;id_rsa</span><br><span class="line">-----BEGIN RSA PRIVATE KEY-----</span><br><span class="line">MIIEpQIBAAKCAQEApAuiPxOkng83NO2d16feoP21XT8HsGhD+KuS050sDE1OqLlM</span><br><span class="line">Dnst8jxv+Ptm+olxasBlowU6TBoPwAoA+EFaLuva53Tvg1C8emrUSlNZgIsLmC+m</span><br><span class="line">33H2g1z9JkznYRQ6i9ib3RhNj30EHap7nAhNnJT3tFp73ePILGHVbpqMzAka5tFU</span><br><span class="line">xsYzr&#x2F;IJjWEh+0kha+6D9bjqIIzlE3qWMo8Xwshe1CF5Yq0yRzp1gZOjN8XOc27h</span><br><span class="line">P5NJMbaTPUdW+pn5WlnpPZhStQ7aE+zFR6exqZOPGWEBr4YfXaj1dgghFJOB1GYj</span><br><span class="line">mIQAaVbWFksCZuu0FB6d1ulwuR3EdtwVGhuxGQIDAQABAoIBAFjAlpIrzXdaYhL3</span><br><span class="line">r9saTn+pY&#x2F;NEA6P0dTnXkcN9mHQ7ayAryDNZf308J5R8Z7WKoNsRpqtxS54ax4St</span><br><span class="line">pOKrcOBL6I4rHN5d0uskWyCvQZAjKi23MkBXxvhBWhDbsJ88M4Svt3wCBwxnpc9r</span><br><span class="line">Ch9b+qmJiw5&#x2F;md5tu0IP2EpTwXMtZ+lf91g&#x2F;XuDrB6nlvv8a&#x2F;x&#x2F;eC3xj+LWBd3MO</span><br><span class="line">DaMsXOwQhsCzepH81oO4a1djzqP5oHe36syCKS2DTxQcQJb+3WMR4QlGD99dF2jl</span><br><span class="line">SsWMPY8Z+8RRo4mLtR5nkk9TZsbNPyArXYzlfKhQJ8P8oaS3yWdHJkDy61RoT0FL</span><br><span class="line">LjrkK7UCgYEA1wHxGXGPWuYar3xXkVnqx+47mRwk7yeeaeyG9n5SqtRodvo0gUzj</span><br><span class="line">OXuaYLMtVc47xzXw3QRGvZgmEvCvGfnIaCuEqEi6Z9f+pewqHcUUDlptzp5So&#x2F;84</span><br><span class="line">r+KpLAGKgZlXAuG4t4B1KnHvbXbyVk0lz6&#x2F;5caoAOE8g2LjDzggMNXcCgYEAw1JW</span><br><span class="line">4zJWH33wGQsr+BZEkLQULPXetsh76DE3AqyiI+8JH1e34Ny4Ed+rJiv4TE1Mvdp+</span><br><span class="line">8bWQIHxxUEwqk1&#x2F;aNOs66Scf6h3mNrnJi2zlGb5pQowKuI0cQVJ2BiAkMfWk7TsM</span><br><span class="line">G4YiTd7hABcO&#x2F;MvWS463tpx7CnwzDxg4F6nWMe8CgYEArCsK5GPx9kjyL3mzKVpG</span><br><span class="line">Teiv4rANx1ADYzCa9kE7cz35lORQLQXjokEe4rY35FDrv16rTGBDQUjXnC0NGhDR</span><br><span class="line">cNEAPj1WvxbP&#x2F;A97vjD1GXVCHsTayiXyP62R6AIn5hVi&#x2F;pS&#x2F;dHmx2NY5cn9gGMlY</span><br><span class="line">MNHqPiFyYaTDWafCa3Y4SIsCgYEAjpKl9cWic+5bugwblkW62t139LGsVkPVnlF+</span><br><span class="line">VCdrW0t6nzRKdormmbVomr5xylCKefLpwsnDYNM1a2WNlnHbN9GU+OekNiKJDt1i</span><br><span class="line">rrFYMgh5kSfkE359Z3knyaTghs9GChyV4+lvVOQh3Qz31bN8wz5z&#x2F;4oNjkPGiXgm</span><br><span class="line">Sa+hOGUCgYEAykaL1zW3BmbbkNtFx1+rtoq67FKXzNLYo5NBWwDI14zpZnzNXwhi</span><br><span class="line">B7IPJOvV&#x2F;LvlmpVRdjTUv4VJiYjhqjNVUPeQxt50A4GgEKYgHKHYIf5QdhbwA4VE</span><br><span class="line">QWwuUhvLptJ7yW81piz8V6CUCxNzg&#x2F;M1YEqJWgU&#x2F;rV2LCD50btwK82s&#x3D;</span><br><span class="line">-----END RSA PRIVATE KEY-----</span><br></pre></td></tr></table></figure><p><img src="http://cdn.winyter.cn/ambari-install-guide_image17.png" alt="image-17.png"></p><p>点击继续后，由于填写的不是 FQDN，所以出现以下提示，点击 CONTINUE 继续下一步<br><img src="http://cdn.winyter.cn/ambari-install-guide_image18.png" alt="image-18.png"></p><hr><h3 id="3-5-ambari-agent-注册"><a href="#3-5-ambari-agent-注册" class="headerlink" title="3.5 ambari-agent 注册"></a>3.5 ambari-agent 注册</h3><p>根据上一步填写的信息安装 ambari-agent，之后 ambari-agent 向 ambari-server 注册，界面显示每个节点的执行结果，当执行失败时点击 Failed 可以查看详细日志。<br><img src="http://cdn.winyter.cn/ambari-install-guide_image19.png" alt="image-19.png"></p><p><strong>报错处理</strong></p><p>按照日志修改后，点击retry  failed按钮重试。所有机器注册成功后，会检测warning；当检测到warning时，点击查看warning，并需要按照提示手动解决。</p><p><img src="http://cdn.winyter.cn/ambari-install-guide_image20.png" alt="image-20.png"></p><p><img src="http://cdn.winyter.cn/ambari-install-guide_image21.png" alt="image-21.png"></p><ul><li>这个warning提示<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The following processes should not be running，&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_65&#x2F;bin&#x2F;java -D…</span><br></pre></td></tr></table></figure></li><li>解决方法<br>可以通过 <code>pgrep java</code> 查看进程占用情况，并通过 <code>kill -9</code>  进程号（注意不要删了 <code>ambari-server</code> 的 8080 进程）其他的进程都杀掉。</li></ul><hr><h3 id="3-6-查看ambari进程状态【在所有-Ambari-Agent-节点的-root-用户下执行】"><a href="#3-6-查看ambari进程状态【在所有-Ambari-Agent-节点的-root-用户下执行】" class="headerlink" title="3.6 查看ambari进程状态【在所有 Ambari Agent 节点的 root 用户下执行】"></a>3.6 查看ambari进程状态【在所有 Ambari Agent 节点的 root 用户下执行】</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@node2 ~]# ambari-agent status</span><br><span class="line"></span><br><span class="line">Found ambari-agent PID: 8935</span><br><span class="line">ambari-agent running.</span><br><span class="line">Agent PID at: &#x2F;run&#x2F;ambari-agent&#x2F;ambari-agent.pid</span><br><span class="line">Agent out at: &#x2F;var&#x2F;log&#x2F;ambari-agent&#x2F;ambari-agent.out</span><br><span class="line">Agent log at: &#x2F;var&#x2F;log&#x2F;ambari-agent&#x2F;ambari-agent.log</span><br><span class="line"></span><br><span class="line">ambari-server status</span><br><span class="line">Using python &#x2F;usr&#x2F;bin&#x2F;python</span><br><span class="line">Ambari-server status</span><br><span class="line">Ambari Server running</span><br><span class="line">Found Ambari Server PID: 32729 at: &#x2F;var&#x2F;run&#x2F;ambari-server&#x2F;ambari-server.pid</span><br></pre></td></tr></table></figure><hr><h3 id="3-7-安装zookeeper、Ambari-Metrics服务"><a href="#3-7-安装zookeeper、Ambari-Metrics服务" class="headerlink" title="3.7 安装zookeeper、Ambari Metrics服务"></a>3.7 安装zookeeper、Ambari Metrics服务</h3><p>安装 zookeeper，Ambari Metrics。SmartSense 是默认安装的，所以必选。<br>在 Ambari 页面选择 <code>Add Service</code><br><img src="http://cdn.winyter.cn/ambari-install-guide_image22.png" alt="image-22.png"></p><p>选择 zookeeper，Ambari Metrics<br><img src="http://cdn.winyter.cn/ambari-install-guide_image23.png" alt="image-23.png"></p><p>点击下一步，会提示没有安装一些必要的服务——Ambari Metrics 和 SmartSence，点击 <code>PROCEED ANYWAY</code><br><img src="http://cdn.winyter.cn/ambari-install-guide_image24.png" alt="image-24.png"></p><hr><h3 id="3-8-zookeeper服务"><a href="#3-8-zookeeper服务" class="headerlink" title="3.8 zookeeper服务"></a>3.8 zookeeper服务</h3><p>选择需要安装zookeeper服务的主机，其中Metrics Collector所在服务器需要测试一下到所有服务器（包括自己）的无密码访问是否可以，由于存在部分服务器进行首次无密码访问时需要手动输入yes，才能进去，不验证的话，后续Metrics Collector服务会有问题。  </p><p>1.选择需要安装Zookeeper Server的主机。这里选择了三台机器。<br><img src="http://cdn.winyter.cn/ambari-install-guide_image25.png" alt="image-25.png"></p><p>2.选择没有安装zookeeper的服务器安装client。</p><blockquote><p>这里需要注意，由于部分组件需要使用 zookeeper 客户端来进行业务调度，已经部署了 zookeeper 的节点自带 zookeeper 客户端，所以可以不部署 zookeeper 客户端，但其他节点必须都安装上 zookeeper 客户端。</p></blockquote><p><img src="http://cdn.winyter.cn/ambari-install-guide_image26.png" alt="image-26.png"></p><p>3.点击下一步，此处配置默认不修改。<br><img src="http://cdn.winyter.cn/ambari-install-guide_image28.png" alt="image-28.png"></p><hr><h3 id="3-9-输入Grafana密码"><a href="#3-9-输入Grafana密码" class="headerlink" title="3.9 输入Grafana密码"></a>3.9 输入Grafana密码</h3><p>用户名和密码都输入admin<br><img src="http://cdn.winyter.cn/ambari-install-guide_image27.png" alt="image-27.png"></p><p>默认即可，一路NEXT</p><hr><h3 id="3-10-确认安装"><a href="#3-10-确认安装" class="headerlink" title="3.10 确认安装"></a>3.10 确认安装</h3><p>确认安装信息，安装的信息可以直接打印出来：<br><img src="http://cdn.winyter.cn/ambari-install-guide_image29.png" alt="image-29.png"></p><p><img src="http://cdn.winyter.cn/ambari-install-guide_image30.png" alt="image-30.png"></p><p>如果进程启动失败或测试没有通过，则显示 WARNING。</p><blockquote><p>注意：安装完成之后是不能返回到之前的步骤检查配置的。</p></blockquote><p>完成 Hadoop 安装<br><img src="http://cdn.winyter.cn/ambari-install-guide_image31.png" alt="image-31.png"></p><p><img src="http://cdn.winyter.cn/ambari-install-guide_image32.png" alt="image-32.png"></p><p><img src="http://cdn.winyter.cn/ambari-install-guide_image33.png" alt="image-33.png"></p><hr><h3 id="3-11-删除SmartSense服务"><a href="#3-11-删除SmartSense服务" class="headerlink" title="3.11 删除SmartSense服务"></a>3.11 删除SmartSense服务</h3><blockquote><p>SmartSense 为 HortonWorks 提供的一个增值服务，用于收集集群诊断数据，协助支持案例故障排除，除非确实有必有，否则建议卸载该服务。</p></blockquote><p>由于 SmartSense 为默认安装的服务，所以，需要我们手动删除该服务</p><p>首先停止服务<br><img src="http://cdn.winyter.cn/ambari-install-guide_image34.png" alt="image-34.png"></p><p>删除此服务<br><img src="http://cdn.winyter.cn/ambari-install-guide_image35.png" alt="image-35.png"></p><p><img src="http://cdn.winyter.cn/ambari-install-guide_image36.png" alt="image-36.png"></p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h2 id=&quot;1、准备工作&quot;&gt;&lt;a href=&quot;#1、准备工作&quot; class=&quot;headerlink&quot; title=&quot;1、准备工作&quot;&gt;&lt;/a&gt;1、准备工作&lt;/h2&gt;&lt;hr&gt;
&lt;h3 id=&quot;1-1-Ambari-版本包解析&quot;&gt;&lt;a href=&quot;#1-1-Ambari-
      
    
    </summary>
    
    
      <category term="CS Soft" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/"/>
    
      <category term="Server" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"/>
    
      <category term="Ambari" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"/>
    
    
      <category term="Bigdataa" scheme="https://winyter.github.io/MyBlog/tags/Bigdataa/"/>
    
      <category term="Ambari" scheme="https://winyter.github.io/MyBlog/tags/Ambari/"/>
    
  </entry>
  
  <entry>
    <title>Linux命令集锦【持续更新】</title>
    <link href="https://winyter.github.io/MyBlog/2020/04/19/UNIX-commands-collection/"/>
    <id>https://winyter.github.io/MyBlog/2020/04/19/UNIX-commands-collection/</id>
    <published>2020-04-19T06:49:43.000Z</published>
    <updated>2020-04-19T07:44:14.686Z</updated>
    
    <content type="html"><![CDATA[<h2 id="查看CPU信息"><a href="#查看CPU信息" class="headerlink" title="查看CPU信息"></a>查看CPU信息</h2><p>查看CPU型号：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat &#x2F;proc&#x2F;cpuinfo | grep name | cut -f2 -d: | uniq -c</span><br></pre></td></tr></table></figure><p>查看CPU颗数及CPU物理核心数(个数)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat &#x2F;proc&#x2F;cpuinfo</span><br><span class="line">查看最后一个CPU的physical id(从0开始，数出的个数即为CPU颗数),cpu cores显示的即为单颗核心数，总个数为颗数*核心数</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 查看CPU颗数</span><br><span class="line">cat &#x2F;proc&#x2F;cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l</span><br><span class="line"># 查看每个CPU内的核数</span><br><span class="line">cat &#x2F;proc&#x2F;cpuinfo| grep &quot;cpu cores&quot;| uniq</span><br></pre></td></tr></table></figure><p>查看逻辑核数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat &#x2F;proc&#x2F;cpuinfo | grep &quot;processor&quot; | wc -l</span><br></pre></td></tr></table></figure><p>CentOS 7.5修改hostname</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl sethostname &lt;hostname&gt;</span><br></pre></td></tr></table></figure><p>释放内存cache</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sync</span><br><span class="line">echo 3 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;查看CPU信息&quot;&gt;&lt;a href=&quot;#查看CPU信息&quot; class=&quot;headerlink&quot; title=&quot;查看CPU信息&quot;&gt;&lt;/a&gt;查看CPU信息&lt;/h2&gt;&lt;p&gt;查看CPU型号：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;tabl
      
    
    </summary>
    
    
      <category term="CS Soft" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/"/>
    
      <category term="OS" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/OS/"/>
    
      <category term="UNIX" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/OS/UNIX/"/>
    
    
      <category term="Collection" scheme="https://winyter.github.io/MyBlog/tags/Collection/"/>
    
      <category term="UNIX" scheme="https://winyter.github.io/MyBlog/tags/UNIX/"/>
    
      <category term="Linux" scheme="https://winyter.github.io/MyBlog/tags/Linux/"/>
    
      <category term="Commands" scheme="https://winyter.github.io/MyBlog/tags/Commands/"/>
    
  </entry>
  
  <entry>
    <title>MacOS 快捷键记录【持续更新】</title>
    <link href="https://winyter.github.io/MyBlog/2020/04/19/MacOS-hotkey-collection/"/>
    <id>https://winyter.github.io/MyBlog/2020/04/19/MacOS-hotkey-collection/</id>
    <published>2020-04-19T06:45:04.000Z</published>
    <updated>2020-04-19T07:43:06.195Z</updated>
    
    <content type="html"><![CDATA[<ul><li>不同 App 窗口切换：command+tab</li><li>App 内窗口切换：command+~</li><li>App 内标签页切换：control+tab</li><li>输入法快速切换：control+space</li><li>快速显示桌面(之一)：F11</li><li>打开关闭 Dock 的自动隐藏：option+command+D</li><li>拷贝 &amp; 剪切：command+c &amp; command+v / command+option+v</li><li>翻页：fn+▲ / fn+▼</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;不同 App 窗口切换：command+tab&lt;/li&gt;
&lt;li&gt;App 内窗口切换：command+~&lt;/li&gt;
&lt;li&gt;App 内标签页切换：control+tab&lt;/li&gt;
&lt;li&gt;输入法快速切换：control+space&lt;/li&gt;
&lt;li&gt;快速显示桌面
      
    
    </summary>
    
    
      <category term="CS Soft" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/"/>
    
      <category term="OS" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/OS/"/>
    
      <category term="UNIX" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/OS/UNIX/"/>
    
      <category term="MacOS" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/OS/UNIX/MacOS/"/>
    
    
      <category term="MacOS" scheme="https://winyter.github.io/MyBlog/tags/MacOS/"/>
    
      <category term="Hotkey" scheme="https://winyter.github.io/MyBlog/tags/Hotkey/"/>
    
      <category term="Collection" scheme="https://winyter.github.io/MyBlog/tags/Collection/"/>
    
  </entry>
  
  <entry>
    <title>网站配置 DNS 时，各类符号的含义</title>
    <link href="https://winyter.github.io/MyBlog/2020/04/19/dns-config-parameters-introduction/"/>
    <id>https://winyter.github.io/MyBlog/2020/04/19/dns-config-parameters-introduction/</id>
    <published>2020-04-19T05:47:41.000Z</published>
    <updated>2020-04-19T07:45:51.404Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>from <a href="https://www.aliyun.com/" target="_blank" rel="noopener">阿里云</a></p></blockquote><p>主机记录就是域名前缀，常见用法有：<br><strong>www</strong>：解析后的域名为<a href="http://www.aliyun.com。" target="_blank" rel="noopener">www.aliyun.com。</a><br><strong>@</strong>：直接解析主域名 aliyun.com。<br><strong>*</strong>：泛解析，匹配其他所有域名 *.aliyun.com。<br><strong>mail</strong>：将域名解析为mail.aliyun.com，通常用于解析邮箱服务器。<br><strong>二级域名</strong>：如：abc.aliyun.com，填写abc。<br><strong>手机网站</strong>：如：m.aliyun.com，填写m。<br><strong>显性URL</strong>：不支持泛解析（泛解析：将所有子域名解析到同一地址）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;from &lt;a href=&quot;https://www.aliyun.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;阿里云&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;主机记录就是域名前缀，常见用法有：&lt;br&gt;&lt;str
      
    
    </summary>
    
    
      <category term="CS Concept" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/"/>
    
      <category term="Basic" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/Basic/"/>
    
      <category term="Network" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/Basic/Network/"/>
    
    
      <category term="Website" scheme="https://winyter.github.io/MyBlog/tags/Website/"/>
    
      <category term="DNS" scheme="https://winyter.github.io/MyBlog/tags/DNS/"/>
    
  </entry>
  
  <entry>
    <title>靠谱的数据来源和数据统计平台</title>
    <link href="https://winyter.github.io/MyBlog/2020/04/19/data-resource-platform/"/>
    <id>https://winyter.github.io/MyBlog/2020/04/19/data-resource-platform/</id>
    <published>2020-04-19T05:43:59.000Z</published>
    <updated>2020-04-19T07:45:02.744Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>via:【<a href="https://www.zhihu.com/question/27798279" target="_blank" rel="noopener">知乎问题：有哪些好的数据来源或者大数据平台？</a>】</p></blockquote><hr><h2 id="国内"><a href="#国内" class="headerlink" title="国内"></a>国内</h2><h3 id="官方数据"><a href="#官方数据" class="headerlink" title="官方数据"></a>官方数据</h3><ul><li><a href="http://www.stats.gov.cn/" target="_blank" rel="noopener">国家统计局</a>：宏观经济、国计民生</li><li><a href="http://www.cnnic.net.cn/hlwfzyj/" target="_blank" rel="noopener">中国互联网信息中心</a></li><li><a href="http://www.caict.ac.cn/kxyj/qwfb/bps/" target="_blank" rel="noopener">中国信通院</a></li></ul><h3 id="指数"><a href="#指数" class="headerlink" title="指数"></a>指数</h3><ul><li><a href="http://index.baidu.com" target="_blank" rel="noopener">百度指数</a>：搜索关键词风向</li><li><a href="https://index.iresearch.com.cn/new/#/" target="_blank" rel="noopener">艾瑞指数</a>：反映中国互联网整体和移动互联网市场客观情况</li><li><a href="https://index.1688.com/" target="_blank" rel="noopener">阿里指数</a>：淘宝平台风向标</li><li><a href="http://www.endata.com.cn/BoxOffice/" target="_blank" rel="noopener">艺恩</a>：院线电影数据统计</li><li><a href="http://index.bitauto.com/login" target="_blank" rel="noopener">易车指数</a>：国内汽车销售市场</li><li><a href="https://report.amap.com/detail.do?city=110000" target="_blank" rel="noopener">高德交通指数</a>：城市实时交通详情</li><li><a href="https://fdc.fang.com/index/" target="_blank" rel="noopener">房天下房地产指数</a>：房地产交易</li></ul><h3 id="研究报告"><a href="#研究报告" class="headerlink" title="研究报告"></a>研究报告</h3><ul><li><a href="http://www.afenxi.com/" target="_blank" rel="noopener">数据分析网</a>：数据分析行业的媒体网站</li><li><a href="https://www.chinaventure.com.cn/report/list.html" target="_blank" rel="noopener">投中研究院</a>：投资数据</li><li><a href="https://zt.360.cn/report/" target="_blank" rel="noopener">360安全中心研究报告</a>：互联网安全领域</li><li><a href="https://bigdata.qq.com//reports?page=1" target="_blank" rel="noopener">腾讯大数据</a>：基于腾讯产品的数据研究报告</li><li><a href="http://www.aliresearch.com/blog/index/lists/tag/3831.html" target="_blank" rel="noopener">阿里研究院</a>：基于阿里巴巴产品的数据研究报告</li><li><a href="http://www.cbndata.com/report" target="_blank" rel="noopener">第一财经商业数据中心</a></li></ul><hr><h2 id="国外"><a href="#国外" class="headerlink" title="国外"></a>国外</h2><h3 id="Gold-Top50"><a href="#Gold-Top50" class="headerlink" title="Gold Top50"></a>Gold Top50</h3><p>美国管理协会（ AMA）旗下杂志《Marketing News》每年会发布一份Gold Top 50（原为Honomichl Top 50）榜单，列举过去一年美国营收排名前50的市场研究公司。上榜的公司就是非常好的数据来源。  </p><ul><li><a href="https://www.ama.org/" target="_blank" rel="noopener">AMA 官网</a></li><li><a href="https://www.ama.org/marketing-news/the-2018-ama-gold-top-50-report/" target="_blank" rel="noopener">2018 年 Gold Top50 榜单</a></li></ul><h3 id="科技-IT"><a href="#科技-IT" class="headerlink" title="科技 IT"></a>科技 IT</h3><ul><li><a href="https://www.idc.com/" target="_blank" rel="noopener">IDC</a></li><li><a href="https://www.gartner.com/en" target="_blank" rel="noopener">Gartner</a></li></ul><p>两家公司在IT、电信、消费电子、应用软件领域有很深积累，每年都会发布全球市场智能手机、平板电脑、PC出货量。<br>其实除了科技产业，IDC和Gartner还会定期公开能源、健康、制造等的调研数据。进行相关领域研究时，可以将它们的数据作为一项参考。</p><h3 id="股市"><a href="#股市" class="headerlink" title="股市"></a>股市</h3><p>研究上市公司的人员结构、业务构成，财报是常用手段。以一定时间跨度分析一家公司的财报，比单纯看某个季度更有价值。美股财报可以访问纳斯达克、纽交所或SEC的网站获取，港股财报可以访问香港联交所网站获取，A股财报可以访问上交所、深交所或证监会网站获取。</p><h4 id="美股"><a href="#美股" class="headerlink" title="美股"></a>美股</h4><ul><li><a href="https://www.nasdaq.com/" target="_blank" rel="noopener">纳斯达克</a></li><li><a href="https://www.nyse.com/index" target="_blank" rel="noopener">纽交所</a></li><li><a href="https://www.sec.gov/" target="_blank" rel="noopener">SEC</a><h4 id="港股"><a href="#港股" class="headerlink" title="港股"></a>港股</h4></li><li><a href="https://www.hkex.com.hk/?sc_lang=zh-HK" target="_blank" rel="noopener">香港联交所</a><h4 id="A股"><a href="#A股" class="headerlink" title="A股"></a>A股</h4></li><li><a href="http://www.sse.com.cn/" target="_blank" rel="noopener">上海证券交易所</a></li><li><a href="http://www.szse.cn/" target="_blank" rel="noopener">深圳证券交易所</a></li><li><a href="http://www.csrc.gov.cn/pub/newsite/" target="_blank" rel="noopener">证监会</a></li></ul><h3 id="媒体与营销数据来源："><a href="#媒体与营销数据来源：" class="headerlink" title="媒体与营销数据来源："></a>媒体与营销数据来源：</h3><ul><li><a href="http://www.pewresearch.org/" target="_blank" rel="noopener">皮尤</a>：独立民调机构，调查范围覆盖政治、社会趋势、宗教，媒体新闻、科技互联网，调查报告和数据可以免费查看。皮尤具有非常现代化的网页设计，体验好过大多数调研机构网站。</li><li><a href="http://vidstatsx.com/" target="_blank" rel="noopener">VidStatsX</a>：第三方YouTube统计平台，可以提供不同频道的订阅数、排名、视频观看量等数据。VidStatsX数据的时间跨度很大，时效性也很强，可以观察一些爆款视频的数据变化。</li></ul><h3 id="移动应用"><a href="#移动应用" class="headerlink" title="移动应用"></a>移动应用</h3><ul><li><a href="https://www.appannie.com/dashboard/home/" target="_blank" rel="noopener">App Annie</a>：App Annie可以提供一款应用在不同应用商店中的日排名，历史排名以及在不同国家的评级数据。用户也可以查看更详细的下载、收入预估等数据，但这些都需要付费订阅。</li></ul><h2 id="数据统计汇总平台导航网站"><a href="#数据统计汇总平台导航网站" class="headerlink" title="数据统计汇总平台导航网站"></a>数据统计汇总平台导航网站</h2><ul><li><a href="https://link.jiandaoyun.com/f/5b35d05ff7f6ef2604d39a93" target="_blank" rel="noopener">简道云</a></li><li><a href="http://www.199it.com/" target="_blank" rel="noopener">199IT</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;via:【&lt;a href=&quot;https://www.zhihu.com/question/27798279&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;知乎问题：有哪些好的数据来源或者大数据平台？&lt;/a&gt;】&lt;/p&gt;
&lt;/bloc
      
    
    </summary>
    
    
      <category term="Source" scheme="https://winyter.github.io/MyBlog/categories/Source/"/>
    
    
      <category term="Data_Resource" scheme="https://winyter.github.io/MyBlog/tags/Data-Resource/"/>
    
  </entry>
  
  <entry>
    <title>ES新建索引分片失败&quot;shards_acknowledged&quot;：false的解决</title>
    <link href="https://winyter.github.io/MyBlog/2020/04/19/ERRORS-ES-shards-acknowledged-false/"/>
    <id>https://winyter.github.io/MyBlog/2020/04/19/ERRORS-ES-shards-acknowledged-false/</id>
    <published>2020-04-19T05:41:56.000Z</published>
    <updated>2020-04-19T07:40:28.575Z</updated>
    
    <content type="html"><![CDATA[<p>起初在新建索引模板时，始终会报”shards_acknowledged” : false，分片失败。<br>查询分片状态全部是Unassigned。<br>使用curl  11.11.11.11:9200/_cat/nodes查询节点状态只显示一条数据（现场为单机三实例的部署规划），一开始我以为因为是单机版的，所以节点只有一个，于是也没有认为这有异常。<br>包括Head插件的页面也只显示一个节点，一开始我都没觉得这是异常</p><p>直到查看两个数据节点的日志，一直有报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2020-03-23T09:48:14,487][INFO ][o.e.d.z.ZenDiscovery     ] [ziqKzd9] failed to send join request to master [&#123;ziqKzd9&#125;&#123;ziqKzd9NTt63gLyFN0XxeA&#125;&#123;dc7R_7KzTHq795myQzhrNg&#125;&#123;baiyun-yewu&#125;&#123;68.4.173.174:9300&#125;&#123;ml.machine_memory&#x3D;134765240320, ml.max_open_jobs&#x3D;20, xpack.installed&#x3D;true, ml.enabled&#x3D;true&#125;], reason [RemoteTransportException[[ziqKzd9][68.4.173.174:9300][internal:discovery&#x2F;zen&#x2F;join]]; nested: IllegalArgumentException[can&#39;t add node &#123;ziqKzd9&#125;&#123;ziqKzd9NTt63gLyFN0XxeA&#125;&#123;g4dOXxBGQ3KRV1Xc1TznsA&#125;&#123;baiyun-yewu&#125;&#123;68.4.173.174:9301&#125;&#123;ml.machine_memory&#x3D;134765240320, ml.max_open_jobs&#x3D;20, xpack.installed&#x3D;true, ml.enabled&#x3D;true&#125;, found existing node &#123;ziqKzd9&#125;&#123;ziqKzd9NTt63gLyFN0XxeA&#125;&#123;dc7R_7KzTHq795myQzhrNg&#125;&#123;baiyun-yewu&#125;&#123;68.4.173.174:9300&#125;&#123;ml.machine_memory&#x3D;134765240320, xpack.installed&#x3D;true, ml.max_open_jobs&#x3D;20, ml.enabled&#x3D;true&#125; with the same id but is a different node instance]; ]</span><br></pre></td></tr></table></figure><p>查询资料得知是因为数据节点连接不上master节点，原因是因为数据节点的数据存储目录下有数据，顿时恍然大悟，此前es曾删掉重装过，且重新分配过实例和磁盘，所以很可能之前的安装内容没有清空。<br>于是进入数据目录，将目录下的内容清空，重启es即可</p><p>（寻找数据目录的方法：查看es配置项：path_data，将配置值目录下的内容清空即可）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;起初在新建索引模板时，始终会报”shards_acknowledged” : false，分片失败。&lt;br&gt;查询分片状态全部是Unassigned。&lt;br&gt;使用curl  11.11.11.11:9200/_cat/nodes查询节点状态只显示一条数据（现场为单机三实例的部
      
    
    </summary>
    
    
      <category term="CS Concept" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/"/>
    
      <category term="Big Data" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/Big-Data/"/>
    
    
      <category term="ERROR" scheme="https://winyter.github.io/MyBlog/tags/ERROR/"/>
    
      <category term="ES" scheme="https://winyter.github.io/MyBlog/tags/ES/"/>
    
  </entry>
  
  <entry>
    <title>Linux NFS挂载共享文件</title>
    <link href="https://winyter.github.io/MyBlog/2020/04/19/linux-NFS-mount-guide/"/>
    <id>https://winyter.github.io/MyBlog/2020/04/19/linux-NFS-mount-guide/</id>
    <published>2020-04-19T05:35:35.000Z</published>
    <updated>2020-04-19T07:49:25.285Z</updated>
    
    <content type="html"><![CDATA[<p>环境介绍：现有两台服务器：均为CentOS7.5环境，挂载端称为主机，被挂载端称为从机</p><hr><h3 id="1、安装NFS服务所需的软件包【主从机均需执行】"><a href="#1、安装NFS服务所需的软件包【主从机均需执行】" class="headerlink" title="1、安装NFS服务所需的软件包【主从机均需执行】"></a>1、安装NFS服务所需的软件包【主从机均需执行】</h3><p>(如果已经安装nfs服务，可以不执行此步骤)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br></pre></td></tr></table></figure><h3 id="2、编辑exports文件，添加从机【主机执行】"><a href="#2、编辑exports文件，添加从机【主机执行】" class="headerlink" title="2、编辑exports文件，添加从机【主机执行】"></a>2、编辑exports文件，添加从机【主机执行】</h3><p>为挂载目录添加777权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod -R 777 &#x2F;home&#x2F;xxx&#x2F;xxx</span><br></pre></td></tr></table></figure><p>修改exports文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;exports</span><br><span class="line">添加如下内容：</span><br><span class="line">&#x2F;home&#x2F;xxx&#x2F;xxx 111.111.111.0&#x2F;24(rw,sync,root_squash,no_subtree_check)</span><br></pre></td></tr></table></figure><p>rw 表示可读写，sync 表示同步写，/home/xxx/xxx 为挂载目录，111.111.111.0表示IP在111.111.111.x域内的IP可以挂载这个目录</p><h3 id="3、启动nfs服务【主机执行】"><a href="#3、启动nfs服务【主机执行】" class="headerlink" title="3、启动nfs服务【主机执行】"></a>3、启动nfs服务【主机执行】</h3><p>设置开机自启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable rpcbind.service</span><br><span class="line">systemctl enable ntp-server.service</span><br></pre></td></tr></table></figure><p>启动 rpcbind 和 nfs 服务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl start rpcbind.service</span><br><span class="line">systemctl start ntp-server.service</span><br></pre></td></tr></table></figure><p>确认NFS服务器启动成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpcinfo -p</span><br></pre></td></tr></table></figure><p>生效配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exportfs</span><br></pre></td></tr></table></figure><p>可以查看到以下内容，即为成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;home&#x2F;xxx&#x2F;xxx 111.111.111.0&#x2F;24</span><br></pre></td></tr></table></figure><h3 id="4、从机上启动NFS客户端【从机执行】"><a href="#4、从机上启动NFS客户端【从机执行】" class="headerlink" title="4、从机上启动NFS客户端【从机执行】"></a>4、从机上启动NFS客户端【从机执行】</h3><p>为rpcbind做开机启动：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable rpcbind.service</span><br></pre></td></tr></table></figure><p>然后启动rpcbind服务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start rpcbin.service</span><br></pre></td></tr></table></figure><p>注意：客户端不需要启动nfs服务</p><h3 id="5、执行挂载【从机执行】"><a href="#5、执行挂载【从机执行】" class="headerlink" title="5、执行挂载【从机执行】"></a>5、执行挂载【从机执行】</h3><p>检查是否能读取到NFS服务端的共享目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">showmount -e 111.111.111.111</span><br></pre></td></tr></table></figure><p>111.111.111.111为主机IP<br>如有以下内容显示，即为成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Export list for 111.111.111.111:&#x2F;home&#x2F;xxx&#x2F;xxx 111.111.111.0&#x2F;24</span><br></pre></td></tr></table></figure><p>执行挂载：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount -t nfs 111.111.111.111:&#x2F;home&#x2F;xxx&#x2F;xxx &#x2F;home&#x2F;xxxx&#x2F;</span><br></pre></td></tr></table></figure><p>/home/xxxx为被挂载目录<br>查看挂载结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df -h</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;环境介绍：现有两台服务器：均为CentOS7.5环境，挂载端称为主机，被挂载端称为从机&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;1、安装NFS服务所需的软件包【主从机均需执行】&quot;&gt;&lt;a href=&quot;#1、安装NFS服务所需的软件包【主从机均需执行】&quot; class=&quot;headerl
      
    
    </summary>
    
    
      <category term="CS Soft" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/"/>
    
      <category term="OS" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/OS/"/>
    
      <category term="UNIX" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/OS/UNIX/"/>
    
      <category term="Linux" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/OS/UNIX/Linux/"/>
    
    
      <category term="Tool" scheme="https://winyter.github.io/MyBlog/tags/Tool/"/>
    
      <category term="Linux" scheme="https://winyter.github.io/MyBlog/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>VIM 编辑器用法大全【持续更新】</title>
    <link href="https://winyter.github.io/MyBlog/2020/04/19/vim-editor-guide/"/>
    <id>https://winyter.github.io/MyBlog/2020/04/19/vim-editor-guide/</id>
    <published>2020-04-19T05:34:14.000Z</published>
    <updated>2020-04-19T07:51:26.487Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>设置鼠标模式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mouse&#x3D;a</span><br></pre></td></tr></table></figure></li><li><p>替换文本<br>将 foo 替换为 bar，全局替换：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:%s&#x2F;foo&#x2F;bar&#x2F;g</span><br></pre></td></tr></table></figure><p>当前行替换 foo 为 bar：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:s&#x2F;foo&#x2F;bar&#x2F;g</span><br></pre></td></tr></table></figure><p>替换每一个 foo 为 bar，但需要确认：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:%s&#x2F;foo&#x2F;bar&#x2F;gc</span><br></pre></td></tr></table></figure><p>单词匹配替换，需要确认：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:%s&#x2F;\&lt;foo\&gt;&#x2F;bar&#x2F;gc</span><br></pre></td></tr></table></figure><p>忽略 foo 大小写，替换为 bar，需确认：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:%s&#x2F;foo&#x2F;bar&#x2F;gci</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;设置鼠标模式&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td clas
      
    
    </summary>
    
    
      <category term="CS Soft" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/"/>
    
      <category term="OS" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/OS/"/>
    
      <category term="UNIX" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/OS/UNIX/"/>
    
    
      <category term="Linux" scheme="https://winyter.github.io/MyBlog/tags/Linux/"/>
    
      <category term="VIM" scheme="https://winyter.github.io/MyBlog/tags/VIM/"/>
    
  </entry>
  
  <entry>
    <title>Hive操作集锦【持续更新】</title>
    <link href="https://winyter.github.io/MyBlog/2020/04/19/hive-commands-collection/"/>
    <id>https://winyter.github.io/MyBlog/2020/04/19/hive-commands-collection/</id>
    <published>2020-04-19T05:30:29.000Z</published>
    <updated>2020-04-19T07:48:38.611Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive操作集锦【持续更新】"><a href="#Hive操作集锦【持续更新】" class="headerlink" title="Hive操作集锦【持续更新】"></a>Hive操作集锦【持续更新】</h1><hr><h3 id="Hive删库操作"><a href="#Hive删库操作" class="headerlink" title="Hive删库操作"></a>Hive删库操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 强制删库</span><br><span class="line"># test_db为库名</span><br><span class="line">drop database if exists test_db cascade;</span><br></pre></td></tr></table></figure><hr><h3 id="Hive删表-完全删除-包括表数据和表结构"><a href="#Hive删表-完全删除-包括表数据和表结构" class="headerlink" title="Hive删表(完全删除,包括表数据和表结构)"></a>Hive删表(完全删除,包括表数据和表结构)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># test_1 替换成实际的表名</span><br><span class="line">drop table if exists test_1;</span><br></pre></td></tr></table></figure><hr><h3 id="Hive删表数据操作-保留表结构"><a href="#Hive删表数据操作-保留表结构" class="headerlink" title="Hive删表数据操作(保留表结构)"></a>Hive删表数据操作(保留表结构)</h3><p>分为分区表和非分区表</p><h4 id="1-分区表删法"><a href="#1-分区表删法" class="headerlink" title="1.分区表删法"></a>1.分区表删法</h4><p>分区表无法在beeline客户端内使用sql删除，需要进入hdfs，直接删除分区下的数据文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -rmr &#x2F;hive&#x2F;test_1&#x2F;date_partition&#x3D;1585065600&#x2F;*</span><br></pre></td></tr></table></figure><h4 id="2-非分区表删法"><a href="#2-非分区表删法" class="headerlink" title="2.非分区表删法"></a>2.非分区表删法</h4><p>即普通表删除方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># test 替换成实际的表名，前后两个都要替换</span><br><span class="line">insert overwrite table test select * from test where 1&#x3D;2;</span><br></pre></td></tr></table></figure><hr><h3 id="Hive导出表数据到本地"><a href="#Hive导出表数据到本地" class="headerlink" title="Hive导出表数据到本地"></a>Hive导出表数据到本地</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite local directory &#39;&#x2F;home&#x2F;hive&#x2F;data&#39; row format delimited fields terminated by &#39;,&#39; select * from test_1</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hive操作集锦【持续更新】&quot;&gt;&lt;a href=&quot;#Hive操作集锦【持续更新】&quot; class=&quot;headerlink&quot; title=&quot;Hive操作集锦【持续更新】&quot;&gt;&lt;/a&gt;Hive操作集锦【持续更新】&lt;/h1&gt;&lt;hr&gt;
&lt;h3 id=&quot;Hive删库操作&quot;&gt;&lt;a
      
    
    </summary>
    
    
      <category term="CS Concept" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/"/>
    
      <category term="Bigdata" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"/>
    
    
      <category term="Hadoop" scheme="https://winyter.github.io/MyBlog/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://winyter.github.io/MyBlog/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>华为FusionInsight大数据集群关系型数据库——Libra入门</title>
    <link href="https://winyter.github.io/MyBlog/2020/01/08/FusionInsight-Libra-introduction/"/>
    <id>https://winyter.github.io/MyBlog/2020/01/08/FusionInsight-Libra-introduction/</id>
    <published>2020-01-07T16:46:51.000Z</published>
    <updated>2020-04-19T07:42:05.319Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="Libra简介"><a href="#Libra简介" class="headerlink" title="Libra简介"></a>Libra简介</h2><p>LibrA是一个基于开源数据库Postgres-XC开发的分布式并行关系型数据库系统。由于是关系型数据库，它拥有所有关系型数据库的特性及模型，在使用上，我们甚至可以简单的把它看成是一个和Oracle、MySQL一样的数据库。当然，在实际的架构上和业务方式上还是有所区别的。</p><h3 id="Libra架构"><a href="#Libra架构" class="headerlink" title="Libra架构"></a>Libra架构</h3><p>LibrA由多个MPPDBServer组成，LibrA结构具体如图1所示</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/FusionInsight-Libra-introduction_image1.png" alt="图1 Libra架构图" title="">                </div>                <div class="image-caption">图1 Libra架构图</div>            </figure><h4 id="模块说明"><a href="#模块说明" class="headerlink" title="模块说明"></a>模块说明</h4><p><strong>MPPDBServer(CM)</strong><br>LibrA集群管理模块CM（Cluster Manager），即含CMServer进程的特殊MPPDBServer，负责管理和监控分布式系统中各个功能单元和物理资源的运行情况，确保整个系统的稳定运行。CM分为主CM和备CM。正常情况下，只由主CM提供LibrA集群管理服务。当主CM发生故障的情况下，备CM会主动升为主CM提供LibrA集群管理服务。</p><p>CM通过CM服务完成对各个MPPDBServer管理。CM服务由CMServer、CMAgent、Monitor组成。</p><p>CMServer是整个LibrA集群的大脑，它会根据CMAgent上报上来的各MPPDBServer状态信息来决定是否需要状态变更。CMServer只部署在主备CM上。</p><p>CMAgent是MPPDBServer上面部署的实例代理线程，负责接收CMServer下发的命令和上报MPPDBServer的Coordinator、Datanode、GTM的状态给CMServer。每个MPPDBServer均会部署一个CMAgent，也包括主备CM和主备GTM。</p><p>Monitor是watchdog定时任务，其唯一的任务是在CMAgent停止的情况下将CMAgent重启。每个MPPDBServer均会部署一个Monitor，也包括主备CM和主备GTM。</p><p><strong>MPPDBServer(GTM)</strong><br>全局事务管理模块GTM（Global Transaction Manager），即含GTM进程的特殊MPPDBServer，负责生成和维护全局事务ID、事务快照、时间戳等需要全局唯一的信息。GTM分为主GTM和备GTM。正常情况下，只由主GTM提供全局事务管理服务。当主GTM发生故障的情况下，备GTM会主动升为主GTM提供服务。</p><p><strong>MPPDBServer</strong><br>业务模块，即除MPPDBServer(CM)、MPPDBServer(GTM)以外的MPPDBServer，由Coordinator及多个Datanode组成，负责执行CM、GTM下发的任务。业务模块与MPPDBServer(CM)、MPPDBServer(GTM)主要区别是不包含CMServer与GTM进程。</p><p><strong>Coordinator</strong><br>负责提供外部应用接口、优化全局执行计划、向Datanode分发执行计划，以及汇总、处理执行结果。</p><p><strong>Datanode</strong><br>负责存储业务数据、执行数据查询任务以及向Coordinator返回执行结果。</p><p>Datanode实例分为主Datanode实例、备Datanode实例和从备Datanode实例，它们之间的工作原理如下：</p><p>主、备实例间可以正常同步数据时，主实例不会同步数据到从备实例。<br>主、备实例间无法正常同步数据时，主实例会将数据同步到从备实例。<br>主、备实例间数据同步恢复正常后，主实例会将异常期间的数据同步到备实例上，并在完成后知会从备实例清空之前同步的这部分数据。备实例同步主实例数据期间，如果主实例发生故障不可用，备实例将升为主实例，并在升为主实例成功后从备实例上同步之前异常期间的数据。</p><p><strong>Storage</strong><br>服务器的本地存储资源，持久化存储数据（支持行存、列存、混合存储）。</p><h3 id="Libra数据查询流程"><a href="#Libra数据查询流程" class="headerlink" title="Libra数据查询流程"></a>Libra数据查询流程</h3><p>作为关系型数据库系统，LibrA主要业务为数据的查询与存储。LibrA进行数据查询的流程如图2所示。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/FusionInsight-Libra-introduction_image2.png" alt="图2 数据查询流程" title="">                </div>                <div class="image-caption">图2 数据查询流程</div>            </figure><p><strong>具体查询流程如下：</strong><br>1、用户通过应用程序发出查询本地数据的SQL请求到Coordinator。<br>2、Coordinator接收用户的SQL请求，分配服务进程，向GTM请求分配全局事务信息。<br>3、GTM接收到Coordinator的请求，返回全局事务信息给Coordinator。<br>4、Coordinator根据数据分布信息以及系统元信息，解析SQL为查询计划树，从查询计划树中提取可以发送到Datanode的执行步骤，封装成SQL语句或者子执行计划树，发送到Datanode执行。<br>5、Datanode接收到读取任务后，查询具体Storage上的本地数据块。<br>6、Datanode任务执行后，将执行结果返回给Coordinator。<br>7、Coordinator将查询结果通过应用程序返回给用户。<br>LibrA数据存储流程与数据查询流程相近，请参考数据查询流程，此处不再介绍。</p><h2 id="Libra客户端的使用"><a href="#Libra客户端的使用" class="headerlink" title="Libra客户端的使用"></a>Libra客户端的使用</h2><h3 id="命令行客户端的使用操作步骤："><a href="#命令行客户端的使用操作步骤：" class="headerlink" title="命令行客户端的使用操作步骤："></a>命令行客户端的使用操作步骤：</h3><p>1、登录安装了客户端的节点及用户<br>2、执行环境变量语句</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source &lt;环境变量文件&gt;</span><br></pre></td></tr></table></figure><p>3、执行安全认证（如果使用的是非安全认证的集群，本步骤可以跳过）<br>进入 认证文件所在目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eg.&#x2F;home&#x2F;websrv&#x2F;keytab&#x2F;hbase</span><br></pre></td></tr></table></figure><p>查看认证用户名：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">klist -kt user.keytab</span><br></pre></td></tr></table></figure><p>执行认证：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kinit -kt user.keytab &lt;组件业务用户名&gt;  --&lt;组件业务用户名&gt;即上一步看到的</span><br></pre></td></tr></table></figure><p>认证用户名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eg. kinit -kt user.keytab admin@HBASE.COM</span><br></pre></td></tr></table></figure><p>4、登录Libra集群命令行交互</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gsql –U &lt;用户名&gt; -W &lt;密码&gt; -d &lt;数据库名&gt; -p &lt;数据库端口号&gt; -h &lt;集群主Master所在节点IP&gt;</span><br><span class="line">eg.gsql -U winter -W winter -d myblog -p 25308 -h 111.111.111.111</span><br></pre></td></tr></table></figure><p>5、Libra个性化操作命令<br>Libra有着完备的SQL支持，只需要使用与其他关系型数据库相同的SQL语句即可对Libra数据库做增删改查操作，但Libra客户端有一些基础的操作命令，可以帮助用户简单快捷的对数据库做操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">Informational</span><br><span class="line">  (options: S &#x3D; show system objects, + &#x3D; additional detail)</span><br><span class="line">  \d[S+]                 list tables, views, and sequences</span><br><span class="line">  \d[S+]  NAME           describe table, view, sequence, or index</span><br><span class="line">  \da[S]  [PATTERN]      list aggregates</span><br><span class="line">  \db[+]  [PATTERN]      list tablespaces</span><br><span class="line">  \dc[S+] [PATTERN]      list conversions</span><br><span class="line">  \dC[+]  [PATTERN]      list casts</span><br><span class="line">  \dd[S]  [PATTERN]      show object descriptions not displayed elsewhere</span><br><span class="line">  \ddp    [PATTERN]      list default privileges</span><br><span class="line">  \dD[S+] [PATTERN]      list domains</span><br><span class="line">  \ded[+] [PATTERN]      list data sources</span><br><span class="line">  \det[+] [PATTERN]      list foreign tables</span><br><span class="line">  \des[+] [PATTERN]      list foreign servers</span><br><span class="line">  \deu[+] [PATTERN]      list user mappings</span><br><span class="line">  \dew[+] [PATTERN]      list foreign-data wrappers</span><br><span class="line">  \df[antw][S+] [PATRN]  list [only agg&#x2F;normal&#x2F;trigger&#x2F;window] functions</span><br><span class="line">  \dF[+]  [PATTERN]      list text search configurations</span><br><span class="line">  \dFd[+] [PATTERN]      list text search dictionaries</span><br><span class="line">  \dFp[+] [PATTERN]      list text search parsers</span><br><span class="line">  \dFt[+] [PATTERN]      list text search templates</span><br><span class="line">  \dg[+]  [PATTERN]      list roles</span><br><span class="line">  \di[S+] [PATTERN]      list indexes</span><br><span class="line">  \dl                    list large objects, same as \lo_list</span><br><span class="line">  \dL[S+] [PATTERN]      list procedural languages</span><br><span class="line">  \dn[S+] [PATTERN]      list schemas</span><br><span class="line">  \do[S]  [PATTERN]      list operators</span><br><span class="line">  \dO[S+] [PATTERN]      list collations</span><br><span class="line">  \dp     [PATTERN]      list table, view, and sequence access privileges</span><br><span class="line">  \drds [PATRN1 [PATRN2]] list per-database role settings</span><br><span class="line">  \ds[S+] [PATTERN]      list sequences</span><br><span class="line">  \dt[S+] [PATTERN]      list tables</span><br><span class="line">  \dT[S+] [PATTERN]      list data types</span><br><span class="line">  \du[+]  [PATTERN]      list roles</span><br><span class="line">  \dv[S+] [PATTERN]      list views</span><br><span class="line">  \dE[S+] [PATTERN]      list foreign tables</span><br><span class="line">  \dx[+]  [PATTERN]      list extensions</span><br><span class="line">  \l[+]                  list all databases</span><br><span class="line">  \sf[+] FUNCNAME        show a function&#39;s definition</span><br><span class="line">  \z      [PATTERN]      same as \dp</span><br></pre></td></tr></table></figure><h3 id="图形化客户端的使用"><a href="#图形化客户端的使用" class="headerlink" title="图形化客户端的使用"></a>图形化客户端的使用</h3><p>由于Libra是基于Postgres进行开发的，所以所有支持Postgres的图形化客户端均能对Libra进行操作(例如：Navicat)，这里推荐一个客户端——Data Studio，供参考使用下面简单讲一讲该软件的使用方法：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/FusionInsight-Libra-introduction_image3.png" alt="图3 Data Studio界面" title="">                </div>                <div class="image-caption">图3 Data Studio界面</div>            </figure><p>①此处为连接按钮，点击后，会有新建连接信息的弹窗，按照要求，填写相关连接信息即可登录；<br>②Data Studio支持对表做一些操作，右击相应的表，即可根据菜单，对表做操作；<br>③此处为SQL命令输入框；<br>④此处为结果反馈显示区域；<br>⑤该按钮是执行语句的按钮，Data Studio对语句执行的操作与PL/SQL对语句的操作基本一样：对于鼠标选中的预警进行执行，如果没有选中，则执行整个命令框内的所有命令；<br>⑥Data Studio支持表修改操作自动提交，该功能默认为开启，如果想关闭，可以点击⑥处的按钮进行关闭，在对生产环境进行操作时，为了保证数据的安全，建议将该功能关闭，手动对表修改进行提交。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h2 id=&quot;Libra简介&quot;&gt;&lt;a href=&quot;#Libra简介&quot; class=&quot;headerlink&quot; title=&quot;Libra简介&quot;&gt;&lt;/a&gt;Libra简介&lt;/h2&gt;&lt;p&gt;LibrA是一个基于开源数据库Postgres-XC开发的分布式并行关系型数据库系统。由于
      
    
    </summary>
    
    
      <category term="CS Soft" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/"/>
    
      <category term="Server" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"/>
    
      <category term="HW-FusionInsight" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/Server/HW-FusionInsight/"/>
    
    
      <category term="Libra" scheme="https://winyter.github.io/MyBlog/tags/Libra/"/>
    
      <category term="FusionInsight" scheme="https://winyter.github.io/MyBlog/tags/FusionInsight/"/>
    
      <category term="PostgreSQL" scheme="https://winyter.github.io/MyBlog/tags/PostgreSQL/"/>
    
      <category term="MPP" scheme="https://winyter.github.io/MyBlog/tags/MPP/"/>
    
      <category term="BigData" scheme="https://winyter.github.io/MyBlog/tags/BigData/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop快速入门系列——篇五：Kafka</title>
    <link href="https://winyter.github.io/MyBlog/2020/01/08/hadoop-kafka/"/>
    <id>https://winyter.github.io/MyBlog/2020/01/08/hadoop-kafka/</id>
    <published>2020-01-07T16:46:17.000Z</published>
    <updated>2020-04-19T07:48:00.225Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="Kafka简介"><a href="#Kafka简介" class="headerlink" title="Kafka简介"></a>Kafka简介</h2><p>Kafka是用于构建实时数据管道和数据流的应用程序。具有实时横向扩展、高吞吐量、支持大量堆积具有容错性和速度快等特点。它是一个高性能分布式消息系统。</p><h3 id="Kafka架构及业务实现原理"><a href="#Kafka架构及业务实现原理" class="headerlink" title="Kafka架构及业务实现原理"></a>Kafka架构及业务实现原理</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-kafka_image1.png" alt="图1 Kafka结构" title="">                </div>                <div class="image-caption">图1 Kafka结构</div>            </figure><p>生产者（Producer）将消息发布到Kafka主题（Topic），消费者（Consumer）订阅这些主题并消费这些消息。在Kafka集群上一个服务器称为一个Broker。对于每一个主题，Kafka集群保留一个用于缩放、并行化和容错性的分区（Partition）。每个分区是一个有序、不可变的消息序列，并不断追加到提交日志文件。分区的消息每个也被赋值一个称为偏移顺序（Offset）的序列化编号。</p><p>消费者使用一个消费者组名称来标记自己，主题的每个消息被传递给每个订阅消费者组中的一个消费者。如果所有的消费者实例都属于同样的消费组，它们就像传统队列负载均衡方式工作。如上图中，Consumer1与Consumer2之间为负载均衡方式；Consumer3、Consumer4、Consumer5与Consumer6之间为负载均衡方式。如果消费者实例都属于不同的消费组，则消息会被广播给所有消费者。如上图中，Topic1中的消息，同时会广播到Consumer Group1与Consumer Group2中。</p><p><strong>结构图说明</strong><br><strong>Broker</strong><br>在Kafka集群上一个服务器称为一个Broker。<br><strong>Topic/主题</strong><br>一个Topic就是一个类别或者一个可订阅的条目名称，也即一类消息。一个主题可以有多个分区，这些分区可以作为并行的一个单元。<br><strong>Partition/分区</strong><br>是一个有序的、不可变的消息序列，这个序列可以被连续地追加—个提交日志。在分区内的每条消息都有一个有序的ID号，这个ID号被称为偏移（Offset），这个偏移量可以唯一确定每条消息在分区内的位置。<br><strong>Producer/生产者</strong><br>向Kafka的主题发布消息。<br><strong>Consumer/消费者</strong><br>向Topic订阅，并且接收发布到这些Topic的消息。</p><h3 id="Kafka特性"><a href="#Kafka特性" class="headerlink" title="Kafka特性"></a>Kafka特性</h3><p><strong>消息可靠性</strong><br>Kafka Broker收到消息后，会持久化到磁盘，同时，Topic的每个Partition有自己的Replica（备份），每个Replica分布在不同的Broker节点上，以保证当某一节点失效时，可以自动故障转移到可用消息节点。</p><p>高吞吐量<br>Kafka通过以下方式提供系统高吞吐量：<br>数据磁盘持久化：消息不在内存中cache，直接写入到磁盘，充分利用磁盘的顺序读写性能。<br>Zero-copy：减少IO操作步骤。<br>数据批量发送：提高网络利用率。<br>Topic划分为多个Partition，提高并发度，可以由多个Producer、Consumer数目之间的关系并发来读、写消息。Producer根据用户指定的算法，将消息发送到指定的Partition。</p><p><strong>消息订阅-通知机制</strong><br>消费者对感兴趣的主题进行订阅，并采取pull的方式消费数据，使得消费者可以根据其消费能力自主地控制消息拉取速度，同时，可以根据自身情况自主选择消费模式，例如批量、重复消费，从尾端开始消费等；另外，需要消费者自己负责维护其自身消息的消费记录。</p><p><strong>可扩展性</strong><br>当在Kafka集群中可通过增加Broker节点以提供更大容量时。新增的Broker会向Zookeeper注册，而Producer及Consumer会及时从Zookeeper感知到这些变化，并及时作出调整。</p><h2 id="Kafka操作指南"><a href="#Kafka操作指南" class="headerlink" title="Kafka操作指南"></a>Kafka操作指南</h2><h3 id="Kafka命令精要"><a href="#Kafka命令精要" class="headerlink" title="Kafka命令精要"></a>Kafka命令精要</h3><p>首先需要声明，由于Kafka很多业务需要有Zookeeper去完成，所以Kafka的操作命令中，有一部分需要用到Zookeeper组件的节点IP，在实际使用中，需要注意哪些命令使用Kafka节点IP，哪些使用Zookeeper节点IP，如果使用混了，很容易会导致查询出错。另外，还需要关注Kafka的不同端口，比如一些带有安全认证的集群，有安全端口和非安全端口的区别。</p><p>查看当前集群Topic列表。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;kafka-topics.sh --list --zookeeper &lt;ZooKeeper集群IP:24002&#x2F;kafka&gt;</span><br></pre></td></tr></table></figure><p>查看单个Topic详细信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;kafka-topics.sh --describe --zookeeper &lt;ZooKeeper集群IP:24002&#x2F;kafka&gt; --topic &lt;Topic名称&gt;</span><br></pre></td></tr></table></figure><p>删除Topic，由管理员用户操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;kafka-topics.sh --delete --zookeeper &lt;ZooKeeper集群IP:24002&#x2F;kafka&gt; --topic &lt;Topic名称&gt;</span><br></pre></td></tr></table></figure><p>创建Topic，由管理员用户操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;kafka-topics.sh --create --zookeeper &lt;ZooKeeper集群IP:24002&#x2F;kafka&gt; --partitions 6 --replication-factor 2 --topic &lt;Topic名称&gt;</span><br></pre></td></tr></table></figure><p>Old Producer API生产数据，服务端“allow.everyone.if.no.acl.found”配置为“True”。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;kafka-console-producer.sh --broker-list &lt;Kafka集群IP:21005&gt; --topic &lt;Topic名称&gt; --old-producer -sync</span><br></pre></td></tr></table></figure><p>Old Consumer API消费数据，服务端“allow.everyone.if.no.acl.found”配置为“True”。该命令从topic头部开始消费数据，也即从最老的数据向新数据开始消费</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;kafka-console-consumer.sh --zookeeper &lt;ZooKeeper集群IP:24002&#x2F;kafka&gt; --topic &lt;Topic名称&gt; --from-beginning</span><br></pre></td></tr></table></figure><p>赋Consumer权限命令，由管理员用户操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;kafka-acls.sh --authorizer-properties zookeeper.connect&#x3D;&lt;ZooKeeper集群IP:24002&#x2F;kafka &gt; --add --allow-principal User:&lt;用户名&gt; --consumer --topic &lt;Topic名称&gt; --group &lt;消费者组名称&gt;</span><br></pre></td></tr></table></figure><p>赋Producer权限命令，由管理员用户操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;kafka-acls.sh --authorizer-properties zookeeper.connect&#x3D;&lt;ZooKeeper集群IP:24002&#x2F;kafka &gt; --add --allow-principal User:&lt;用户名&gt; --producer --topic &lt;Topic名称&gt;</span><br></pre></td></tr></table></figure><p>New Producer API生产消息，需要拥有该Topic生产者权限。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;kafka-console-producer.sh --broker-list &lt;Kafka集群IP:21007&gt; --topic &lt;Topic名称&gt; --producer.config config&#x2F;producer.properties</span><br></pre></td></tr></table></figure><p>New Consumer API消费数据，需要拥有该Topic的消费者权限，该命令是从topic尾部开始消费数据，也即从最新的数据向老数据开始消费</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;kafka-console-consumer.sh --topic &lt;Topic名称&gt; --bootstrap-server &lt;Kafka集群IP:21007&gt; --new-consumer --consumer.config config&#x2F;consumer.properties</span><br></pre></td></tr></table></figure><p>消费kafka数据并输出（通常用来查看topic中是否有数据，以及监控topic数据）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;kafka-console-consumer.sh --topic business_imperson --bootstrap-server 17.22.148.17:21005,17.22.148.18:21005,17.22.148.19:21005,17.22.148.20:21005</span><br></pre></td></tr></table></figure><p>Tips：在已经运行了一段时间的现场环境中，如果有些命令如果不知道怎么使用，可以在命令行下，Ctrl+R，然后输入相应命令的关键词，以查找该命令的历史使用记录（该技巧不仅可以用于Kafka命令的查找，也可以用于其他命令）<br>更多Kafka知识和命令介绍参考hadoop官网说明文档：<a href="http://kafka.apache.org/" target="_blank" rel="noopener">http://kafka.apache.org/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h2 id=&quot;Kafka简介&quot;&gt;&lt;a href=&quot;#Kafka简介&quot; class=&quot;headerlink&quot; title=&quot;Kafka简介&quot;&gt;&lt;/a&gt;Kafka简介&lt;/h2&gt;&lt;p&gt;Kafka是用于构建实时数据管道和数据流的应用程序。具有实时横向扩展、高吞吐量、支持大量堆
      
    
    </summary>
    
    
      <category term="CS Concept" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/"/>
    
      <category term="Bigdata" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"/>
    
    
      <category term="Hadoop" scheme="https://winyter.github.io/MyBlog/tags/Hadoop/"/>
    
      <category term="Kafka" scheme="https://winyter.github.io/MyBlog/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop快速入门系列——篇六：其他组件简介</title>
    <link href="https://winyter.github.io/MyBlog/2020/01/08/hadoop-other/"/>
    <id>https://winyter.github.io/MyBlog/2020/01/08/hadoop-other/</id>
    <published>2020-01-07T16:46:01.000Z</published>
    <updated>2020-04-19T07:48:14.777Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h2><p>Flink是一个批处理和流处理结合的统一计算框架，其核心是一个提供了数据分发以及并行化计算的流数据处理引擎。它的最大亮点是流处理，是业界最顶级的开源流处理引擎。<br>Flink最适合的应用场景是低时延的数据处理（Data Processing）场景：高并发pipeline处理数据，时延毫秒级，且兼具可靠性。</p><h3 id="Flink架构"><a href="#Flink架构" class="headerlink" title="Flink架构"></a>Flink架构</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-other_image1.png" alt="图1 Flink架构" title="">                </div>                <div class="image-caption">图1 Flink架构</div>            </figure><p>Flink整个系统包含三个部分：<br><strong>Client</strong><br>Flink Client主要给用户提供向Flink系统提交用户任务（流式作业）的能力。<br><strong>TaskManager</strong><br>Flink系统的业务执行节点，执行具体的用户任务。TaskManager可以有多个，各个TaskManager都平等。<br><strong>JobManager</strong><br>Flink系统的管理节点，管理所有的TaskManager，并决策用户任务在哪些Taskmanager执行。JobManager在HA模式下可以有多个，但只有一个主JobManager。</p><h3 id="Flink系统特性："><a href="#Flink系统特性：" class="headerlink" title="Flink系统特性："></a>Flink系统特性：</h3><p><strong>低时延</strong><br>提供ms级时延的处理能力。<br><strong>Exactly Once</strong><br>提供异步快照机制，保证所有数据真正只处理一次。<br><strong>HA</strong><br>JobManager支持主备模式，保证无单点故障。<br><strong>水平扩展能力</strong><br>TaskManager支持手动水平扩展。</p><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><p>在分布式框架中，分布式应用面临的最大的问题就是数据一致性。那么Zookeeper就是一个比较好的解决方案。在分布式框架中起到协调作用。</p><h3 id="Zookeeper是什么"><a href="#Zookeeper是什么" class="headerlink" title="Zookeeper是什么"></a>Zookeeper是什么</h3><p>zookeeper是高性能的分布式协作服务和分布式数据一致性解决方案，由雅虎创建，是Goole Chubby的开源实现。</p><p>zookeeper可以保证分布式一致性特性，包括顺序一致性、原子性、单一视图（无论客户端连接哪一个ZK服务器看到的都是一样的数据模型）、可靠性、实时性（在一定时间内客户端可以读取到最新数据状态而不是提交后所有服务器马上就全部更新。）</p><p>zookeeper的数据模型是一个树形节点，服务启动后，所有数据加载到内存中这样来提高服务器吞吐并减少延迟。</p><h3 id="Zookeeper结构"><a href="#Zookeeper结构" class="headerlink" title="Zookeeper结构"></a>Zookeeper结构</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-other_image2.png" alt="图2 Zookeeper结构" title="">                </div>                <div class="image-caption">图2 Zookeeper结构</div>            </figure><p>ZooKeeper集群中的节点分为三种角色：Leader、Follower和Observer，其结构和相互关系如图2-21所示。通常来说，需要在集群中配置奇数个（2N+1）ZooKeeper服务，至少（N+1）个投票才能成功的执行写操作。<br>Leader：ZK集群的核心角色，通过选举产生，为客户端提供读写服务，也就是处理事务请求<br>Follower：集群状态的跟随者，参加选举，没有被选上就是这个角色，提供读取服务，也就是处理非事务请求，对于收到的事务请求会转发给Leader服务器<br>Observer：观察者角色，不参加选举，但是提供数据读取服务，提供读取服务，也就是处理非事务请求，对于收到的事务请求会转发给Leader服务器。</p><h2 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h2><p>YARN是Hadoop2.0中的资源管理系统，是一个通用的资源管理模块，可以为各类应用程序进行资源管理和调度。YARN起源于Hadoop1.0中的MRv1，起初这个模块仅用来作为MapReduce的资源管理工具，后来逐步发展，在Hadoop2.0中，该模块也成为了其他计算框架的同意资源调度工具。YARN主要包括ResourceManager、ApplicationMaster与NodeManager三个部分。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-other_image3.png" alt="图3 Yarn的基本架构" title="">                </div>                <div class="image-caption">图3 Yarn的基本架构</div>            </figure><p>结构图说明：<br><strong>Client</strong><br>YARN Application客户端，用户可以通过客户端向ResourceManager提交任务，查询Application运行状态等。<br><strong>ResourceManager(RM)</strong><br>负责集群中所有资源的统一管理和分配。接收来自各个节点（NodeManager）的资源汇报信息，并根据收集的资源按照一定的策略分配给各个应用程序。</p><p>RM是一个全局的资源管理器，负责整个系统的资源管理和分配。主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager）。</p><ul><li>调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念Container表示。Container是一个动态资源分配单位，将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需要设计新的调度器，YARN提供了多种直接可用的调度器，比如Fair Scheduler和Capacity Scheduler等。</li><li>应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动等。</li></ul><p><strong>NodeManager(NM)</strong><br>NodeManager（NM）是YARN中每个节点上的代理，管理Hadoop集群中单个计算节点，包括与ResourceManger保持通信，监督Container的生命周期管理，监控每个Container的资源使用（内存、CPU等）情况，追踪节点健康状况，管理日志和不同应用程序用到的附属服务（auxiliary service）。</p><p>NM是每个节点上的资源和任务管理器，一方面，会定时向RM汇报本节点上的资源使用情况和各个Container的运行状态；另一方面，接收并处理来自AM的Container启动/停止等请求。</p><p><strong>ApplicationMaster(AM)</strong><br>即图中的App Mstr，负责一个Application生命周期内的所有工作。包括：与RM调度器协商以获取资源；将得到的资源进一步分配给内部任务（资源的二次分配）；与NM通信以启动/停止任务；监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。</p><p>AM负责一个Application生命周期内的所有工作。包括：</p><ul><li>与RM调度器协商以获取资源。</li><li>将得到的资源进一步分配给内部的任务(资源的二次分配)。</li><li>与NM通信以启动/停止任务。</li><li>监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。</li></ul><p><strong>Container</strong><br>Container是YARN中的资源抽象，封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等（目前仅封装内存和CPU），当AM向RM申请资源时，RM为AM返回的资源便是用Container表示。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。</p><h2 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h2><p>Redis，本质上上一个KEY-VALUE类型的内存数据库，整个数据库都加载在内存当中进行操作，定期通过异步操作把数据库数据flush到硬盘上进行保存。因此它是纯内存操作，Redis的性能非常出色，每秒可以处理超过10万次读写操作。虽然是内存数据库，但是其数据可以持久化，而且支持丰富的数据类型。</p><p>Redis支持保存LIST列表和SET集合的数据结构，而且还支持对LIST进行各种操作，例如从LIST两端进行PUSH和POP数据，取LIST区间，排序等等。对SET支持各种集合的并集交集操作，单个value的最大限制是1GB。</p><p>Redis主要的缺点是受到物理内存限制，不能用作海量数据的高性能读写，而且它没有原生的可扩展机制，不具有扩展能力，要依赖客户端来实现分布式读写，因此其适合的应用场景主要局限在较小数据量的高性能操作和运算上。</p><p>键值类型的数据库主要使用哈希表，这个表中有一个特定的键和一个指针指向特定数据。KEY/VALUE模型对于IT系统来说的优势在于简单、容易部署。主要特点是具有极高的并发读写性能。</p><h3 id="Redis逻辑架构"><a href="#Redis逻辑架构" class="headerlink" title="Redis逻辑架构"></a>Redis逻辑架构</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-other_image4.png" alt="图4 Redis逻辑架构" title="">                </div>                <div class="image-caption">图4 Redis逻辑架构</div>            </figure><p><strong>Redis Server</strong>：Redis组件的核心模块，负责Redis协议的数据读写、数据持久化、主从复制、集群功能。<br><strong>Redis-WS</strong>：Redis WebService管理模块，主要负责Redis集群的创建、扩容、减容、查询、删除等操作，集群管理信息存入DB数据库。</p><h3 id="Redis持久化"><a href="#Redis持久化" class="headerlink" title="Redis持久化"></a>Redis持久化</h3><p>Redis的所有数据都保存在内存中，然后不定期的通过异步方式保存到磁盘上（这称为半持久化）；也可以把每一次数据变化都写入到磁盘（这称为全持久化）。所谓持久化就是将内存数据转换为硬盘数据，内存模型到存储模型的转换，或者说是瞬时状态与持久状态的相互转换。<br>Redis有两种持久化方式，默认是snapshot方式（RDB方式），实现方法是定时将内存的快照持久化到硬盘，这种方式的缺点是持久化之后如果出现crash则会丢失一段数据。另外一种是AOF方式，在写入内存数据的同时将操作命令保存到日志文件中。<br><strong>1、RDB方式：</strong><br>这种快照方式和虚拟机的快照一样，保存某一时刻的完整数据。Redis在使用这种方式做持久化的时候，定期（默认5分钟）会先写入到一个临时文件，写入完成后，会用这个文件去替换上次的旧的文件。这种方式的好处是，任何一次的快照文件都是完整可用的。但是缺点是，它每隔一段时间（默认最快1分钟，最慢15分钟）做一次，所以会存在一段时间的数据丢失。<br><strong>2、AOF方式：</strong><br>这种方式就是把对Redis内存数据的的写指令记录下来，这些指令会被记录在AOF文件的末尾，然后每秒做一次fsync操作（默认每秒一次），把指令在后台在执行一次执行过程其实就是修改磁盘上的数据库内容。所以如果出现故障也只丢失1秒的数据。</p><p>上面这种方式就很类似于传统数据库服务器的事务日志。</p><p>如果遇到在追加日志的时候遇到意外，可以使用redis-check-aof工具进行日志修复。</p><p>因为采用了追加方式，所以AOF会越来越大（这一点又和传统数据库不一样，传统数据库事务日志文件都比较小），因此redis有另外一个机制就是AOF文件重写，当AOF文件达到一个设定的阈值后，会自动启动AOF文件压缩，只保留可以恢复数据的最小指令集。</p><p>通过上面的对AOF的描述，可以看到AOF是一个面向过程的，而RDB是面向对象的。</p><p>AOF方式的优点：</p><ul><li>丢失数据最小</li></ul><p>AOF方式的缺点：</p><ul><li>同等数据量，AOF文件比RDB文件体积大</li><li>AOF恢复速度比RDB方式慢</li></ul><h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><p>MapReduce是一种简化并行计算的编程模型，名字源于该模型中的两项核心操作：Map和Reduce。Map将一个作业分解成为多个任务，Reduce将分解后多个任务处理的结果汇总起来，得出最终的分析结果。MapReduce的推出给大数据并行处理带来了巨大的革命性影响，使其已经成为事实上的大数据处理的工业标准,是到目前为止最为成功、最广为接受和最易于使用的大数据并行处理技术（批处理技术）。当然，随着技术的进步，以MapReduce为代表的传统批处理计算框架也逐渐被流处理、批流合一的计算框架取代。</p><h3 id="MapReduce的原理"><a href="#MapReduce的原理" class="headerlink" title="MapReduce的原理"></a>MapReduce的原理</h3><p>Hadoop MapReduce的思想来源于Google在03、04年发表的两篇关于GFS和MapReduce的论文，在那篇MapReduce的论文中，Google表示，MapReduce的灵感来源于函数式语言（比如Lisp）中的内置函数map和reduce。在函数式语言里，map表示对一个列表（List）中的每个元素做计算，reduce表示对一个列表中的每个元素做迭代计算。它们具体的计算是通过传入的函数来实现的，map和reduce提供的是计算的框架。不过从这样的解释到现实中的MapReduce还太远，仍然需要一个跳跃。再仔细看，reduce既然能做迭代计算，那就表示列表中的元素是相关的，比如我想对列表中的所有元素做相加求和，那么列表中至少都应该是数值吧。而map是对列表中每个元素做单独处理的，这表示列表中可以是杂乱无章的数据。这样看来，就有点联系了。在MapReduce里，Map处理的是原始数据，自然是杂乱无章的，每条数据之间互相没有关系；到了Reduce阶段，数据是以key后面跟着若干个value来组织的，这些value有相关性，至少它们都在一个key下面，于是就符合函数式语言里map和reduce的基本思想了。</p><p>这样我们就可以把MapReduce理解为，把一堆杂乱无章的数据按照某种特征归纳起来，然后处理并得到最后的结果。Map面对的是杂乱无章的互不相关的数据，它解析每个数据，从中提取出key和value，也就是提取了数据的特征。经过MapReduce的Shuffle阶段之后，在Reduce阶段看到的都是已经归纳好的数据了，在此基础上我们可以做进一步的处理以便得到结果。这就回到了最初，终于知道MapReduce为何要这样设计。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h2 id=&quot;Flink&quot;&gt;&lt;a href=&quot;#Flink&quot; class=&quot;headerlink&quot; title=&quot;Flink&quot;&gt;&lt;/a&gt;Flink&lt;/h2&gt;&lt;p&gt;Flink是一个批处理和流处理结合的统一计算框架，其核心是一个提供了数据分发以及并行化计算的流数据处理引擎
      
    
    </summary>
    
    
      <category term="CS Concept" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/"/>
    
      <category term="Bigdata" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"/>
    
    
      <category term="Hadoop" scheme="https://winyter.github.io/MyBlog/tags/Hadoop/"/>
    
      <category term="Redis" scheme="https://winyter.github.io/MyBlog/tags/Redis/"/>
    
      <category term="Flink" scheme="https://winyter.github.io/MyBlog/tags/Flink/"/>
    
      <category term="Zookeeper" scheme="https://winyter.github.io/MyBlog/tags/Zookeeper/"/>
    
      <category term="Yarn" scheme="https://winyter.github.io/MyBlog/tags/Yarn/"/>
    
      <category term="Mapreduce" scheme="https://winyter.github.io/MyBlog/tags/Mapreduce/"/>
    
  </entry>
  
  <entry>
    <title>MacOS 安装 Hexo 时提示 EACCES 时的解决方法</title>
    <link href="https://winyter.github.io/MyBlog/2020/01/06/ERRORS-hexo-MacOS-EACCES/"/>
    <id>https://winyter.github.io/MyBlog/2020/01/06/ERRORS-hexo-MacOS-EACCES/</id>
    <published>2020-01-05T16:19:44.000Z</published>
    <updated>2020-04-19T07:41:01.410Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MacOS-安装-Hexo-时提示-EACCES-时的解决方法"><a href="#MacOS-安装-Hexo-时提示-EACCES-时的解决方法" class="headerlink" title="MacOS 安装 Hexo 时提示 EACCES 时的解决方法"></a>MacOS 安装 Hexo 时提示 EACCES 时的解决方法</h1><hr><p>该错误的解决方法在官方文档中有提到，但写的不甚详细，所以我在亲自测试后，写下详细步骤。</p><p><a href="https://docs.npmjs.com/resolving-eacces-permissions-errors-when-installing-packages-globally" target="_blank" rel="noopener">官方文档中提到的解决方法的链接</a></p><h4 id="该文档提供了两个方法"><a href="#该文档提供了两个方法" class="headerlink" title="该文档提供了两个方法"></a>该文档提供了两个方法</h4><ol><li>使用 node 版本管理工具；</li><li>修改 npm 的默认路径</li></ol><p>由于建议使用第一种方法，因此，下面的步骤就选择了第一种方法进行讲解。<br>而每个平台有不同的 node 版本管理工具，这里建议使用 nvm</p><h4 id="下载并安装-nvm"><a href="#下载并安装-nvm" class="headerlink" title="下载并安装 nvm"></a>下载并安装 nvm</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -o- https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;nvm-sh&#x2F;nvm&#x2F;v0.35.2&#x2F;install.sh | bash</span><br></pre></td></tr></table></figure><p>或者使用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -qO- https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;nvm-sh&#x2F;nvm&#x2F;v0.35.2&#x2F;install.sh | bash</span><br></pre></td></tr></table></figure><p>提供 NVM 的 github 地址，方便获取最新的 nvm 版本：<a href="https://github.com/nvm-sh/nvm" target="_blank" rel="noopener">点击这里</a></p><h4 id="获取-nodejs-版本信息："><a href="#获取-nodejs-版本信息：" class="headerlink" title="获取 nodejs 版本信息："></a>获取 nodejs 版本信息：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvm ls-remote</span><br></pre></td></tr></table></figure><p>该命令会把 nodejs 所有版本打印出来，你可以按照需求选择你想要的版本</p><h4 id="安装-nodejs"><a href="#安装-nodejs" class="headerlink" title="安装 nodejs"></a>安装 nodejs</h4><p>你不需要担心你之前已经安装的 nodejs，nvm 会直接覆盖安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvm install v12.14.0</span><br></pre></td></tr></table></figure><p>至此，EACCES 的问题就已经解决。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;MacOS-安装-Hexo-时提示-EACCES-时的解决方法&quot;&gt;&lt;a href=&quot;#MacOS-安装-Hexo-时提示-EACCES-时的解决方法&quot; class=&quot;headerlink&quot; title=&quot;MacOS 安装 Hexo 时提示 EACCES 时的解决方
      
    
    </summary>
    
    
      <category term="CS Soft" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/"/>
    
      <category term="Client" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/Client/"/>
    
      <category term="Hexo" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/Client/Hexo/"/>
    
    
      <category term="Hexo" scheme="https://winyter.github.io/MyBlog/tags/Hexo/"/>
    
      <category term="Tool" scheme="https://winyter.github.io/MyBlog/tags/Tool/"/>
    
      <category term="MacOS" scheme="https://winyter.github.io/MyBlog/tags/MacOS/"/>
    
      <category term="ERROR" scheme="https://winyter.github.io/MyBlog/tags/ERROR/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop快速入门系列——篇四：HDFS</title>
    <link href="https://winyter.github.io/MyBlog/2020/01/06/hadoop-HDFS/"/>
    <id>https://winyter.github.io/MyBlog/2020/01/06/hadoop-HDFS/</id>
    <published>2020-01-05T16:17:10.000Z</published>
    <updated>2020-04-19T07:47:07.804Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="HDFS简介"><a href="#HDFS简介" class="headerlink" title="HDFS简介"></a>HDFS简介</h2><p>Hadoop分布式文件系统（Hadoop Distributed File System）能提供高吞吐量的数据访问，适合大规模数据集方面的应用，为海量数据提供存储。</p><h3 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h3><p>HDFS包含主、备NameNode和多个DataNode，如图1所示。</p><p>HDFS是一个Master/Slave的架构，在Master上运行NameNode，而在每一个Slave上运行DataNode，ZKFC需要和NameNode一起运行。</p><p>NameNode和DataNode之间的通信都是建立在TCP/IP的基础之上的。NameNode、DataNode、ZKFC和JournalNode能部署在运行Linux的服务器上。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-HDFS_image1.png" alt="图1 HDFS结构图" title="">                </div>                <div class="image-caption">图1 HDFS结构图</div>            </figure><h4 id="模块说明"><a href="#模块说明" class="headerlink" title="模块说明"></a>模块说明</h4><p><strong>NameNode</strong><br>用于管理文件系统的命名空间、目录结构、元数据信息以及提供备份机制等，分为：</p><p>Active NameNode：管理文件系统的命名空间、维护文件系统的目录结构树以及元数据信息；记录写入的每个“数据块”与其归属文件的对应关系。</p><p>Standby NameNode：与Active NameNode中的数据保持同步；随时准备在Active NameNode出现异常时接管其服务。<br><strong>DataNode</strong><br>用于存储每个文件的“数据块”数据，并且会周期性地向NameNode报告该DataNode的数据存放情况。<br><strong>JournalNode</strong><br>HA集群下，用于同步主备NameNode之间的元数据信息。<br><strong>ZKFC</strong><br>ZKFC是需要和NameNode一一对应的服务，即每个NameNode都需要部署ZKFC。它负责监控NameNode的状态，并及时把状态写入Zookeeper。ZKFC也有选择谁作为Active NameNode的权利。<br><strong>ZK Cluster</strong><br>ZooKeeper是一个协调服务，帮助ZKFC执行主NameNode的选举。<br><strong>HttpFS gateway</strong><br>HttpFS是个单独无状态的gateway进程，对外提供webHDFS接口，对HDFS使用FileSystem接口对接。可用于不同Hadoop版本间的数据传输，及用于访问在防火墙后的HDFS(HttpFS用作gateway)。</p><h3 id="HDFS原理"><a href="#HDFS原理" class="headerlink" title="HDFS原理"></a>HDFS原理</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-HDFS_image2.png" alt="图2 HDFS运行原理" title="">                </div>                <div class="image-caption">图2 HDFS运行原理</div>            </figure><p>在HDFS内部，一个文件分成一个或多个“数据块”，这些“数据块”存储在DataNode集合里，NameNode负责保存和管理所有的HDFS元数据。客户端连接到NameNode，执行文件系统的“命名空间”操作，例如打开、关闭、重命名文件和目录，同时决定“数据块”到具体DataNode节点的映射。DataNode在NameNode的指挥下进行“数据块”的创建、删除和复制。客户端连接到DataNode，执行读写数据块操作。</p><h3 id="HDFS与其他组件之间的关系"><a href="#HDFS与其他组件之间的关系" class="headerlink" title="HDFS与其他组件之间的关系"></a>HDFS与其他组件之间的关系</h3><p>HDFS是整个Hadoop系统的底层存储实现，与其他很多组件都有着交互，了解HDFS与他们之间的交互将有助于理解Hadoop整套系统的交互运作方式。</p><h4 id="HDFS和HBase的配合关系"><a href="#HDFS和HBase的配合关系" class="headerlink" title="HDFS和HBase的配合关系"></a>HDFS和HBase的配合关系</h4><p>HDFS是Apache的Hadoop项目的子项目，HBase利用Hadoop HDFS作为其文件存储系统。HBase位于结构化存储层，Hadoop HDFS为HBase提供了高可靠性的底层存储支持。除了HBase产生的一些日志文件，HBase中的所有数据文件都可以存储在Hadoop HDFS文件系统上。</p><h4 id="MapReduce和HDFS的配合关系"><a href="#MapReduce和HDFS的配合关系" class="headerlink" title="MapReduce和HDFS的配合关系"></a>MapReduce和HDFS的配合关系</h4><p>HDFS是Hadoop分布式文件系统，具有高容错和高吞吐量的特性，可以部署在价格低廉的硬件上，存储应用程序的数据，适合有超大数据集的应用程序。<br>而MapReduce是一种编程模型，用于大数据集（大于1TB）的并行运算。在MapReduce程序中计算的数据可以来自多个数据源，如Local FileSystem、HDFS、数据库等。最常用的是HDFS，可以利用HDFS的高吞吐性能读取大规模的数据进行计算。同时在计算完成后，也可以将数据存储到HDFS。</p><h4 id="Spark和HDFS的配合关系"><a href="#Spark和HDFS的配合关系" class="headerlink" title="Spark和HDFS的配合关系"></a>Spark和HDFS的配合关系</h4><p>通常，Spark中计算的数据可以来自多个数据源，如Local File、HDFS等。最常用的是HDFS，用户可以一次读取大规模的数据进行并行计算。在计算完成后，也可以将数据存储到HDFS。<br>分解来看，Spark分成控制端(Driver)和执行端（Executor）。控制端负责任务调度，执行端负责任务执行。</p><h4 id="ZooKeeper和HDFS的配合关系"><a href="#ZooKeeper和HDFS的配合关系" class="headerlink" title="ZooKeeper和HDFS的配合关系"></a>ZooKeeper和HDFS的配合关系</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-HDFS_image3.png" alt="图3 ZooKeeper与HDFS" title="">                </div>                <div class="image-caption">图3 ZooKeeper与HDFS</div>            </figure><p>ZKFC（ZKFailoverController）作为一个ZooKeeper集群的客 户端，用来监控NameNode的状态信息。ZKFC进程仅在部署了NameNode的节点中存在。HDFS NameNode的Active和Standby节点均部署有zkfc进程。<br>1、HDFS NameNode的ZKFC连接到ZooKeeper，把主机名等信息保存到ZooKeeper中，即“/hadoop-ha”下的znode目录里。先创建znode目录的NameNode节点为主节点，另一个为备节点。HDFS NameNode Standby通过ZooKeeper定时读取NameNode信息。<br>2、当主节点进程异常结束时，HDFS NameNode Standby通过ZooKeeper感知“/hadoop-ha”目录下发生了变化，NameNode会进行主备切换。</p><h2 id="HDFS操作指南"><a href="#HDFS操作指南" class="headerlink" title="HDFS操作指南"></a>HDFS操作指南</h2><p>HDFS命令需要在安装了HDFS命令环境的操作系统上才能使用，由于各家大数据集群产品客户端使用方法不尽相同，因此，这里不再介绍环境加载的方法，仅提供HDFS操作命令。</p><p>操作HDFS需要使用hadoop fs 或者 hdfs dfs命令，这两个命令没有实质上的区别，实际使用中可以随意选择，下面会简单介绍一些常用的命令使用方法，如果想要了解这两条命令的详细使用，可以在服务器上执行帮助命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs –help 或者 hdfs dfs –help</span><br><span class="line">命令举例：</span><br><span class="line">eg. hdfs dfs -ls &#x2F;tenant&#x2F;</span><br></pre></td></tr></table></figure><p>常用的hadoop fs(或 hdfs dfs)命令：</p><table><thead><tr><th>命令</th><th>使用方法</th><th>作用</th></tr></thead><tbody><tr><td>-ls</td><td>-ls &lt;路径&gt;</td><td>查看指定路径的当前目录结构</td></tr><tr><td>-lsr</td><td>-lsr &lt;路径&gt;</td><td>递归查看指定路径的目录结构</td></tr><tr><td>-du</td><td>-du &lt;路径&gt;</td><td>统计目录下各文件的大小</td></tr><tr><td>-dus</td><td>-dus &lt;路径&gt;</td><td>汇总统计目录下文件(夹)大小</td></tr><tr><td>-count</td><td>-count [-q] &lt;路径&gt;</td><td>统计文件(夹)数量</td></tr><tr><td>-mv</td><td>-mv &lt;源路径&gt; &lt;目的路径&gt;</td><td>移动</td></tr><tr><td>-cp</td><td>-cp &lt;源路径&gt; &lt;目的路径&gt;</td><td>复制</td></tr><tr><td>-rm</td><td>-rm [-skipTrash] &lt;路径&gt;</td><td>删除文件/空白文件夹 &lt;慎用，数据无价&gt;</td></tr><tr><td>-rmr</td><td>-rmr [-skipTrash] &lt;路径&gt;</td><td>递归删除 &lt;慎用，数据无价&gt;</td></tr><tr><td>-put</td><td>-put &lt;多个Linux上的文件&gt; &lt;hdfs路径&gt;</td><td>上传文件</td></tr><tr><td>-copyFromLocal</td><td>-copyFromLocal &lt;多个Linux上的文件&gt; &lt;hdfs路径&gt;</td><td>从本地复制</td></tr><tr><td>-moveFromLocal</td><td>-moveFromLocal &lt;多个Linux上的文件&gt; &lt;hdfs路径&gt;</td><td>从本地移动</td></tr><tr><td>-getmerge</td><td>-getmerge &lt;源路径&gt; &lt;Linux路径&gt;</td><td>合并到本地</td></tr><tr><td>-cat</td><td>-cat &lt;hdfs路径&gt;</td><td>查看文件内容</td></tr><tr><td>-text</td><td>-text &lt;hdfs路径&gt;</td><td>查看文件内容</td></tr><tr><td>-copyToLocal</td><td>-copyToLocal [-ignoreCrc] [-crc] [hdfs源路径] [Linux目的路径]</td><td>从本地复制</td></tr><tr><td>-moveToLocal</td><td>-moveToLocal [-crc] &lt;hdfs源路径&gt; &lt;Linux目的路径&gt;</td><td>从本地移动</td></tr><tr><td>-mkdir</td><td>-mkdir &lt;hdfs路径&gt;</td><td>创建空白文件夹</td></tr><tr><td>-setrep</td><td>-setrep [-R] [-w] &lt;副本数&gt; &lt;路径&gt;</td><td>修改副本数量</td></tr><tr><td>-touchz</td><td>-touchz &lt;文件路径&gt;</td><td>创建空白文件</td></tr><tr><td>-stat</td><td>-stat [format] &lt;路径&gt;</td><td>显示文件统计信息</td></tr><tr><td>-tail</td><td>-tail [-f] &lt;文件&gt;</td><td>查看文件尾部信息</td></tr><tr><td>-chmod</td><td>-chmod [-R] &lt;权限模式&gt; [路径]</td><td>修改权限</td></tr><tr><td>-chown</td><td>-chown [-R] [属主][:[属组]] &lt;路径&gt;</td><td>修改属主</td></tr><tr><td>-chgrp</td><td>-chgrp [-R] &lt;属组名称&gt; &lt;路径&gt;</td><td>修改属组</td></tr><tr><td>-help</td><td>-help [命令选项]</td><td>帮助</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h2 id=&quot;HDFS简介&quot;&gt;&lt;a href=&quot;#HDFS简介&quot; class=&quot;headerlink&quot; title=&quot;HDFS简介&quot;&gt;&lt;/a&gt;HDFS简介&lt;/h2&gt;&lt;p&gt;Hadoop分布式文件系统（Hadoop Distributed File System）能提供高
      
    
    </summary>
    
    
      <category term="CS Concept" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/"/>
    
      <category term="Bigdata" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"/>
    
    
      <category term="Hadoop" scheme="https://winyter.github.io/MyBlog/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="https://winyter.github.io/MyBlog/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop快速入门系列——篇三：Hive</title>
    <link href="https://winyter.github.io/MyBlog/2020/01/06/hadoop-hive/"/>
    <id>https://winyter.github.io/MyBlog/2020/01/06/hadoop-hive/</id>
    <published>2020-01-05T16:14:47.000Z</published>
    <updated>2020-04-19T07:47:49.693Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="Hive简介"><a href="#Hive简介" class="headerlink" title="Hive简介"></a>Hive简介</h2><p>Hive是建立在Hadoop上的数据仓库框架，提供大数据平台批处理计算能力，能够对结构化/半结构化数据进行批量分析汇总完成数据计算。提供类似SQL的Hive Query Language语言操作结构化数据，其基本原理是将HQL语言自动转换成MapReduce任务，从而完成对Hadoop集群中存储的海量数据进行查询和分析。</p><p>Hive构建在关系型数据库结构之上，有着与Oracle、MySQL相似的数据结构，同时Hive还提供了类SQL语句进行数据库的增删改查，因此，一个习惯使用传统关系型数据库的操作者，也能无缝对接上Hive的使用。</p><h3 id="Hive架构"><a href="#Hive架构" class="headerlink" title="Hive架构"></a>Hive架构</h3><p><strong>HiveServer</strong>：一个集群内可部署多个HiveServer，负荷分担。对外提供Hive数据库服务，将用户提交的HQL语句进行编译，解析成对应的Yarn任务或者HDFS操作，从而完成数据的提取、转换、分析。</p><p><strong>MetaStore</strong>：一个集群内可部署多个MetaStore，负荷分担。提供Hive的元数据服务，负责Hive表的结构和属性信息读、写、维护和修改。提供Thrift接口，供HiveServer、Spark、WebHCat等MetaStore客户端来访问，操作元数据。</p><p><strong>WebHCat</strong>：一个集群内可部署多个WebHCat，负荷分担。提供Rest接口，通过Rest执行Hive命令，提交MapReduce任务。</p><p><strong>Hive客户端</strong>：包括人机交互命令行Beeline、提供给JDBC应用的JDBC驱动、提供给Python应用的Python驱动、提供给Mapreduce的HCatalog相关JAR包。</p><p><strong>ZooKeeper集群</strong>：Zookeeper作为临时节点记录各HiveServer实例的IP地址列表，客户端驱动连接Zookeeper获取该列表，并根据路由机制选取对应的HiveServer实例。</p><p><strong>HDFS/HBase集群</strong>：Hive表数据存储在HDFS集群中。</p><p><strong>MapReduce/Yarn集群</strong>：提供分布式计算服务：Hive的大部分数据操作依赖MapReduce，HiveServer的主要功能是将HQL语句转换成MapReduce任务，从而完成对海量数据的处理。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-hive_image1.png" alt="图1 Hive结构" title="">                </div>                <div class="image-caption">图1 Hive结构</div>            </figure><h2 id="Hive客户端的使用"><a href="#Hive客户端的使用" class="headerlink" title="Hive客户端的使用"></a>Hive客户端的使用</h2><p>由于Hive有着与SQL几乎一样的查询语句，故本节将不再讲述Hive数据库基础的增删改查命令，如果对命令有所疑问，建议查阅SQL相关资料。本节将着重介绍基于天津现场华为集群的Hive Beeline客户端的使用，以及Hive客户端一些高级特性，开源的Hive组件的使用也能从本文中找到启发。</p><h3 id="Hive-Beeline客户端使用步骤"><a href="#Hive-Beeline客户端使用步骤" class="headerlink" title="Hive Beeline客户端使用步骤"></a>Hive Beeline客户端使用步骤</h3><p>1、登陆到客户端所在服务器<br>2、安全认证(如果有的话)<br>3、登陆客户端</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline或spark-sql</span><br></pre></td></tr></table></figure><p>4、执行操作命令，Hive操作命令与sql语言几乎一样，一般的增删改查操作均可使用sql语句完成，同时，Hive数据的组织和展示形式，与关系型数据库相似。<br>个别命令举例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">查看数据库：show databases;</span><br><span class="line">进入某个数据库：use &lt;database_name&gt;;</span><br><span class="line">查看所有表：show tables;</span><br><span class="line">查看某一表结构：show create table &lt;table_name&gt;;</span><br></pre></td></tr></table></figure><h3 id="Hive的“分区剪枝”特性"><a href="#Hive的“分区剪枝”特性" class="headerlink" title="Hive的“分区剪枝”特性"></a>Hive的“分区剪枝”特性</h3><p>我们知道，在Oracle中，面对大数据量的情况下，一张表无法存储海量数据，而通过以时间分区来切分一张表，则能实现逻辑上的一张表存储海量数据的能力，其实，前面讲到的HBase中也有类似的特性(Region)，而Hive也拥有这样的能力，Hive表中，使用date_partition来实现Hive分区，date_partition会从文件存储层就将数据进行分离，如下图为某张表数据存储在HDFS上的结构：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-hive_image2.png" alt="图2 date_partition在HDFS上的路径" title="">                </div>                <div class="image-caption">图2 date_partition在HDFS上的路径</div>            </figure><p>从图中可以看到，一张表的目录下，还会根据date_partition再分目录切分Hive表的数据文件，这会给Hive表的分区剪枝功能带来实质性的性能提升，而且，在有些查询场景数据量过大的情况下，甚至会导致Hive语句的执行失败，下图展示了未启用分区剪枝的大数据查询场景下出现的报错情况：</p><p>未启用分区剪枝，报错原因主要是由于大数据量的查询会使提交的数据超过设置的输入数据限制大小：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-hive_image3.png" alt="图3 输入数据限制报错" title="">                </div>                <div class="image-caption">图3 输入数据限制报错</div>            </figure><p>已启用分区剪枝：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-hive_image4.png" alt="图4 启用分区剪枝后的查询结果" title="">                </div>                <div class="image-caption">图4 启用分区剪枝后的查询结果</div>            </figure><p>同时，在这张图中还能看到，Hive的查询是调用了MapReduce，同时，还显示出了执行语句所消耗的CPU性能。</p><p>查询时分区剪枝功能的启用：<br>在查询时，如果需要启用分区剪枝功能，需要在查询语句的where子句中，添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">date_partition between &lt;start_partition&gt; and &lt;end_partition&gt;</span><br></pre></td></tr></table></figure><p>且必须注意：该条件必须放在where子句中所有条件中的第一个，否则分区剪枝功能将不会启用。</p><h3 id="Hive删表操作"><a href="#Hive删表操作" class="headerlink" title="Hive删表操作"></a>Hive删表操作</h3><p>因Hive基于HDFS建表，Hive中直接drop操作删表会报错。以网吧流程为例，正确的删表方式是如下：</p><p>1、查询建表语句</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">show create table &lt;table_name&gt;</span><br><span class="line">找到hive表的location位置</span><br></pre></td></tr></table></figure><p>2、hadoop客户端中执行命令删除<code>/&lt;table_name&gt;</code>文件目录即可<br>注意：每个现场的Hive表文件的HDFS目录结构均不一样，删除之前，需要进行确认，以免误删，另外，删表操作生产环境，务必谨慎操作！<br>下图展示了Hive数据在HDFS上的存储结构，其中，1为库名，2为表名，3为分区名，4为实际存储的数据文件，下图展示的是parquet存储格式:</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-hive_image5.png" alt="图5 Hive数据在HDFS上的存储结构" title="">                </div>                <div class="image-caption">图5 Hive数据在HDFS上的存储结构</div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h2 id=&quot;Hive简介&quot;&gt;&lt;a href=&quot;#Hive简介&quot; class=&quot;headerlink&quot; title=&quot;Hive简介&quot;&gt;&lt;/a&gt;Hive简介&lt;/h2&gt;&lt;p&gt;Hive是建立在Hadoop上的数据仓库框架，提供大数据平台批处理计算能力，能够对结构化/半结构化
      
    
    </summary>
    
    
      <category term="CS Concept" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/"/>
    
      <category term="Bigdata" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"/>
    
    
      <category term="Hadoop" scheme="https://winyter.github.io/MyBlog/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://winyter.github.io/MyBlog/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop快速入门系列——篇二：ElasticSearch</title>
    <link href="https://winyter.github.io/MyBlog/2019/12/27/hadoop-elasticsearch/"/>
    <id>https://winyter.github.io/MyBlog/2019/12/27/hadoop-elasticsearch/</id>
    <published>2019-12-26T16:35:11.000Z</published>
    <updated>2020-04-19T07:47:31.426Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ES简介"><a href="#ES简介" class="headerlink" title="ES简介"></a>ES简介</h2><h3 id="ES数据模型"><a href="#ES数据模型" class="headerlink" title="ES数据模型"></a>ES数据模型</h3><p><strong>Index</strong>：即索引，是Elasticsearch中一个逻辑命名空间，指向一个或多个分片，内部Apache Lucene实现索引中数据的读写。索引与关系数据库实例Database相当。一个Elasticsearch实例可以包含多个索引。<br><strong>Type</strong>：文档类型，文档类型使得同一个索引中在存储结构不同的文档时，只需要依据文档类型就可以找到对应的参数映射信息，方便文档的存储。相当于数据库中的Table。一个索引对应一个文档类型。<br><strong>Document</strong>：文档，是可以被索引的基本单位，特指最顶层结构或根对象序列化成的JSON数据。相当于数据库中的Row。一个类型包含多个文档。<br><strong>Field</strong>：字段，组成文档的最小单位。相当于数据库中的Column。每个文档包含多个字段。<br><strong>Mapping</strong>：相当于数据库中的schema，用来约束字段的类型，不过 Elasticsearch 的 mapping 可以自动根据数据创建</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-elasticsearch_image1.png" alt="图1 ES数据模型模拟" title="">                </div>                <div class="image-caption">图1 ES数据模型模拟</div>            </figure><h3 id="ES架构"><a href="#ES架构" class="headerlink" title="ES架构"></a>ES架构</h3><p><strong>EsNode</strong>：Elasticsearch节点，一个节点就是一个Elasticsearch实例。<br><strong>EsMaster</strong>：主节点，可以临时管理集群级别的一些变更，例如新建或删除索引、增加或移除节点等。主节点不参与文档级别的变更或搜索，在流量增长时，该主节点不会成为集群的瓶颈。<br><strong>primary shard</strong>：主分片，索引中的每个文档属于一个单独的主分片，主分片的数量决定了索引最多能存储多少数据。<br><strong>replica shard</strong>：即复制分片，它是主分片的一个副本，可以防止硬件故障导致的数据丢失，同时可以提供读请求，比如搜索或者从别的shard取回文档。<br><strong>recovery</strong>：代表数据恢复或叫数据重新分布，Elasticsearch在有节点加入或退出时会根据机器的负载对索引分片进行重新分配，挂掉的节点重新启动时也会进行数据恢复。<br><strong>gateway</strong>：代表Elasticsearch索引快照的存储方式，默认是先把索引存放到内存中，当内存满了时再持久化到本地硬盘。gateway对索引快照进行存储，当这个Elasticsearch集群关闭再重新启动时就会从gateway中读取索引备份数据。支持多种类型的gateway，有本地文件系统（默认），分布式文件系统，Hadoop的HDFS和amazon的s3云存储服务。<br><strong>transport</strong>：代表Elasticsearch内部节点或集群与客户端的交互方式，默认内部是使用tcp协议进行交互，同时它支持http协议（json格式）、thrift、servlet、memcached、zeroMQ等的传输协议（通过插件方式集成）。<br><strong>ZooKeeper</strong>：它在Elasticsearch是必须的，提供安全认证信息的存储等功能。<br><strong>shard（分片）</strong>：是工作单元(worker unit) 底层的一员，用来分配集群中的数据，它只负责保存索引中所有数据的一小片。</p><ul><li>分片是一个独立的Lucene实例，并且它自身也是一个完整的搜索引擎。</li><li>文档存储并且被索引在分片中，但是我们的程序并不会直接与它们通信。取而代之，它们直接与索引进行通信的</li><li>把分片想象成一个数据的容器。数据被存储在分片中，然后分片又被分配在集群的节点上。当你的集群扩展或者缩小时，elasticsearch 会自动的在节点之间迁移分配分片，以便集群保持均衡</li><li>分片分为 主分片(primary shard) 以及 从分片(replica shard) 两种。在你的索引中，每一个文档都属于一个主分片</li><li>从分片只是主分片的一个副本，它用于提供数据的冗余副本，在硬件故障时提供数据保护，同时服务于搜索和检索这种只读请求</li><li>索引中的主分片的数量在索引创建后就固定下来了，但是从分片的数量可以随时改变。</li><li>一个索引默认设置了5个主分片，每个主分片有一个从分片对应</li></ul><h2 id="ES环境加载"><a href="#ES环境加载" class="headerlink" title="ES环境加载"></a>ES环境加载</h2><p>1、登录安装了ES客户端的节点<br>2、登录相应用户<br>3、加载环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eg.source &#x2F;home&#x2F;websrv&#x2F;client&#x2F;hadoopClient&#x2F;bigdata_env</span><br></pre></td></tr></table></figure><p>4、执行安全认证（如果使用的是非安全认证的集群，本步骤可以跳过）<br>进入 认证文件所在目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eg.&#x2F;home&#x2F;websrv&#x2F;keytab&#x2F;hbase</span><br></pre></td></tr></table></figure><p>查看认证用户名：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">klist -kt user.keytab</span><br></pre></td></tr></table></figure><p>执行认证：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kinit -kt user.keytab &lt;组件业务用户名&gt;  --&lt;组件业务用户名&gt;即上一步看到的</span><br></pre></td></tr></table></figure><p>认证用户名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eg. kinit -kt user.keytab tjhj_zxsk_jj@HBASE.COM</span><br></pre></td></tr></table></figure><p>然后再登录hbase shell即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">拓展：klist  ——&gt;  查看认证时间</span><br></pre></td></tr></table></figure><p>5、使用curl命令进行操作<br>命令使用方法参见下一节的介绍</p><h2 id="ES命令的使用"><a href="#ES命令的使用" class="headerlink" title="ES命令的使用"></a>ES命令的使用</h2><p>ES 的命令主要是curl命令。ElasticSearch的命令调用是基于http的，提供了丰富的RESTFul API，从功能上来分可以分为3类：<br>    (1)    获取集群基础信息；<br>    (2)    基本的数据增删改命令；<br>(3)    ES搜索/查询命令，以及高级的query、filter、聚合搜索操作；<br>其中，对于ES数据的搜索时本节讲述的重点，也是日常维护工作中使用最频繁的ES命令。ES的搜索命令从命令格式上来划分有两种方式：<br>一是通过RESTfulrequest API传递查询参数，也称“query-string”；<br>另一个是通过发送REST request body，也称作JSON格式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># query-string格式：</span><br><span class="line">curl -XGET ‘192.168.226.133:9200&#x2F;my_index&#x2F;my_type&#x2F;_search?pretty’</span><br><span class="line"># JSON格式：</span><br><span class="line">curl - XGET &#39;localhost:9200&#x2F;my_index&#x2F;my_type&#x2F;_search?pretty&#39; -d&#39;</span><br><span class="line">       &#123;</span><br><span class="line">         &quot;query&quot;: &#123;&quot;match_all&quot;: &#123;&#125; &#125;</span><br><span class="line">       &#125;&#39;</span><br></pre></td></tr></table></figure><p>由于JSON格式更加友好美观，所以目前大部分对ES的查询命令都使用该格式表达，本节也会以这种格式介绍为主，所以在介绍详细命令的使用之前，会先介绍基本的JSON语法原则和基本CURL命令格式。</p><h3 id="JSON基本语法原则"><a href="#JSON基本语法原则" class="headerlink" title="JSON基本语法原则"></a>JSON基本语法原则</h3><ul><li>数据在键/值对中：<br>JSON 数据的书写格式是：键/值对。键/值对包括字段名称（在双引号中），后面写一个冒号，然后是值：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;firstName&quot; : &quot;John&quot;</span><br></pre></td></tr></table></figure></li><li>数据由逗号分隔</li><li>花括号保存对象：<br>JSON 对象在花括号中书写，对象可以包含多个键/值对，多个键值对用逗号分隔：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; &quot;firstName&quot;:&quot;John&quot; , &quot;lastName&quot;:&quot;Doe&quot; &#125;</span><br></pre></td></tr></table></figure></li><li>方括号保存数组：<br>JSON 数组在方括号中书写，数组可包含多个对象：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;employees&quot;: [</span><br><span class="line">&#123; &quot;firstName&quot;:&quot;John&quot; , &quot;lastName&quot;:&quot;Doe&quot; &#125;,</span><br><span class="line">&#123; &quot;firstName&quot;:&quot;Anna&quot; , &quot;lastName&quot;:&quot;Smith&quot; &#125;,</span><br><span class="line">&#123; &quot;firstName&quot;:&quot;Peter&quot; , &quot;lastName&quot;:&quot;Jones&quot; &#125;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="基础curl命令"><a href="#基础curl命令" class="headerlink" title="基础curl命令"></a>基础curl命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">curl [options…] &lt;url&gt;</span><br><span class="line"></span><br><span class="line">常用操作符：</span><br><span class="line">-XGET--指定本次命令为查询请求</span><br><span class="line">-XPUT--指定本次命令为新增修改请求</span><br><span class="line">-XDELETE--指定本次命令为删除请求</span><br><span class="line">### 上面三个操作符必须选择一个，具体如何选择根据实际的增删改查操作需求</span><br><span class="line">--negotiate--使用HTTP身份认证，如果使用HTTP协议方式连接，该操作符必选</span><br><span class="line">-k--允许不使用证书到SSL站点，天津现场建议选择，其他现场按需选择</span><br><span class="line">-v--显示版本信息</span><br><span class="line">-u--设置服务器的用户名和密码</span><br><span class="line">-H--自定义头信息传递给服务，默认即可</span><br><span class="line">-d--HTTP POST方式传送数据，此处指文档内容</span><br><span class="line">以上操作符为常用的curl命令操作符，如果有其它操作符需求，可以在命令行键入：curl –help</span><br></pre></td></tr></table></figure><h3 id="获取集群基础信息"><a href="#获取集群基础信息" class="headerlink" title="获取集群基础信息"></a>获取集群基础信息</h3><p>获取集群健康状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET --negotiate -k -v -u : https:&#x2F;&#x2F;ip:httpport&#x2F;_cluster&#x2F;health?pretty</span><br></pre></td></tr></table></figure><p>注意：ip:httpport需要填写ES集群中一个节点的IP地址，不需要填写所有的节点IP，ES集群会自行查询整个集群，httpport是该节点上已安装的任意ES实例的HTTP端口，需要查询确定<br>查询所有索引</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;_cat&#x2F;indices?v&#39;</span><br></pre></td></tr></table></figure><p>查看指定索引的mapping</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;索引名&#x2F;_mapping?pretty&#39;</span><br></pre></td></tr></table></figure><p>查看指定索引的setting</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;索引名&#x2F;_settings?pretty&#39; --setting即该索引的结构</span><br></pre></td></tr></table></figure><p>检查文档是否存在</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -i -XHEAD --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;索引名&#x2F;文档类型&#x2F;索引ID&#39;</span><br></pre></td></tr></table></figure><h3 id="基本的数据增删改命令"><a href="#基本的数据增删改命令" class="headerlink" title="基本的数据增删改命令"></a>基本的数据增删改命令</h3><p>新建索引，并设置副本数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -XPUT --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;索引名?pretty&#39; -H &#39;Content-Type: application&#x2F;json&#39; -d&#39; &#123;&quot;settings&quot; : &#123;&quot;number_of_shards&quot; : 分片数量,&quot;number_of_replicas&quot; : 副本数量&#125; &#125;&#39;</span><br><span class="line"></span><br><span class="line">例如：curl -XPUT --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;1.1.1.1:24148&#x2F;website?pretty&#39; -H &#39;Content-Type: application&#x2F;json&#39; -d&#39; &#123;&quot;settings&quot; : &#123;&quot;number_of_shards&quot; : 3,&quot;number_of_replicas&quot; : 1&#125; &#125;&#39;</span><br></pre></td></tr></table></figure><p>写入数据，更新索引</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;索引名&#x2F;文档类型&#x2F;索引ID?pretty&#39; -H &#39;Content-Type: application&#x2F;json&#39; -d&#39; &#123; &#125;&#39;</span><br><span class="line"></span><br><span class="line">例如：curl -XPOST --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;1.1.1.1:24148&#x2F;website&#x2F;blog&#x2F;123?pretty&#39; -H &#39;Content-Type: application&#x2F;json&#39; -d&#39; &#123;&quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Just trying this out...&quot;,&quot;date&quot;: &quot;2014&#x2F;01&#x2F;01&quot;&#125;&#39;</span><br></pre></td></tr></table></figure><p>删除文档</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -XDELETE --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;索引名&#x2F;索引类型&#x2F;索引ID?pretty&#39;</span><br><span class="line"></span><br><span class="line">例如：curl -XDELETE --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;1.1.1.1:24148&#x2F;website&#x2F;blog&#x2F;123?pretty&#39;</span><br></pre></td></tr></table></figure><h3 id="ES搜索-查询命令，以及高级的query、filter、聚合搜索操作"><a href="#ES搜索-查询命令，以及高级的query、filter、聚合搜索操作" class="headerlink" title="ES搜索/查询命令，以及高级的query、filter、聚合搜索操作"></a>ES搜索/查询命令，以及高级的query、filter、聚合搜索操作</h3><h4 id="基础的查询命令"><a href="#基础的查询命令" class="headerlink" title="基础的查询命令"></a>基础的查询命令</h4><p>查询所有数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;索引名&#x2F;文档类型&#x2F;_search?pretty&#39;  -d&#39; &#123;</span><br><span class="line">&quot;query&quot; : &#123;</span><br><span class="line">        &quot;match_all&quot; : &#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&#39;</span><br></pre></td></tr></table></figure><p>根据rowkey查询</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;索引名&#x2F;文档类型&#x2F;_search?pretty&#39;  -d&#39; &#123;</span><br><span class="line">&quot;query&quot; : &#123;</span><br><span class="line">        &quot;match&quot; : &#123;&quot;rowkey&quot;:&quot;KX6ncmYBWgNWo4LeB0VD&quot;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&#39;</span><br><span class="line">索引名可以用2019*形式，表示查询范围是以2019开头的索引</span><br></pre></td></tr></table></figure><p>返回文档的一部分数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;索引名&#x2F;文档类型&#x2F;索引ID?_source&#x3D;文档属性名,文档属性名&amp;pretty&#39;</span><br></pre></td></tr></table></figure><p>此外，在搜索时url中可以指定索引名和类型名以减少搜索的范围：</p><ul><li>/_search：搜索所有索引的所有类型；</li><li>/my_index/_search：搜索my_index索引的所有类型；</li><li>/students, my_index /_search：搜索students和my_index索引的所有类型；</li><li>/ my_*/_search：搜索名称以my_开头的所有索引的所有类型；</li><li>/ my_index/customer/_search：搜索my_index索引的customer类型；</li><li>/_all/customer/_search：搜索所有索引的customer类型</li></ul><p>PS：一旦你取回了你的搜索结果，Elasticsearch就完成了使命，它不会维护任何服务器端的资源或者在你的结果中打开游标，这也是RESTFul风格的一个特性，也注定了ES每次返回值默认是有限制的。</p><p>由于REST request body的JSON格式从可读性和灵活性来说都给开发者带来太多的美感，所以实际应用中重点使用JSON格式，这种查询语句又被称为DSL。而DSL又因原理不同被划分为查询DSL(query DSL)和过滤DSL(filter DSL)。</p><p>DSL格式如果再使用linux的curl命令来书写那真是太累了，建议使用ELK组合中的Kibana来帮助查询ES。<br>下载地址：<a href="https://www.elastic.co/downloads/kibana" target="_blank" rel="noopener">https://www.elastic.co/downloads/kibana</a></p><p>按照官网的教程：<br>1下载解压<br>2配置config/kibana.yml<br>3执行bin/kibana<br>4浏览器访问<a href="http://localhost:5601" target="_blank" rel="noopener">http://localhost:5601</a></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-elasticsearch_image2.png" alt="图2 Kibana界面展示" title="">                </div>                <div class="image-caption">图2 Kibana界面展示</div>            </figure><p>从上面的很多命令可以看到，基础的curl命令实际上是使用的query-string格式，但是，该格式仅在简单的命令中有着优势，在面对较为复杂的查询需求，query-string格式就显得心有余而力不足，因此，随着查询需求的越来越高级，curl命令主体的格式也慢慢转变成了JSON格式，这个现象在本段上面的两条查询语句中得到了证实，而下面将会深入讲解curl命令的几个高级搜索特性，这些搜索特性，全都基于JSON格式。</p><h4 id="查询DSL-—-query"><a href="#查询DSL-—-query" class="headerlink" title="查询DSL — query"></a>查询DSL — query</h4><p>在查询上下文中，查询会回答这个问题——“这个文档匹不匹配这个查询，它的相关度高么？”<br>虽然与Oracle、MySQL同有数据存储的功能，但ES本质上是一个搜索引擎，搜索引擎意味着它需要去判断查询请求与查询内容之间的“相关度”的问题，ES为了实现“相关度”，引入了“评分”（score）这个概念，而query查询就是实现“评分”这个能力的关键<br>下面会使用一个例子体现query查询的特性，顺带，也会展示ES查询的请求与响应的细节：</p><p>请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;sample&#x2F;type&#x2F;_search?pretty&#39;  -d&#39; &#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;match&quot;: &#123;</span><br><span class="line">      &quot;state&quot;: &quot;UT&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;from&quot;: 10, </span><br><span class="line">  &quot;size&quot;: 2,</span><br><span class="line">  &quot;sort&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;age&quot;: &#123;</span><br><span class="line">        &quot;order&quot;: &quot;desc&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  &quot;_source&quot;: [&quot;account_number&quot;,&quot;address&quot;,&quot;state&quot;,&quot;age&quot;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">--query与之相对的是filter，两者的区别后面会详细介绍</span><br><span class="line">--match里面是查询条件，与query匹配使用</span><br><span class="line">--size代表结果取数返回记录数，像limit或rownum的作用。默认为10</span><br><span class="line">--from是标识从第几条记录开始取值。默认为0</span><br><span class="line">--sort标识按什么排序</span><br><span class="line">--_source标识返回集中的字段名，像select后的属性，默认是select *</span><br></pre></td></tr></table></figure><p>响应：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;took&quot;: 30,</span><br><span class="line">  &quot;timed_out&quot;: false,</span><br><span class="line">  &quot;_shards&quot;: &#123;</span><br><span class="line">    &quot;total&quot;: 5,</span><br><span class="line">    &quot;successful&quot;: 5,</span><br><span class="line">    &quot;skipped&quot;: 0,</span><br><span class="line">    &quot;failed&quot;: 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;hits&quot;: &#123;</span><br><span class="line">    &quot;total&quot;: 20,</span><br><span class="line">    &quot;max_score&quot;: null,</span><br><span class="line">    &quot;hits&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot;: &quot;my_index&quot;,</span><br><span class="line">        &quot;_type&quot;: &quot;customer&quot;,</span><br><span class="line">        &quot;_id&quot;: &quot;465&quot;,</span><br><span class="line">        &quot;_score&quot;: null,</span><br><span class="line">        &quot;_source&quot;: &#123;</span><br><span class="line">          &quot;account_number&quot;: 465,</span><br><span class="line">          &quot;address&quot;: &quot;916 Evergreen Avenue&quot;,</span><br><span class="line">          &quot;state&quot;: &quot;UT&quot;,</span><br><span class="line">          &quot;age&quot;: 29</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;sort&quot;: [</span><br><span class="line">          29</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot;: &quot;my_index&quot;,</span><br><span class="line">        &quot;_type&quot;: &quot;customer&quot;,</span><br><span class="line">        &quot;_id&quot;: &quot;758&quot;,</span><br><span class="line">        &quot;_score&quot;: null,</span><br><span class="line">        &quot;_source&quot;: &#123;</span><br><span class="line">          &quot;account_number&quot;: 758,</span><br><span class="line">          &quot;address&quot;: &quot;149 Surf Avenue&quot;,</span><br><span class="line">          &quot;state&quot;: &quot;UT&quot;,</span><br><span class="line">          &quot;age&quot;: 28</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;sort&quot;: [</span><br><span class="line">          28</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">简单说返回内容包括2部分。</span><br><span class="line">一部分是本次搜索的基本信息：</span><br><span class="line">--took消耗的时间，单位是ms</span><br><span class="line">--shards分片被检索的信息</span><br><span class="line">--hits.total满足搜索条件的记录个数；</span><br><span class="line">--hits.hits返回结果集</span><br><span class="line">另一部分是搜索结果集：</span><br><span class="line">--index结果所在索引</span><br><span class="line">--type结果所在类型</span><br><span class="line">--id结果的ID</span><br><span class="line">--score结果的评分（需要了解Lucene中df&#x2F;tf的概念Lucene原理分析）</span><br><span class="line">--source如前面所说select *里的内容</span><br></pre></td></tr></table></figure><h4 id="过滤DSL-—-filter"><a href="#过滤DSL-—-filter" class="headerlink" title="过滤DSL — filter"></a>过滤DSL — filter</h4><p>在过滤器上下文中，查询会回答这个问题——“这个文档匹不匹配？”</p><p>答案很简单，是或者不是。它不会去计算任何分值，也不会关心返回的排序问题，因此效率会高一点。</p><p>过滤上下文 是在使用filter参数时候的执行环境，比如在bool查询中使用Must_not或者filter</p><p>另外，经常使用过滤器，ES会自动的缓存过滤器的内容，这对于查询来说，会提高很多性能。</p><p>为了便于理解，下面列举一些过滤的情况：<br>创建日期是否在2013-2014年间？<br>status字段是否为published？<br>lat_lon字段是否在某个坐标的10公里范围内？</p><p>从上面的例子中可以看到，过滤器上下文中，查询操作仅判断是否满足查询条件。</p><p>最后，使用一个例子，来演示一下filter的使用：<br>搜索年龄在20-25之间，余额在20000-35000之间，地址包含Street，或者邮箱包含schultzmoreno的这样的记录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;sample&#x2F;type&#x2F;_search?pretty&#39;  -d&#39; &#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;bool&quot;:&#123;</span><br><span class="line">      &quot;should&quot;: [</span><br><span class="line">        &#123;&quot;match&quot;: &#123;</span><br><span class="line">          &quot;address&quot;: &quot;Street&quot;</span><br><span class="line">        &#125;&#125;,</span><br><span class="line">        &#123;&quot;match&quot;: &#123;</span><br><span class="line">          &quot;email&quot;: &quot;schultzmoreno&quot;</span><br><span class="line">        &#125;&#125;</span><br><span class="line">      ],</span><br><span class="line">      &quot;filter&quot;: &#123;</span><br><span class="line">        &quot;range&quot;: &#123;</span><br><span class="line">        &quot;age&quot;: &#123;</span><br><span class="line">          &quot;gte&quot;: 20,</span><br><span class="line">          &quot;lte&quot;: 25</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;filter&quot;: &#123;</span><br><span class="line">        &quot;range&quot;: &#123;</span><br><span class="line">        &quot;balance&quot;: &#123;</span><br><span class="line">          &quot;gte&quot;: 20000,</span><br><span class="line">          &quot;lte&quot;: 35000</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">  &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;from&quot;: 0, </span><br><span class="line">  &quot;size&quot;: 10</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="query与filter的区别"><a href="#query与filter的区别" class="headerlink" title="query与filter的区别"></a>query与filter的区别</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-elasticsearch_image3.png" alt="图3 query与filter的区别" title="">                </div>                <div class="image-caption">图3 query与filter的区别</div>            </figure><p>query与filter虽然有相对关系，但在实际使用中，并不存在两者不能共用的问题，正如上面的那个查询语句示例，filter嵌套在query里面，在实际使用中，两者的使用仅看查询的需求，如果需要query查询那样的全文搜索、评分功能，那么就使用query，如果需要filter过滤那样的精准判断，那么就使用filter，下面同样以一个例子，展示query与filter的混合使用，以及如何去根据实际需求，组织查询语句：</p><p>假设现在我们需要查询一个邮箱数据库，查询收件箱中，邮件内容包含business opportunity 的邮件：</p><p>首先，获取邮件内容中有“business opportunity”的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">    &quot;match&quot;: &#123; </span><br><span class="line">        &quot;email&quot;: &quot;business opportunity&quot; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后，限定只在收件箱中查找：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">    &quot;term&quot;: &#123; </span><br><span class="line">        &quot;folder&quot;: &quot;inbox&quot; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接着，对这两条语句进行拼合，match语句很明显是一个query查询，而team语句则可以看出是一个filter过滤，由于search API中只能包含 query 语句，所以我们需要用 filtered 来同时包含 “query” 和 “filter” 子句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">    &quot;filtered&quot;: &#123; </span><br><span class="line">        &quot;query&quot;:  &#123; &quot;match&quot;: &#123; &quot;email&quot;: &quot;business opportunity&quot; &#125;&#125;, </span><br><span class="line">        &quot;filter&quot;: &#123; &quot;term&quot;:  &#123; &quot;folder&quot;: &quot;inbox&quot; &#125;&#125; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后，我们在最外层加上query的上下文关系以及curl命令主体：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;sample&#x2F;type&#x2F;_search?pretty&#39;  -d&#39;</span><br><span class="line">&#123; </span><br><span class="line">    &quot;query&quot;: &#123; </span><br><span class="line">        &quot;filtered&quot;: &#123; </span><br><span class="line">            &quot;query&quot;:  &#123; &quot;match&quot;: &#123; &quot;email&quot;: &quot;business opportunity&quot; &#125;&#125;, </span><br><span class="line">            &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;folder&quot;: &quot;inbox&quot; &#125;&#125; </span><br><span class="line">        &#125; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至此，一个简单的复合查询命令就已经完成，只需要执行，便能得到想要的结果。</p><h4 id="聚合查询"><a href="#聚合查询" class="headerlink" title="聚合查询"></a>聚合查询</h4><p>Elasticsearch具有称为聚合的功能，允许您对数据生成复杂的分组和分析，像是Oracle中的group by、avg等，而且更强大。</p><h5 id="Metric聚合"><a href="#Metric聚合" class="headerlink" title="Metric聚合"></a>Metric聚合</h5><p>先看运算，跟Oracle类似有sum、max、min、avg、count等，在ES中可以单独算某一种运算外，还提供了一个stats参数，一次请求把以上所有结果都返回出来。</p><p>请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;sample&#x2F;type&#x2F;_search?pretty&#39;  -d&#39;&#123;</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;my_name&quot;: &#123;</span><br><span class="line">      &quot;stats&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;age&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;size&quot;: 0</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">--Aggs代表是启用聚合功能了，固定套路。</span><br><span class="line">--My_name给聚合返回的结果集起一个别名，单独看不出意义，嵌套聚合的时候体现价值。</span><br><span class="line">--Stats聚合的操作命令，这里是统计的命令，可以换成sum、max、min、avg、term等各种各样ES内置的命令。</span><br><span class="line">--Field指定被聚合的属性名。</span><br><span class="line">--Size指定结果集中要返回多少条被本次聚合命中的document，如果只关心聚合结果不关心命中的记录，size请指定为0。</span><br></pre></td></tr></table></figure><p>返回：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;took&quot;: 24,</span><br><span class="line">  &quot;timed_out&quot;: false,</span><br><span class="line">  &quot;_shards&quot;: &#123;</span><br><span class="line">    &quot;total&quot;: 5,</span><br><span class="line">    &quot;successful&quot;: 5,</span><br><span class="line">    &quot;skipped&quot;: 0,</span><br><span class="line">    &quot;failed&quot;: 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;hits&quot;: &#123;</span><br><span class="line">    &quot;total&quot;: 1000,</span><br><span class="line">    &quot;max_score&quot;: 0,</span><br><span class="line">    &quot;hits&quot;: []</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;aggregations&quot;: &#123;</span><br><span class="line">    &quot;my_name&quot;: &#123;</span><br><span class="line">      &quot;count&quot;: 1000,</span><br><span class="line">      &quot;min&quot;: 20,</span><br><span class="line">      &quot;max&quot;: 40,</span><br><span class="line">      &quot;avg&quot;: 30.171,</span><br><span class="line">      &quot;sum&quot;: 30171</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="Bucket聚合"><a href="#Bucket聚合" class="headerlink" title="Bucket聚合"></a>Bucket聚合</h5><p>桶子的意思，根据条件把数据按照木桶封装好，有点Oracle中group by的意思。</p><p>请求：按照年龄把文档按桶子分分类。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;sample&#x2F;type&#x2F;_search?pretty&#39;  -d&#39;&#123;</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;my_name&quot;: &#123;</span><br><span class="line">      &quot;terms&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;age&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;size&quot;: 0</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>返回：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;took&quot;: 92,</span><br><span class="line">  &quot;timed_out&quot;: false,</span><br><span class="line">  &quot;_shards&quot;: &#123;</span><br><span class="line">    &quot;total&quot;: 5,</span><br><span class="line">    &quot;successful&quot;: 5,</span><br><span class="line">    &quot;skipped&quot;: 0,</span><br><span class="line">    &quot;failed&quot;: 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;hits&quot;: &#123;</span><br><span class="line">    &quot;total&quot;: 1000,</span><br><span class="line">    &quot;max_score&quot;: 0,</span><br><span class="line">    &quot;hits&quot;: []</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;aggregations&quot;: &#123;</span><br><span class="line">    &quot;my_name&quot;: &#123;</span><br><span class="line">      &quot;doc_count_error_upper_bound&quot;: 0,</span><br><span class="line">      &quot;sum_other_doc_count&quot;: 463,</span><br><span class="line">      &quot;buckets&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;key&quot;: 31,</span><br><span class="line">          &quot;doc_count&quot;: 61</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;key&quot;: 39,</span><br><span class="line">          &quot;doc_count&quot;: 60</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;key&quot;: 26,</span><br><span class="line">          &quot;doc_count&quot;: 59</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;key&quot;: 32,</span><br><span class="line">          &quot;doc_count&quot;: 52</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;key&quot;: 35,</span><br><span class="line">          &quot;doc_count&quot;: 52</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;key&quot;: 36,</span><br><span class="line">          &quot;doc_count&quot;: 52</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;key&quot;: 22,</span><br><span class="line">          &quot;doc_count&quot;: 51</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;key&quot;: 28,</span><br><span class="line">          &quot;doc_count&quot;: 51</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;key&quot;: 33,</span><br><span class="line">          &quot;doc_count&quot;: 50</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;key&quot;: 34,</span><br><span class="line">          &quot;doc_count&quot;: 49</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每个年龄都有一桶记录在里面，默认是按照桶里数据多少来排序的，可以在field后添加 <code>&quot;order&quot; : { &quot;_term&quot; : &quot;asc&quot;}</code>指定按照内容来排序。</p><p>还可以与前面的运算嵌套使用，我想算一下每个年龄存款的平均水平：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;sample&#x2F;type&#x2F;_search?pretty&#39;  -d&#39;&#123;</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;my_name&quot;: &#123;</span><br><span class="line">      &quot;terms&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;age&quot;,</span><br><span class="line">        &quot;order&quot;: &#123;</span><br><span class="line">          &quot;avg_balance&quot;: &quot;desc&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;avg_balance&quot;: &#123;</span><br><span class="line">          &quot;avg&quot;: &#123;</span><br><span class="line">            &quot;field&quot;: &quot;balance&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;size&quot;: 0</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至此，ES命令的介绍基本结束，本章讲述的命令并非ES搜索命令的全部，ES有着强大的搜索功能，这也意味着ES的查询语句将比较复杂，建议在掌握了本节所列举的这些基础语法后，再上网搜索其他更复杂更强大的命令。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ES简介&quot;&gt;&lt;a href=&quot;#ES简介&quot; class=&quot;headerlink&quot; title=&quot;ES简介&quot;&gt;&lt;/a&gt;ES简介&lt;/h2&gt;&lt;h3 id=&quot;ES数据模型&quot;&gt;&lt;a href=&quot;#ES数据模型&quot; class=&quot;headerlink&quot; title=&quot;ES数据模
      
    
    </summary>
    
    
      <category term="CS Concept" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/"/>
    
      <category term="Bigdata" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"/>
    
    
      <category term="Hadoop" scheme="https://winyter.github.io/MyBlog/tags/Hadoop/"/>
    
      <category term="ElasticSearch" scheme="https://winyter.github.io/MyBlog/tags/ElasticSearch/"/>
    
  </entry>
  
  <entry>
    <title>HBase Shell 过滤器 —— HBase Shell Filter</title>
    <link href="https://winyter.github.io/MyBlog/2019/12/25/hbase-shell-filter/"/>
    <id>https://winyter.github.io/MyBlog/2019/12/25/hbase-shell-filter/</id>
    <published>2019-12-25T15:29:33.000Z</published>
    <updated>2020-04-19T07:48:24.745Z</updated>
    
    <content type="html"><![CDATA[<hr><p>转自：<a href="https://blog.csdn.net/u014034934/article/details/74330848" target="_blank" rel="noopener">https://blog.csdn.net/u014034934/article/details/74330848</a> &amp; <a href="https://blog.csdn.net/liuxiao723846/article/details/73823056" target="_blank" rel="noopener">https://blog.csdn.net/liuxiao723846/article/details/73823056</a></p><hr><h3 id="基础查询"><a href="#基础查询" class="headerlink" title="基础查询"></a>基础查询</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">hbase(main):046:0&gt; scan &#39;hbaseFilter&#39;</span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row0                                            column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;age0                                                                                             </span><br><span class="line"> row0                                            column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0                                                                                           </span><br><span class="line"> row1                                            column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1                                                                                             </span><br><span class="line"> row1                                            column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1                                                                                           </span><br><span class="line"> row10                                           column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;age10                                                                                           </span><br><span class="line"> row10                                           column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10                                                                                         </span><br><span class="line"> row11                                           column&#x3D;f2:age, timestamp&#x3D;1499150787905, value&#x3D;age11                                                                                           </span><br><span class="line"> row11                                           column&#x3D;f2:name, timestamp&#x3D;1499150787905, value&#x3D;name11                                                                                         </span><br><span class="line"> row12                                           column&#x3D;f2:age, timestamp&#x3D;1499150787908, value&#x3D;age12                                                                                           </span><br><span class="line"> row12                                           column&#x3D;f2:name, timestamp&#x3D;1499150787908, value&#x3D;name12                                                                                         </span><br><span class="line"> row13                                           column&#x3D;f2:age, timestamp&#x3D;1499150787911, value&#x3D;age13                                                                                           </span><br><span class="line"> row13                                           column&#x3D;f2:name, timestamp&#x3D;1499150787911, value&#x3D;name13                                                                                         </span><br><span class="line"> row14                                           column&#x3D;f2:age, timestamp&#x3D;1499150787913, value&#x3D;age14                                                                                           </span><br><span class="line"> row14                                           column&#x3D;f2:name, timestamp&#x3D;1499150787913, value&#x3D;name14                                                                                         </span><br><span class="line"> row15                                           column&#x3D;f2:age, timestamp&#x3D;1499150787917, value&#x3D;age15                                                                                           </span><br><span class="line"> row15                                           column&#x3D;f2:name, timestamp&#x3D;1499150787917, value&#x3D;name15                                                                                         </span><br><span class="line"> row16                                           column&#x3D;f2:age, timestamp&#x3D;1499150787920, value&#x3D;age16                                                                                           </span><br><span class="line"> row16                                           column&#x3D;f2:name, timestamp&#x3D;1499150787920, value&#x3D;name16                                                                                         </span><br><span class="line"> row17                                           column&#x3D;f2:age, timestamp&#x3D;1499150787923, value&#x3D;age17                                                                                           </span><br><span class="line"> row17                                           column&#x3D;f2:name, timestamp&#x3D;1499150787923, value&#x3D;name17                                                                                         </span><br><span class="line"> row18                                           column&#x3D;f2:age, timestamp&#x3D;1499150787927, value&#x3D;age18                                                                                           </span><br><span class="line"> row18                                           column&#x3D;f2:name, timestamp&#x3D;1499150787927, value&#x3D;name18                                                                                         </span><br><span class="line"> row19                                           column&#x3D;f2:age, timestamp&#x3D;1499150787930, value&#x3D;age19                                                                                           </span><br><span class="line"> row19                                           column&#x3D;f2:name, timestamp&#x3D;1499150787930, value&#x3D;name19                                                                                         </span><br><span class="line"> row2                                            column&#x3D;f:age, timestamp&#x3D;1499150787879, value&#x3D;age2                                                                                             </span><br><span class="line"> row2                                            column&#x3D;f:name, timestamp&#x3D;1499150787879, value&#x3D;name2                                                                                           </span><br><span class="line"> row3                                            column&#x3D;f:age, timestamp&#x3D;1499150787882, value&#x3D;age3                                                                                             </span><br><span class="line"> row3                                            column&#x3D;f:name, timestamp&#x3D;1499150787882, value&#x3D;name3                                                                                           </span><br><span class="line"> row4                                            column&#x3D;f:age, timestamp&#x3D;1499150787885, value&#x3D;age4                                                                                             </span><br><span class="line"> row4                                            column&#x3D;f:name, timestamp&#x3D;1499150787885, value&#x3D;name4                                                                                           </span><br><span class="line"> row5                                            column&#x3D;f:age, timestamp&#x3D;1499150787888, value&#x3D;age5                                                                                             </span><br><span class="line"> row5                                            column&#x3D;f:name, timestamp&#x3D;1499150787888, value&#x3D;name5                                                                                           </span><br><span class="line"> row6                                            column&#x3D;f:age, timestamp&#x3D;1499150787890, value&#x3D;age6                                                                                             </span><br><span class="line"> row6                                            column&#x3D;f:name, timestamp&#x3D;1499150787890, value&#x3D;name6                                                                                           </span><br><span class="line"> row7                                            column&#x3D;f:age, timestamp&#x3D;1499150787893, value&#x3D;age7                                                                                             </span><br><span class="line"> row7                                            column&#x3D;f:name, timestamp&#x3D;1499150787893, value&#x3D;name7                                                                                           </span><br><span class="line"> row8                                            column&#x3D;f:age, timestamp&#x3D;1499150787896, value&#x3D;age8                                                                                             </span><br><span class="line"> row8                                            column&#x3D;f:name, timestamp&#x3D;1499150787896, value&#x3D;name8                                                                                           </span><br><span class="line"> row9                                            column&#x3D;f:age, timestamp&#x3D;1499150787898, value&#x3D;age9                                                                                             </span><br><span class="line"> row9                                            column&#x3D;f:name, timestamp&#x3D;1499150787898, value&#x3D;name9                                                                                           </span><br><span class="line">20 row(s) in 0.1990 seconds</span><br></pre></td></tr></table></figure><h3 id="在hbase-shell用show-filters命令查看一下可以用什么Filter。"><a href="#在hbase-shell用show-filters命令查看一下可以用什么Filter。" class="headerlink" title="在hbase shell用show_filters命令查看一下可以用什么Filter。"></a>在hbase shell用show_filters命令查看一下可以用什么Filter。</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):009:0&gt; show_filters</span><br><span class="line">ColumnPrefixFilter                   </span><br><span class="line">TimestampsFilter                                 </span><br><span class="line">PageFilter                 </span><br><span class="line">MultipleColumnPrefixFilter                         </span><br><span class="line">FamilyFilter         </span><br><span class="line">ColumnPaginationFilter                       </span><br><span class="line">SingleColumnValueFilter             </span><br><span class="line">RowFilter            </span><br><span class="line">QualifierFilter                          </span><br><span class="line">ColumnRangeFilter                    </span><br><span class="line">ValueFilter                  </span><br><span class="line">PrefixFilter                        </span><br><span class="line">SingleColumnValueExcludeFilter                       </span><br><span class="line">ColumnCountGetFilter     </span><br><span class="line">InclusiveStopFilter               </span><br><span class="line">DependentColumnFilter                </span><br><span class="line">FirstKeyOnlyFilter              </span><br><span class="line">KeyOnlyFilter</span><br></pre></td></tr></table></figure><h3 id="1-keyOnlyFilter"><a href="#1-keyOnlyFilter" class="headerlink" title="1.keyOnlyFilter"></a>1.keyOnlyFilter</h3><p>返回的列值全部为空，即只要key，不要value。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.hbase.filter.KeyOnlyFilter;</span><br><span class="line">scan &#39;hbaseFilter&#39;, &#123;FILTER&#x3D;&gt;KeyOnlyFilter.new()&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row0                                            column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;                                                                                                 </span><br><span class="line"> row0                                            column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;                                                                                                </span><br><span class="line"> row1                                            column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;                                                                                                 </span><br><span class="line"> row1                                            column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;                                                                                                </span><br><span class="line"> row10                                           column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;</span><br></pre></td></tr></table></figure><h3 id="2-FirstKeyOnlyFilter"><a href="#2-FirstKeyOnlyFilter" class="headerlink" title="2.FirstKeyOnlyFilter"></a>2.FirstKeyOnlyFilter</h3><p>一个rowkey可以有多个version,同一个rowkey的同一个column也会有多个的值, 只拿出key中的第一个column的第一个version。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):096:0&gt; import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;</span><br><span class="line">hbase(main):097:0* scan &#39;hbaseFilter&#39;,&#123;FILTER&#x3D;&gt;FirstKeyOnlyFilter.new()&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row0                                            column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;age0                                                                                             </span><br><span class="line"> row1                                            column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1                                                                                             </span><br><span class="line"> row10                                           column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;age10                                                                                           </span><br><span class="line"> row11                                           column&#x3D;f2:age, timestamp&#x3D;1499150787905, value&#x3D;age11                                                                                           </span><br><span class="line"> row12                                           column&#x3D;f2:age, timestamp&#x3D;1499150787908, value&#x3D;age12</span><br></pre></td></tr></table></figure><h3 id="3-PrefixFilter"><a href="#3-PrefixFilter" class="headerlink" title="3.PrefixFilter"></a>3.PrefixFilter</h3><p>根据行的前缀过滤行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):106:0&gt; import org.apache.hadoop.hbase.filter.PrefixFilter;</span><br><span class="line">hbase(main):107:0* import org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line">hbase(main):108:0* scan &#39;hbaseFilter&#39;,&#123;FILTER&#x3D;&gt;PrefixFilter.new(Bytes.toBytes(&#39;row3&#39;))&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row3                                            column&#x3D;f:age, timestamp&#x3D;1499150787882, value&#x3D;age3                                                                                             </span><br><span class="line"> row3                                            column&#x3D;f:name, timestamp&#x3D;1499150787882, value&#x3D;name3                                                                                           </span><br><span class="line">1 row(s) in 0.1670 seconds</span><br></pre></td></tr></table></figure><h3 id="4-ColumnPrefixFilter"><a href="#4-ColumnPrefixFilter" class="headerlink" title="4.ColumnPrefixFilter"></a>4.ColumnPrefixFilter</h3><p>返回满足条件的列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):113:0&gt; import org.apache.hadoop.hbase.filter.ColumnPrefixFilter;</span><br><span class="line">hbase(main):114:0* import org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line">hbase(main):115:0* scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;ColumnPrefixFilter.new(Bytes.toBytes(&#39;n&#39;))&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row0                                            column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0                                                                                           </span><br><span class="line"> row1                                            column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1                                                                                           </span><br><span class="line"> row10                                           column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10                                                                                         </span><br><span class="line"> row11                                           column&#x3D;f2:name, timestamp&#x3D;1499150787905, value&#x3D;name11                                                                                         </span><br><span class="line"> row12                                           column&#x3D;f2:name, timestamp&#x3D;1499150787908, value&#x3D;name12                                                                                         </span><br><span class="line"> row13                                           column&#x3D;f2:name, timestamp&#x3D;1499150787911, value&#x3D;name13</span><br></pre></td></tr></table></figure><h3 id="5-multipleColumnPrefixFilter"><a href="#5-multipleColumnPrefixFilter" class="headerlink" title="5.multipleColumnPrefixFilter"></a>5.multipleColumnPrefixFilter</h3><p>根据列名得前缀过滤，有范围，下面是列名‘a’开始到‘b’结束。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):011:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;MultipleColumnPrefixFilter(&#39;a&#39;,&#39;b&#39;)&quot;&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row0                                            column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;age0                                                                                             </span><br><span class="line"> row1                                            column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1                                                                                             </span><br><span class="line"> row10                                           column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;age10                                                                                           </span><br><span class="line"> row11                                           column&#x3D;f2:age, timestamp&#x3D;1499150787905, value&#x3D;age11                                                                                           </span><br><span class="line"> row12                                           column&#x3D;f2:age, timestamp&#x3D;1499150787908, value&#x3D;age12</span><br></pre></td></tr></table></figure><h3 id="6-ColumnCountGetFilter"><a href="#6-ColumnCountGetFilter" class="headerlink" title="6.ColumnCountGetFilter"></a>6.ColumnCountGetFilter</h3><p>返回多少列。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):012:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;ColumnCountGetFilter(2)&quot;&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row0                                            column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;age0                                                                                             </span><br><span class="line"> row0                                            column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0                                                                                           </span><br><span class="line"> row1                                            column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1                                                                                             </span><br><span class="line"> row1                                            column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1                                                                                           </span><br><span class="line"> row10                                           column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;age10                                                                                           </span><br><span class="line"> row10                                           column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10                                                                                         </span><br><span class="line"> row11                                           column&#x3D;f2:age, timestamp&#x3D;1499150787905, value&#x3D;age11                                                                                           </span><br><span class="line"> row11                                           column&#x3D;f2:name, timestamp&#x3D;1499150787905, value&#x3D;name11                                                                                         </span><br><span class="line"> row12                                           column&#x3D;f2:age, timestamp&#x3D;1499150787908, value&#x3D;age12                                                                                           </span><br><span class="line"> row12                                           column&#x3D;f2:name, timestamp&#x3D;1499150787908, value&#x3D;name12</span><br></pre></td></tr></table></figure><h3 id="7-PageFilter"><a href="#7-PageFilter" class="headerlink" title="7. PageFilter"></a>7. PageFilter</h3><p>返回多少行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):013:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;PageFilter(3)&quot;&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row0                                            column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;age0                                                                                             </span><br><span class="line"> row0                                            column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0                                                                                           </span><br><span class="line"> row1                                            column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1                                                                                             </span><br><span class="line"> row1                                            column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1                                                                                           </span><br><span class="line"> row10                                           column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;age10                                                                                           </span><br><span class="line"> row10                                           column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10                                                                                         </span><br><span class="line">3 row(s) in 0.1680 seconds</span><br></pre></td></tr></table></figure><h3 id="8-ColumnPaginationFilter"><a href="#8-ColumnPaginationFilter" class="headerlink" title="8. ColumnPaginationFilter"></a>8. ColumnPaginationFilter</h3><p>根据limit和offset得到数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):014:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;ColumnPaginationFilter(2,1)&quot;&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row0                                            column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0                                                                                           </span><br><span class="line"> row1                                            column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1                                                                                           </span><br><span class="line"> row10                                           column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10                                                                                         </span><br><span class="line"> row11                                           column&#x3D;f2:name, timestamp&#x3D;1499150787905, value&#x3D;name11                                                                                         </span><br><span class="line"> row12                                           column&#x3D;f2:name, timestamp&#x3D;1499150787908, value&#x3D;name12                                                                                         </span><br><span class="line"> row13                                           column&#x3D;f2:name, timestamp&#x3D;1499150787911, value&#x3D;name13</span><br></pre></td></tr></table></figure><h3 id="9-InclusiveStopFilter"><a href="#9-InclusiveStopFilter" class="headerlink" title="9. InclusiveStopFilter"></a>9. InclusiveStopFilter</h3><p>设置停止的行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):015:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;InclusiveStopFilter(&#39;row15&#39;)&quot;&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row0                                            column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;age0                                                                                             </span><br><span class="line"> row0                                            column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0                                                                                           </span><br><span class="line"> row1                                            column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1                                                                                             </span><br><span class="line"> row1                                            column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1                                                                                           </span><br><span class="line"> row10                                           column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;age10                                                                                           </span><br><span class="line"> row10                                           column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10                                                                                         </span><br><span class="line"> row11                                           column&#x3D;f2:age, timestamp&#x3D;1499150787905, value&#x3D;age11                                                                                           </span><br><span class="line"> row11                                           column&#x3D;f2:name, timestamp&#x3D;1499150787905, value&#x3D;name11                                                                                         </span><br><span class="line"> row12                                           column&#x3D;f2:age, timestamp&#x3D;1499150787908, value&#x3D;age12                                                                                           </span><br><span class="line"> row12                                           column&#x3D;f2:name, timestamp&#x3D;1499150787908, value&#x3D;name12                                                                                         </span><br><span class="line"> row13                                           column&#x3D;f2:age, timestamp&#x3D;1499150787911, value&#x3D;age13                                                                                           </span><br><span class="line"> row13                                           column&#x3D;f2:name, timestamp&#x3D;1499150787911, value&#x3D;name13                                                                                         </span><br><span class="line"> row14                                           column&#x3D;f2:age, timestamp&#x3D;1499150787913, value&#x3D;age14                                                                                           </span><br><span class="line"> row14                                           column&#x3D;f2:name, timestamp&#x3D;1499150787913, value&#x3D;name14                                                                                         </span><br><span class="line"> row15                                           column&#x3D;f2:age, timestamp&#x3D;1499150787917, value&#x3D;age15                                                                                           </span><br><span class="line"> row15                                           column&#x3D;f2:name, timestamp&#x3D;1499150787917, value&#x3D;name15                                                                                         </span><br><span class="line">8 row(s) in 0.2250 seconds</span><br></pre></td></tr></table></figure><h3 id="10-TimeStampsFilter"><a href="#10-TimeStampsFilter" class="headerlink" title="10. TimeStampsFilter"></a>10. TimeStampsFilter</h3><p>返回指定时间戳的数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):016:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;TimestampsFilter(1499150787875,1499150787913)&quot;&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row1                                            column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1                                                                                             </span><br><span class="line"> row1                                            column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1                                                                                           </span><br><span class="line"> row14                                           column&#x3D;f2:age, timestamp&#x3D;1499150787913, value&#x3D;age14                                                                                           </span><br><span class="line"> row14                                           column&#x3D;f2:name, timestamp&#x3D;1499150787913, value&#x3D;name14                                                                                         </span><br><span class="line">2 row(s) in 0.0340 seconds</span><br></pre></td></tr></table></figure><h3 id="11-RowFilter"><a href="#11-RowFilter" class="headerlink" title="11.RowFilter"></a>11.RowFilter</h3><p>根据rowkey的值过滤</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):018:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;RowFilter(&gt;&#x3D;,&#39;binary:row6&#39;)&quot;&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row6                                            column&#x3D;f:age, timestamp&#x3D;1499150787890, value&#x3D;age6                                                                                             </span><br><span class="line"> row6                                            column&#x3D;f:name, timestamp&#x3D;1499150787890, value&#x3D;name6                                                                                           </span><br><span class="line"> row7                                            column&#x3D;f:age, timestamp&#x3D;1499150787893, value&#x3D;age7                                                                                             </span><br><span class="line"> row7                                            column&#x3D;f:name, timestamp&#x3D;1499150787893, value&#x3D;name7                                                                                           </span><br><span class="line"> row8                                            column&#x3D;f:age, timestamp&#x3D;1499150787896, value&#x3D;age8                                                                                             </span><br><span class="line"> row8                                            column&#x3D;f:name, timestamp&#x3D;1499150787896, value&#x3D;name8                                                                                           </span><br><span class="line"> row9                                            column&#x3D;f:age, timestamp&#x3D;1499150787898, value&#x3D;age9                                                                                             </span><br><span class="line"> row9                                            column&#x3D;f:name, timestamp&#x3D;1499150787898, value&#x3D;name9                                                                                           </span><br><span class="line">4 row(s) in 0.2340 seconds</span><br></pre></td></tr></table></figure><h3 id="12-FamilyFilter"><a href="#12-FamilyFilter" class="headerlink" title="12. FamilyFilter"></a>12. FamilyFilter</h3><p>根据列族过滤</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):020:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;FamilyFilter(&#x3D;,&#39;substring:f&#39;)&quot;&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row0                                            column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;age0                                                                                             </span><br><span class="line"> row0                                            column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0                                                                                           </span><br><span class="line"> row1                                            column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1                                                                                             </span><br><span class="line"> row1                                            column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1                                                                                           </span><br><span class="line"> row10                                           column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;age10                                                                                           </span><br><span class="line"> row10                                           column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10                                                                                         </span><br><span class="line"> row11                                           column&#x3D;f2:age, timestamp&#x3D;1499150787905, value&#x3D;age11                                                                                           </span><br><span class="line"> row11                                           column&#x3D;f2:name, timestamp&#x3D;1499150787905, value&#x3D;name11</span><br></pre></td></tr></table></figure><h3 id="13-QualifierFilter"><a href="#13-QualifierFilter" class="headerlink" title="13. QualifierFilter"></a>13. QualifierFilter</h3><p>根据列名过滤</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):023:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;QualifierFilter(&#x3D;,&#39;regexstring:n.&#39;)&quot;&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row0                                            column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0                                                                                           </span><br><span class="line"> row1                                            column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1                                                                                           </span><br><span class="line"> row10                                           column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10                                                                                         </span><br><span class="line"> row11                                           column&#x3D;f2:name, timestamp&#x3D;1499150787905, value&#x3D;name11                                                                                         </span><br><span class="line"> row12                                           column&#x3D;f2:name, timestamp&#x3D;1499150787908, value&#x3D;name12                                                                                         </span><br><span class="line"> row13                                           column&#x3D;f2:name, timestamp&#x3D;1499150787911, value&#x3D;name13</span><br></pre></td></tr></table></figure><h3 id="14-ValueFilter"><a href="#14-ValueFilter" class="headerlink" title="14. ValueFilter"></a>14. ValueFilter</h3><p>根据值过滤，只返回匹配的列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">hbase(main):024:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;ValueFilter(&#x3D;,&#39;binary:name3&#39;)&quot;&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row3                                            column&#x3D;f:name, timestamp&#x3D;1499150787882, value&#x3D;name3                                                                                           </span><br><span class="line">1 row(s) in 0.0140 seconds</span><br></pre></td></tr></table></figure><p>还有一种值过滤方式，以下返回的是：值包含88的数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scan &#39;test1&#39;, FILTER&#x3D;&gt;&quot;ValueFilter(&#x3D;,&#39;substring:88&#39;)&quot;</span><br><span class="line"> </span><br><span class="line">ROW                          COLUMN+CELL                                                                       </span><br><span class="line"> user1|ts2                   column&#x3D;sf:c1, timestamp&#x3D;1409122354918, value&#x3D;sku188                               </span><br><span class="line"> user2|ts5                   column&#x3D;sf:c2, timestamp&#x3D;1409122355030, value&#x3D;sku288</span><br></pre></td></tr></table></figure><h3 id="15-SingleColumnValueFilter"><a href="#15-SingleColumnValueFilter" class="headerlink" title="15. SingleColumnValueFilter"></a>15. SingleColumnValueFilter</h3><p>根据列值返回行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):035:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,COLUMN&#x3D;&gt;&#39;f&#39;,FILTER&#x3D;&gt;&quot;SingleColumnValueFilter(&#39;f&#39;,&#39;name&#39;,&gt;&#x3D;,&#39;binary:name3&#39;)&quot;&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row3                                            column&#x3D;f:age, timestamp&#x3D;1499150787882, value&#x3D;age3                                                                                             </span><br><span class="line"> row3                                            column&#x3D;f:name, timestamp&#x3D;1499150787882, value&#x3D;name3                                                                                           </span><br><span class="line"> row4                                            column&#x3D;f:age, timestamp&#x3D;1499150787885, value&#x3D;age4                                                                                             </span><br><span class="line"> row4                                            column&#x3D;f:name, timestamp&#x3D;1499150787885, value&#x3D;name4                                                                                           </span><br><span class="line"> row5                                            column&#x3D;f:age, timestamp&#x3D;1499150787888, value&#x3D;age5                                                                                             </span><br><span class="line"> row5                                            column&#x3D;f:name, timestamp&#x3D;1499150787888, value&#x3D;name5                                                                                           </span><br><span class="line"> row6                                            column&#x3D;f:age, timestamp&#x3D;1499150787890, value&#x3D;age6                                                                                             </span><br><span class="line"> row6                                            column&#x3D;f:name, timestamp&#x3D;1499150787890, value&#x3D;name6                                                                                           </span><br><span class="line"> row7                                            column&#x3D;f:age, timestamp&#x3D;1499150787893, value&#x3D;age7                                                                                             </span><br><span class="line"> row7                                            column&#x3D;f:name, timestamp&#x3D;1499150787893, value&#x3D;name7                                                                                           </span><br><span class="line"> row8                                            column&#x3D;f:age, timestamp&#x3D;1499150787896, value&#x3D;age8                                                                                             </span><br><span class="line"> row8                                            column&#x3D;f:name, timestamp&#x3D;1499150787896, value&#x3D;name8                                                                                           </span><br><span class="line"> row9                                            column&#x3D;f:age, timestamp&#x3D;1499150787898, value&#x3D;age9                                                                                             </span><br><span class="line"> row9                                            column&#x3D;f:name, timestamp&#x3D;1499150787898, value&#x3D;name9                                                                                           </span><br><span class="line">7 row(s) in 0.1520 seconds</span><br></pre></td></tr></table></figure><h3 id="16-使用AND"><a href="#16-使用AND" class="headerlink" title="16.使用AND"></a>16.使用AND</h3><p>相当于FilterList的FilterList.Operator.MUST_PASS_ALL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):042:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;(FamilyFilter(&#x3D;,&#39;substring:f&#39;)) AND (ValueFilter(&gt;,&#39;binary:name6&#39;))&quot;&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row7                                            column&#x3D;f:name, timestamp&#x3D;1499150787893, value&#x3D;name7                                                                                           </span><br><span class="line"> row8                                            column&#x3D;f:name, timestamp&#x3D;1499150787896, value&#x3D;name8                                                                                           </span><br><span class="line"> row9                                            column&#x3D;f:name, timestamp&#x3D;1499150787898, value&#x3D;name9                                                                                           </span><br><span class="line">3 row(s) in 0.0100 seconds</span><br></pre></td></tr></table></figure><h3 id="17-使用OR"><a href="#17-使用OR" class="headerlink" title="17.使用OR"></a>17.使用OR</h3><p>相当于FilterList的FilterList.Operator.MUST_PASS_ONE。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">hbase(main):044:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;(FamilyFilter(&#x3D;,&#39;substring:f&#39;)) AND (ValueFilter(&gt;,&#39;binary:name6&#39;)) OR (FamilyFilter(&#x3D;,&#39;substring:f2&#39;)) AND (ValueFilter(&gt;,&#39;binary:name17&#39;))&quot;&#125;</span><br><span class="line"></span><br><span class="line">ROW                                              COLUMN+CELL                                                                                                                                   </span><br><span class="line"> row18                                           column&#x3D;f2:name, timestamp&#x3D;1499150787927, value&#x3D;name18                                                                                         </span><br><span class="line"> row19                                           column&#x3D;f2:name, timestamp&#x3D;1499150787930, value&#x3D;name19                                                                                         </span><br><span class="line"> row7                                            column&#x3D;f:name, timestamp&#x3D;1499150787893, value&#x3D;name7                                                                                           </span><br><span class="line"> row8                                            column&#x3D;f:name, timestamp&#x3D;1499150787896, value&#x3D;name8                                                                                           </span><br><span class="line"> row9                                            column&#x3D;f:name, timestamp&#x3D;1499150787898, value&#x3D;name9                                                                                           </span><br><span class="line">5 row(s) in 0.1450 seconds</span><br></pre></td></tr></table></figure><h3 id="18、查询rowkey里面包含ts的"><a href="#18、查询rowkey里面包含ts的" class="headerlink" title="18、查询rowkey里面包含ts的"></a>18、查询rowkey里面包含ts的</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.hbase.filter.CompareFilter</span><br><span class="line">import org.apache.hadoop.hbase.filter.SubstringComparator</span><br><span class="line">import org.apache.hadoop.hbase.filter.RowFilter</span><br><span class="line">scan &#39;test1&#39;, &#123;FILTER &#x3D;&gt; RowFilter.new(CompareFilter::CompareOp.valueOf(&#39;EQUAL&#39;), SubstringComparator.new(&#39;ts&#39;))&#125;</span><br><span class="line"> </span><br><span class="line">ROW                          COLUMN+CELL                                                                       </span><br><span class="line"> user1|ts1                   column&#x3D;sf:c1, timestamp&#x3D;1409122354868, value&#x3D;sku1                                 </span><br><span class="line"> user1|ts2                   column&#x3D;sf:c1, timestamp&#x3D;1409122354918, value&#x3D;sku188                               </span><br><span class="line"> user1|ts3                   column&#x3D;sf:s1, timestamp&#x3D;1409122354954, value&#x3D;sku123                               </span><br><span class="line"> user2|ts4                   column&#x3D;sf:c1, timestamp&#x3D;1409122354998, value&#x3D;sku2                                 </span><br><span class="line"> user2|ts5                   column&#x3D;sf:c2, timestamp&#x3D;1409122355030, value&#x3D;sku288                               </span><br><span class="line"> user2|ts6                   column&#x3D;sf:s1, timestamp&#x3D;1409122355970, value&#x3D;sku222</span><br></pre></td></tr></table></figure><p>此外，该语句还支持正则表达式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.hbase.filter.RegexStringComparator</span><br><span class="line">import org.apache.hadoop.hbase.filter.CompareFilter</span><br><span class="line">import org.apache.hadoop.hbase.filter.SubstringComparator</span><br><span class="line">import org.apache.hadoop.hbase.filter.RowFilter</span><br><span class="line">scan &#39;test1&#39;, &#123;FILTER &#x3D;&gt; RowFilter.new(CompareFilter::CompareOp.valueOf(&#39;EQUAL&#39;),RegexStringComparator.new(&#39;^user\d+\|ts\d+$&#39;))&#125;</span><br><span class="line"> </span><br><span class="line">ROW                          COLUMN+CELL                                                                       </span><br><span class="line"> user1|ts1                   column&#x3D;sf:c1, timestamp&#x3D;1409122354868, value&#x3D;sku1                                 </span><br><span class="line"> user1|ts2                   column&#x3D;sf:c1, timestamp&#x3D;1409122354918, value&#x3D;sku188                               </span><br><span class="line"> user1|ts3                   column&#x3D;sf:s1, timestamp&#x3D;1409122354954, value&#x3D;sku123                               </span><br><span class="line"> user2|ts4                   column&#x3D;sf:c1, timestamp&#x3D;1409122354998, value&#x3D;sku2                                 </span><br><span class="line"> user2|ts5                   column&#x3D;sf:c2, timestamp&#x3D;1409122355030, value&#x3D;sku288                               </span><br><span class="line"> user2|ts6                   column&#x3D;sf:s1, timestamp&#x3D;1409122355970, value&#x3D;sku222</span><br></pre></td></tr></table></figure><h3 id="番外、hbase-zkcli-的使用"><a href="#番外、hbase-zkcli-的使用" class="headerlink" title="番外、hbase zkcli 的使用"></a>番外、hbase zkcli 的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">hbase zkcli</span><br><span class="line">ls &#x2F;</span><br><span class="line">[hbase, zookeeper]</span><br><span class="line"> </span><br><span class="line">[zk: hadoop000:2181(CONNECTED) 1] ls &#x2F;hbase</span><br><span class="line">[meta-region-server, backup-masters, table, draining, region-in-transition, running, table-lock, master, namespace, hbaseid, online-snapshot, replication, splitWAL, recovering-regions, rs]</span><br><span class="line"> </span><br><span class="line">[zk: hadoop000:2181(CONNECTED) 2] ls &#x2F;hbase&#x2F;table</span><br><span class="line">[member, test1, hbase:meta, hbase:namespace]</span><br><span class="line"> </span><br><span class="line">[zk: hadoop000:2181(CONNECTED) 3] ls &#x2F;hbase&#x2F;table&#x2F;test1</span><br><span class="line">[]</span><br><span class="line"> </span><br><span class="line">[zk: hadoop000:2181(CONNECTED) 4] get &#x2F;hbase&#x2F;table&#x2F;test1</span><br><span class="line">?master:60000&#125;l$??lPBUF</span><br><span class="line">cZxid &#x3D; 0x107</span><br><span class="line">ctime &#x3D; Wed Aug 27 14:52:21 HKT 2014</span><br><span class="line">mZxid &#x3D; 0x10b</span><br><span class="line">mtime &#x3D; Wed Aug 27 14:52:22 HKT 2014</span><br><span class="line">pZxid &#x3D; 0x107</span><br><span class="line">cversion &#x3D; 0</span><br><span class="line">dataVersion &#x3D; 2</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line">dataLength &#x3D; 31</span><br><span class="line">numChildren &#x3D; 0</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;p&gt;转自：&lt;a href=&quot;https://blog.csdn.net/u014034934/article/details/74330848&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/u0140349
      
    
    </summary>
    
    
      <category term="CS Concept" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/"/>
    
      <category term="Bigdata" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"/>
    
    
      <category term="HBase" scheme="https://winyter.github.io/MyBlog/tags/HBase/"/>
    
      <category term="Hadoop" scheme="https://winyter.github.io/MyBlog/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop快速入门系列——篇一：HBase入门</title>
    <link href="https://winyter.github.io/MyBlog/2019/12/25/hadoop-hbase/"/>
    <id>https://winyter.github.io/MyBlog/2019/12/25/hadoop-hbase/</id>
    <published>2019-12-25T15:01:04.000Z</published>
    <updated>2020-04-19T07:47:40.034Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="HBase概念介绍"><a href="#HBase概念介绍" class="headerlink" title="HBase概念介绍"></a>HBase概念介绍</h2><p>HBase是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统。HBase适合于存储大表数据（表的规模可以达到数十亿行以及数百万列），并且对大表数据的读、写访问可以达到实时级别。<br>利用Hadoop HDFS（Hadoop Distributed File System）作为其文件存储系统，提供高可靠性、高性能、列存储、可伸缩、实时读写的数据库系统。<br>为Spark和Hadoop MapReduce提供海量数据实时处理能力。<br>利用ZooKeeper作为协同服务。</p><h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><p><strong>Rowkey</strong>：行键，相当于关系数据库中的主键，行数据的唯一标识<br><strong>Column Family</strong>：CF，列族，一个表在水平方向上由一个或多个CF组成，一个CF由任意多个Column组成，Column为CF下的一个标签，所以CF支持动态扩展，而不需要预先定义Column，且不同行之间，列的个数和类型都可以不同<br><strong>Timestamp</strong>：每次数据操作是对应的时间戳，该列是实现Hbase一行多版本的关键，一行数据按时间戳区分版本，每个Cell的多版本按时间戳倒序存储<br><strong>Region</strong>：可以理解为表分区，随着数据量的增多，Hbase会根据设定好的配置阀值，自动分区<br><strong>Cell</strong>：HBase最小的存储单元，由Key和Value组成。Key由row、column family、column qualifier、timestamp、type、MVCC v ersion这6个字段组成。Value就是对应存储的二进制数据对象。</p><p>为了便于理解，图1通过将HBase数据模型转化成传统数据库表的实时来表现，但真实的HBase数据模型在表现形式上，还是有所区别的，这一点会在图2有所体现。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-hbase_image1.png" alt="图1 HBase数据模型" title="">                </div>                <div class="image-caption">图1 HBase数据模型</div>            </figure><p>图2为实际查询的HBase数据返回结果的截图：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-hbase_image2.png" alt="图2 HBase数据查询结果" title="">                </div>                <div class="image-caption">图2 HBase数据查询结果</div>            </figure><p>HBase的索引依据是行键（Rowkey）、列键（ColumnFamily:Column）、时间戳（Timestamp）。<br>可以看到，同一条rowkey，按照HBase列存储的规则，每一列为一条记录，相同的rowkey代表一行数据，而HBase并没有按照传统数据库行存储的方式，把一条数据的所有内容全部放在一条记录里面，取而代之的，是使用Key-Value的形式，对数据进行列存储，同时，在COLUMN+CELL中，column的值的组织形式是&lt;列族名&gt;:&lt;列名&gt;，列族与列名的组合，共同构建起了key，与之对应的value则包含在cell中，从图中也可以看出来，cell由timestamp和value组成，timestamp是HBase实现数据版本控制的核心，对同一条数据的不同迭代，以不同的timestamp进行标记，而value就是真正的这条数据，指定的这个列（key）所对应的值（value）</p><h3 id="HBase架构"><a href="#HBase架构" class="headerlink" title="HBase架构"></a>HBase架构</h3><p><strong>Master</strong>：HMaster，RegionServer的管理者，主要控制与Region相关的功能，包括但不限于：表的增删改查、RegionServer的负载均衡、Region分布调整、Region的分裂、RegionServer失效后Region的迁移。在HA模式下，会有主备Master，主Master故障，备用Master会替代。<br><strong>RegionServer</strong>：Hbase的数据处理和计算单元，提供表数据的读写等服务，RegionServer一般与HDFS的DataNode部署在一起，实现数据存储功能<br><strong>Zookeeper</strong>：为Hbase各进程提供分布式协作服务。各RegionServer将自己的信息注册到Zookeeper，主Master据此感知各个RegionServer的健康状态<br><strong>client</strong>：可以通俗的理解为客户端，client与Master进行管理类通信，与RegionServer进行数据操作类通信<br><strong>HDFS</strong>：为Hbase提供高可靠的文件存储服务，Hbase的数据全部存在HDFS中</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-hbase_image3.png" alt="图3 HBase存储架构" title="">                </div>                <div class="image-caption">图3 HBase存储架构</div>            </figure><pre><code>拓展：Region、Store、StoreFlie、ColumnFamily的关系：The HRegionServer opens the region and creates a corresponding HRegion object. When the HRegion is opened it sets up a Store instance for each HColumnFamily for every table as defined by the user beforehand. Each Store instance can, in turn, have one or more StoreFile instances, which are lightweight wrappers around the actual storage file called HFile. A Store also has a MemStore, and the HRegionServer a shared HLog in-stance。RegionServer打开一个region的时候，会创建一个相应的HRegion对象。当这个HRegion被打开，他会为每一个表中的每一个列簇创建一个Stroe实例，就向用户之前创建的那样。每一个Store实例相应地有一个或者多个StoreFile实例，StoreFile是对真正存储数据的文件(HFile)的轻量级封装。一个Store还会有一个Memstore。每一个HRegionServer中的所有东西会共享一个HLog实例。结合前面的图看就很清晰了。From:https://www.cnblogs.com/mrxiaohe/p/5271578.html</code></pre><h3 id="RegionServer数据存储结构"><a href="#RegionServer数据存储结构" class="headerlink" title="RegionServer数据存储结构"></a>RegionServer数据存储结构</h3><p><strong>Store</strong>：一个Region由一个或多个Store组成，每个Store对应一个Column Family。<br><strong>Memstore</strong>：一个Store包含一个MemStore，MemStore缓存客户端向Region插入的数据，当RegionServer中的MemStore大小达到配置的容量上限时，RegionServer会将MemStore中的数据“flush”到HDFS中。<br><strong>StoreFile</strong>：MemStore的数据flush到HDFS后成为StoreFile，随着数据的插入，一个Store会产生多个StoreFile，当StoreFile的个数达到配置的最大值时，RegionServer会将多个StoreFile合并为一个大的StoreFile。<br><strong>HFile</strong>：HFile定义了StoreFile在文件系统中的存储格式，它是当前HBase系统中StoreFile的具体实现。<br><strong>HLog</strong>：HLog日志保证了当RegionServer故障的情况下用户写入的数据不丢失，RegionServer的多个Region共享一个相同的HLog。<br><strong>元数据表</strong>：用来帮助client定位到具体的Region。元数据表中包括’hbase:meta’表，用来记录用户表的Region信息。 </p><p><img src="http://cdn.winyter.cn/hadoop-hbase_image4.png" alt="图4 HBase RegionServer存储结构"></p><h2 id="Hbase-Shell使用"><a href="#Hbase-Shell使用" class="headerlink" title="Hbase Shell使用"></a>Hbase Shell使用</h2><h3 id="HBase客户端的使用"><a href="#HBase客户端的使用" class="headerlink" title="HBase客户端的使用"></a>HBase客户端的使用</h3><p>要访问HBase shell，必须导航进入到HBase的主文件夹。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;usr&#x2F;localhost&#x2F;</span><br><span class="line">cd Hbase</span><br></pre></td></tr></table></figure><p>可以使用“hbase shell”命令来启动HBase的交互shell，如下图所示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;hbase shell</span><br></pre></td></tr></table></figure><p>如果已成功在系统中安装HBase，那么它会给出 HBase shell 提示符，如下图所示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HBase Shell; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands.</span><br><span class="line">Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell</span><br><span class="line">Version 0.94.23, rf42302b28aceaab773b15f234aa8718fff7eea3c, Wed Aug 27</span><br><span class="line">00:54:09 UTC 2014</span><br><span class="line"></span><br><span class="line">hbase(main):001:0&gt;</span><br></pre></td></tr></table></figure><p>要退出交互shell命令，在任何时候键入 <code>exit</code> 或使用 <code>&lt;Ctrl + C&gt;</code>。进一步处理检查shell功能之前，使用 <code>list</code> 命令用于列出所有可用命令。<code>list</code>是用来获取所有HBase 表的列表。首先，验证安装HBase在系统中使用如下所示。<code>hbase(main):001:0&gt; list</code> 当输入这个命令，它给出下面的输出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; list</span><br><span class="line">TABLE</span><br></pre></td></tr></table></figure><h3 id="HBase-Shell命令的使用"><a href="#HBase-Shell命令的使用" class="headerlink" title="HBase Shell命令的使用"></a>HBase Shell命令的使用</h3><p>这里仅写出一些简单的HBase Shell命令的使用，HBase Shell Filter的使用可以参考我的另外一篇文章：<a href="http://winyter.cn/2019/12/25/hbase-shell-filter/" target="_blank" rel="noopener">HBase Shell 过滤器 —— HBase Shell Filter</a></p><p>HBase Shell命令是操作HBase数据库的命令集合。 </p><h4 id="1、查询表中数据：scan-amp-get"><a href="#1、查询表中数据：scan-amp-get" class="headerlink" title="1、查询表中数据：scan &amp; get"></a>1、查询表中数据：scan &amp; get</h4><p>scan用于查询整表数据，使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan &#39;table_name&#39;</span><br></pre></td></tr></table></figure><p>get则是根据指定的表、rowkey、列组合，获取行值或行的指定cell值，使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get &#39;table_name&#39;,&#39;rowkey&#39;,&#39;cf_name:c_name&#39;</span><br></pre></td></tr></table></figure><hr><p>HBase shell支持通过过滤器Filter对数据进行带条件的查询，由于复杂的过滤查询需要调用HBase Filter的Jave接口，所以这里简单介绍一些常用的不需要调用接口的Filter查询命令，如果想要深入了解，可以上网搜索。<br>1、查询表table1中，所有值为abcde的列的数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan ‘table1’, FILTER &#x3D;&gt; “ValueFilter(&#x3D;,’binary:abcde’)”</span><br></pre></td></tr></table></figure><p>2、查询表table1中，所有值包含abc的列的数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan ‘table1’, FILTER &#x3D;&gt; “ValueFilter(&#x3D;,’subtring:abc’)”</span><br></pre></td></tr></table></figure><p>3、查询表table1，列c1中，值包含abc的数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan ‘table1’, FILTER &#x3D;&gt; “ColumnPrefixFilter(‘c1’) AND ValueFilter(&#x3D;,’substring:abc’)”</span><br></pre></td></tr></table></figure><hr><h4 id="2、统计数据条数："><a href="#2、统计数据条数：" class="headerlink" title="2、统计数据条数："></a>2、统计数据条数：</h4><p>count<br>count ‘table_name’, {INTERVAL =&gt; 1000, CACHE =&gt; 10}<br>INTERVAL为统计的行数间隔，默认为1000，CACHE为统计的数据缓存,默认为10<br>HBase的count命令与SQL数据库的count统计方法不一样，HBase在统计时，会根据INTERVAL的值对数据进行截断，实际效果如图5所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-hbase_image5.png" alt="图5 HBase Shell count命令统计结果" title="">                </div>                <div class="image-caption">图5 HBase Shell count命令统计结果</div>            </figure><h4 id="3、插入数据：put"><a href="#3、插入数据：put" class="headerlink" title="3、插入数据：put"></a>3、插入数据：put</h4><p>put插入数据仅支持cell为单位的数据插入，不支持rowkey为单位的整条数据插入<br>使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">put &#39;table_name&#39;,&#39;rowkey&#39;,&#39;cf_name:c_name&#39;,&#39;value&#39;</span><br></pre></td></tr></table></figure><h4 id="4、删除数据：delete-amp-deleteall"><a href="#4、删除数据：delete-amp-deleteall" class="headerlink" title="4、删除数据：delete &amp; deleteall"></a>4、删除数据：delete &amp; deleteall</h4><p>delete：删除指定的cell，使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delete &#39;table_name&#39;,&#39;rowkey&#39;,&#39;cf_name:c_name&#39;</span><br></pre></td></tr></table></figure><p>deleteall：删除指定行的所有cell，使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deleteall &#39;table_name&#39;,&#39;rowkey&#39;</span><br></pre></td></tr></table></figure><h4 id="5、查看表描述：describe"><a href="#5、查看表描述：describe" class="headerlink" title="5、查看表描述：describe"></a>5、查看表描述：describe</h4><p>describe可以获取表的描述信息，使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">describe &#39;table_name&#39;</span><br></pre></td></tr></table></figure><p>describe命令会返回指定表的状态(enable/disable)，列族信息，如图6所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cdn.winyter.cn/hadoop-hbase_image6.png" alt="图6 HBaseShell describe命令查询结果" title="">                </div>                <div class="image-caption">图6 HBaseShell describe命令查询结果</div>            </figure><h4 id="6、获取所有表名：list"><a href="#6、获取所有表名：list" class="headerlink" title="6、获取所有表名：list"></a>6、获取所有表名：list</h4><p>list命令能直接获取所有表名，使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list</span><br></pre></td></tr></table></figure><h4 id="7、创建表：create"><a href="#7、创建表：create" class="headerlink" title="7、创建表：create"></a>7、创建表：create</h4><p>使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create &#39;table_name&#39;,&#39;cf1&#39;,&#39;cf2&#39;,&#39;cf3&#39;</span><br></pre></td></tr></table></figure><h4 id="8、启动与停止表：enable-amp-disable"><a href="#8、启动与停止表：enable-amp-disable" class="headerlink" title="8、启动与停止表：enable &amp; disable"></a>8、启动与停止表：enable &amp; disable</h4><p>Hbase引入了表的启动与停止，顾名思义，即在不删除表和表数据的情况下，停用表，以及在不需要新建表和导入表数据的情况下，启用表，增加了数据仓库的灵活性和安全性<br>使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">enable &#39;table_name&#39; &amp; disable &#39;table_name&#39;</span><br></pre></td></tr></table></figure><p>但在现场环境，该组命令不常用</p><h4 id="9、删除表：drop"><a href="#9、删除表：drop" class="headerlink" title="9、删除表：drop"></a>9、删除表：drop</h4><p>Hbase里，表在删除之前，必须保证该表已经停止，否则删除不成功，保障数据安全，使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop &#39;table_name&#39;</span><br></pre></td></tr></table></figure><h4 id="10、更改表结构：alter"><a href="#10、更改表结构：alter" class="headerlink" title="10、更改表结构：alter"></a>10、更改表结构：alter</h4><p>更改表结构。可以通过alter命令增加、修改、删除列族信息以及表相关的参数值，使用方法举例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter &#39;test&#39;, &#123;NAME &#x3D;&gt; &#39;cf3&#39;, METHOD &#x3D;&gt; &#39;delete&#39;&#125;。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h2 id=&quot;HBase概念介绍&quot;&gt;&lt;a href=&quot;#HBase概念介绍&quot; class=&quot;headerlink&quot; title=&quot;HBase概念介绍&quot;&gt;&lt;/a&gt;HBase概念介绍&lt;/h2&gt;&lt;p&gt;HBase是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统。HBas
      
    
    </summary>
    
    
      <category term="CS Concept" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/"/>
    
      <category term="Bigdata" scheme="https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"/>
    
    
      <category term="HBase" scheme="https://winyter.github.io/MyBlog/tags/HBase/"/>
    
      <category term="Hadoop" scheme="https://winyter.github.io/MyBlog/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>网站推荐（持续更新哟）</title>
    <link href="https://winyter.github.io/MyBlog/2019/12/23/webpage-recommend/"/>
    <id>https://winyter.github.io/MyBlog/2019/12/23/webpage-recommend/</id>
    <published>2019-12-22T16:14:43.000Z</published>
    <updated>2020-04-19T07:51:58.930Z</updated>
    
    <content type="html"><![CDATA[<h1 id="网站推荐（持续更新哟）"><a href="#网站推荐（持续更新哟）" class="headerlink" title="网站推荐（持续更新哟）"></a>网站推荐（持续更新哟）</h1><pre><code>首先声明：本文推荐的所有网站，均是我多年来的心血收藏，当然以后我这个博客做的好的话，不排除帮真正好的网站推一推（如果帮推的网站我会注明的），但不管怎样，这里的推荐一定是纯公益的，也一定是符合我的推荐审美的：无毒、无诱导、页面清爽、内容积极的。同时，我也会在我的推荐语中，言明它的优点、缺点，以供大家选择。</code></pre><hr><h2 id="编程语言类"><a href="#编程语言类" class="headerlink" title="编程语言类"></a>编程语言类</h2><p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400" target="_blank" rel="noopener">廖雪峰的Python教程</a>：这是廖雪峰老师的Python教程，基于Python3，全面的讲解了Python的知识点，是很多人的Python启蒙教程。<br><a href="https://anaconda.org/" target="_blank" rel="noopener">Anaconda离线包下载</a>：Anaconda作为Python包管理工具，其官网集成了Python绝大部分包，如果你Anaconda所处的环境无法连接互联网，那么你可能需要使用这个网站，下载所需的包，然后在你的环境中使用“conda install”命令部署。</p><h2 id="云与大数据类"><a href="#云与大数据类" class="headerlink" title="云与大数据类"></a>云与大数据类</h2><p><a href="https://forum.huawei.com/enterprise/zh/forum-783.html" target="_blank" rel="noopener">华为大数据官方论坛</a> &amp; <a href="https://forum.huawei.com/enterprise/zh/forum-775.html" target="_blank" rel="noopener">华为云官方论坛</a>：这两个论坛算是与我工作相关，所以才关注了，华为FusionInsight大数据集群和FusionSphere云，算是国内比较出色的政企云，尤其在目前的政府领域，而这两个论坛是华为这个产品的官方论坛，其中的知识较为全面，能帮助有掌握华为大数据和云平台知识需求的童鞋快速上手，当然如果你本身的工作没有这方面的需求，这两个论坛对你价值可能不大，毕竟这两个论坛产品针对性很强，比较偏产品业务应用方向，而非通用知识技能。<br><a href="https://www.yiibai.com/hbase" target="_blank" rel="noopener">HBase教程</a>：来自民间的HBase教程(工具书)，优势在于没有高谈阔论，简单粗暴，直接面向日常使用，按照日常使用需求，给出相应的方法，同时，方法讲解也非常直接：第一步、第二步……，非常适合运维人员或者开发人员临场查询使用。如果你是HBase小白，甚至不知道HBase原理，看不懂HBase术语，那么我建议你先系统学习HBase知识后，再来阅读使用该教程。<br><a href="http://www.apache.org/" target="_blank" rel="noopener">Apache软件基金官网</a>：如果说南迦巴瓦是很多藏传佛教徒心中的圣地，那么这个网站，就是很多IT从业者心中的南迦巴瓦。用Apache官网的一句话来描述其地位，就是：“THE WORLD’S LARGEST OPEN SOURCE FOUNDATION”，你在点开这个网站后，可以拉到首页最底下，有其下辖管理运营的所有IT技术，其中包括：Tomcat、HTTP Server、Hadoop这些几乎家喻户晓的技术协议，不用疑惑，这些技术协议均由Apache基金会进行管理运营。尤其在大数据时代，整个Hadoop项目，是Apache顶级项目之一，也因此，这个网站有着全世界最官方的Hadoop技术资讯。当然，吹归吹，这个网站，缺点还是有的：内容分散(由于需要服务于各类使用Hadoop的人，所以官方文档写的大而全，导致你可能仅寻找一个命令，却花费了你数个小时)、全英文(这大概是你的问题，而不是网站的问题，哈哈)。如果你能解决英文网页的问题，那么这个网站将是你全面了解大数据最好的地方。</p><h2 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h2><p><a href="http://www.gpsspg.com/maps.htm" target="_blank" rel="noopener">在线经纬度查询</a>：这个网站针对性较强，受众比较窄，如果你发现你看不懂下面的文字，那么这个网站基本是不适合你的。这个网站用来对经纬度地址进行查询，并提供多种坐标系的转换，能把经纬度数据转换为地址，能解析你在地图上点击的地方的经纬度，同时，还支持批量经纬度转换查询功能。</p><h2 id="软件资源类"><a href="#软件资源类" class="headerlink" title="软件资源类"></a>软件资源类</h2><p>以下推荐的网站，都是无毒、无全家桶、无诱导下载、几乎无广告的网站，宗旨是方便大家，大家用的好，可以悄悄的收藏起来，如果你有一颗感恩的心，也希望你在觉得这个网站不错的时候，可以打个赏，支持一下博主，毕竟，这些几乎完全是公益的资源网站，背后还是需要较多运营费用的。</p><p><a href="http://www.dnbbn.com/" target="_blank" rel="noopener">达牛帮帮你</a>：资源分享博客，胜在免费、干净、无广告、无毒，目前支持的软件不多，但都是大家平时使用率极高的软件，所以还是很有用的，目前支持的软件：Office系列、Windows系列镜像、Adobe Ps Ai Pr系列、CAD Revit Maya 3D系列，思维脑图系列。但需要注意，这个网站提供的均是官方下载通道，且没有“小工具”哟，而且没有安装教程，部分安装较为复杂的软件，可能需要你自行上网搜索安装方法。适合乐于使用正版的童鞋或者装机大牛使用，如果你不想花钱买正版，也不会自己动手找资源破解，那这个网站可能不太适合你。</p><p><a href="http://www.carrotchou.blog/" target="_blank" rel="noopener">胡萝卜周博客</a>：资源分享博客，胜在软件多、教程多、小工具多、无广告、无毒、免费。如果你的软件比较偏门，你可以来这里试一试，你可以把这里当做华军、西西，但没有诱导下载、没有广告、没有病毒、没有全家桶，而且他家有微信公众号的，如果喜欢使用微信来获取软件或者装机方面的咨询的话，还是比较合适的关注的。同时，他家还特别的对软件所在的操作系统进行了分类，目前支持有Windows、macOS、Android三个平台，算是很好很贴心的一个功能了。但和华军西西一样，由于软件较多，所有你想要的软件你可能得去搜索一下，挑选一下，即使这个软件是家喻户晓的。</p><p><a href="http://vault.centos.org/" target="_blank" rel="noopener">CentOS系统包官方下载</a>：CentOS官网的下载目录，如果你需要看源码，或者需要离线安装，这是最合适的网站。</p><h2 id="人工智能-amp-机器学习类"><a href="#人工智能-amp-机器学习类" class="headerlink" title="人工智能 &amp; 机器学习类"></a>人工智能 &amp; 机器学习类</h2><h2 id="算法类"><a href="#算法类" class="headerlink" title="算法类"></a>算法类</h2><p><a href="https://www.techiedelight.com/top-algorithms-data-structures-concepts-computer-science/" target="_blank" rel="noopener">必备算法</a>：这篇文章名为：”Top Algorithms/Data Structures/Concepts every computer science student should know”，是必备算法的合集。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;网站推荐（持续更新哟）&quot;&gt;&lt;a href=&quot;#网站推荐（持续更新哟）&quot; class=&quot;headerlink&quot; title=&quot;网站推荐（持续更新哟）&quot;&gt;&lt;/a&gt;网站推荐（持续更新哟）&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;首先声明：本文推荐的所有网站，均是我多年来的心血收藏
      
    
    </summary>
    
    
      <category term="Source" scheme="https://winyter.github.io/MyBlog/categories/Source/"/>
    
    
  </entry>
  
  <entry>
    <title>PostgreSQL表间复制数据</title>
    <link href="https://winyter.github.io/MyBlog/2019/12/23/postgreSQL-copy-data-between-two-tables/"/>
    <id>https://winyter.github.io/MyBlog/2019/12/23/postgreSQL-copy-data-between-two-tables/</id>
    <published>2019-12-22T16:08:01.000Z</published>
    <updated>2020-04-19T07:50:34.144Z</updated>
    
    <content type="html"><![CDATA[<p>表间复制即从源表复制到目标表，根据目标表是否存在，分为两种语句：</p><h3 id="1、目标表不存在（即目标表需要新建）的情况"><a href="#1、目标表不存在（即目标表需要新建）的情况" class="headerlink" title="1、目标表不存在（即目标表需要新建）的情况"></a>1、目标表不存在（即目标表需要新建）的情况</h3><p>语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT column1, column2, ... into Table2 from Table1</span><br></pre></td></tr></table></figure><p>该语句将数据从Table1复制到Table2，前提是Table2不存在，语句执行时会自动创建Table2，Table2相关字段的约束规则会继承Table1。另外，该语句还支持选择Table1中需要复制的字段。</p><h3 id="2、目标表已存在的情况"><a href="#2、目标表已存在的情况" class="headerlink" title="2、目标表已存在的情况"></a>2、目标表已存在的情况</h3><p>语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Insert into Table2(field1,field2,…) select value1,value2,… from Table1</span><br></pre></td></tr></table></figure><p>该语句将Table1的数据复制到Table2，Table1和Table2均支持自选字段，只需要value字段和field字段一一对应即可，但需要注意字段约束规则，如果对应字段的约束规则冲突，语句运行可能失败。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;表间复制即从源表复制到目标表，根据目标表是否存在，分为两种语句：&lt;/p&gt;
&lt;h3 id=&quot;1、目标表不存在（即目标表需要新建）的情况&quot;&gt;&lt;a href=&quot;#1、目标表不存在（即目标表需要新建）的情况&quot; class=&quot;headerlink&quot; title=&quot;1、目标表不存在（即
      
    
    </summary>
    
    
      <category term="CS Soft" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/"/>
    
      <category term="Server" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"/>
    
      <category term="Postgre" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Postgre/"/>
    
    
      <category term="Postgre" scheme="https://winyter.github.io/MyBlog/tags/Postgre/"/>
    
      <category term="SQL" scheme="https://winyter.github.io/MyBlog/tags/SQL/"/>
    
      <category term="Database" scheme="https://winyter.github.io/MyBlog/tags/Database/"/>
    
  </entry>
  
  <entry>
    <title>Shell脚本实现FTP文件增量传输</title>
    <link href="https://winyter.github.io/MyBlog/2019/12/19/ftp-increment-download-with-shell/"/>
    <id>https://winyter.github.io/MyBlog/2019/12/19/ftp-increment-download-with-shell/</id>
    <published>2019-12-19T11:29:51.000Z</published>
    <updated>2020-04-19T07:46:35.366Z</updated>
    
    <content type="html"><![CDATA[<p>#Shell脚本实现FTP文件增量传输</p><hr><p>环境：CentOS 7.3<br>使用语言：shell<br>From：<a href="https://blog.csdn.net/qq_34485930/article/details/79916169" target="_blank" rel="noopener">shell脚本中的实时ftp传输实例</a></p><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>该脚本基本思路是定时读取FTP远程目录上的文件名列表，然后将本次列表与上次列表对比，获取增量文件列表，然后按照此列表下载文件。</p><h2 id="获取当前文件列表"><a href="#获取当前文件列表" class="headerlink" title="获取当前文件列表"></a>获取当前文件列表</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">cd &lt;本机目录&gt;  # 进入本机目录，你需要指定一个目录，用来存放文件列表记录</span><br><span class="line"></span><br><span class="line">ftp -n -v &lt;远程FTP IP地址&gt; &lt;&lt;!</span><br><span class="line">   user &lt;用户名&gt; &lt;密码&gt;</span><br><span class="line">   prompt off    # 关闭交互模式</span><br><span class="line">   nlist &lt;文件名正则表达式&gt; list_origin.txt    # 获取本次文件列表，生成list_origin.txt文件</span><br><span class="line">   close</span><br><span class="line">   bye</span><br><span class="line">!</span><br></pre></td></tr></table></figure><p>这时，在本机目录下，会产生一个<code>list_origin.txt</code>文件，这个文件就是本次运行读取的当前文件列表。</p><h2 id="对比文件列表"><a href="#对比文件列表" class="headerlink" title="对比文件列表"></a>对比文件列表</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将当前文件列表与前次文件列表比对，获得增量文件列表</span></span><br><span class="line">comm -3 list_origin.txt list_last.txt &gt; list_mid.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 统计增量文件数量</span></span><br><span class="line">num="`cat list_mid.txt | wc -l`"</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将本次文件列表以覆盖方式写入前次文件列表</span></span><br><span class="line">cat list_origin.txt &gt; list_last.txt</span><br></pre></td></tr></table></figure><p>这个步骤就是增量传输思路的核心，利用<code>comm</code>命令，将本次列表与前次列表比对的结果写入增量列表，供下一步FTP下载文件使用</p><h2 id="下载增量文件"><a href="#下载增量文件" class="headerlink" title="下载增量文件"></a>下载增量文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">for ((i = 1; i &lt;= $num; i++));</span><br><span class="line">do</span><br><span class="line">ftp -n -v &lt;远程FTP IP地址&gt; &lt;&lt;!</span><br><span class="line">   user &lt;用户名&gt; &lt;密码&gt;</span><br><span class="line">   binary  # 使用二进制传输</span><br><span class="line">   passive  # 进入被动传输</span><br><span class="line">   prompt off  # 关闭交互模式</span><br><span class="line">   cd &lt;远程文件目录&gt;</span><br><span class="line">   lcd &lt;本地数据文件存储目录&gt;    # 这个目录可以和上面的“本地目录”不一样，且建议不一样</span><br><span class="line">   get `cat list_mid.txt | tail -n +$i | head -n 1`</span><br><span class="line">   close</span><br><span class="line">   bye</span><br><span class="line">!</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>这段代码即是实时下载文件，使用<code>for</code>循环，逐行下载增量文件列表中的文件，至此，就完成了增量传输脚本的所有内容。</p><h2 id="设置定时任务"><a href="#设置定时任务" class="headerlink" title="设置定时任务"></a>设置定时任务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crontab -e</span><br></pre></td></tr></table></figure><p>写入定时任务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*&#x2F;5 * * * * nohup &#x2F;bin&#x2F;bash &lt;脚本路径&gt; &gt;&gt; &lt;脚本日志路径&gt; 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个脚本简单粗暴，适用于非严谨场合下的快速部署。该脚本稳定性较好，实测生产环境一年的运行中，未出现问题。但由于使用<code>for</code>循环，每次下载一个文件都需要打开关闭一次FTP，所以，这个脚本性能一般，而且日志生成量较大，大量文件传输的情况下，需要再搭配日志清除的定时任务，基于此，也建议设置脚本执行的定时任务时，定时时间间隔尽可能小一些。另外，三个文件列表文件，尤其是<code>list_last.txt</code>这个前次列表文件，切勿删除，否则，将会丢失所有增量传输进度。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;#Shell脚本实现FTP文件增量传输&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;环境：CentOS 7.3&lt;br&gt;使用语言：shell&lt;br&gt;From：&lt;a href=&quot;https://blog.csdn.net/qq_34485930/article/details/79916169&quot; 
      
    
    </summary>
    
    
      <category term="CS Soft" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/"/>
    
      <category term="Language" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/Language/"/>
    
      <category term="Shell" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/Language/Shell/"/>
    
    
      <category term="Shell" scheme="https://winyter.github.io/MyBlog/tags/Shell/"/>
    
      <category term="FTP" scheme="https://winyter.github.io/MyBlog/tags/FTP/"/>
    
      <category term="Code" scheme="https://winyter.github.io/MyBlog/tags/Code/"/>
    
  </entry>
  
  <entry>
    <title>Nginx快速部署指南</title>
    <link href="https://winyter.github.io/MyBlog/2019/12/18/Nginx-install-guide/"/>
    <id>https://winyter.github.io/MyBlog/2019/12/18/Nginx-install-guide/</id>
    <published>2019-12-18T10:04:44.000Z</published>
    <updated>2020-04-19T07:43:46.092Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Nginx快速部署指南"><a href="#Nginx快速部署指南" class="headerlink" title="Nginx快速部署指南"></a>Nginx快速部署指南</h1><p>转自：<a href="https://www.runoob.com/linux/nginx-install-setup.html" target="_blank" rel="noopener">菜鸟教程</a>  （在其基础上做了一些修改）</p><h2 id="什么是Nginx"><a href="#什么是Nginx" class="headerlink" title="什么是Nginx"></a>什么是Nginx</h2><p>Nginx(“engine x”)是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理 服务器，也是一个 IMAP/POP3/SMTP 代理服务器。</p><p>在高连接并发的情况下，Nginx是Apache服务器不错的替代品。</p><h2 id="Nginx安装"><a href="#Nginx安装" class="headerlink" title="Nginx安装"></a>Nginx安装</h2><p>系统平台：CentOS 7.3 x86_64</p><h3 id="一、安装编译工具及库文件"><a href="#一、安装编译工具及库文件" class="headerlink" title="一、安装编译工具及库文件"></a>一、安装编译工具及库文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install make zlib zlib-devel gcc-c++ libtool  openssl openssl-devel</span><br></pre></td></tr></table></figure><h3 id="二、新建Nginx用户"><a href="#二、新建Nginx用户" class="headerlink" title="二、新建Nginx用户"></a>二、新建Nginx用户</h3><p>基于环境安全，不建议使用root用户部署Nginx，尤其是生产环境。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">useradd -g users nginx -m -d &#x2F;home&#x2F;nginx</span><br><span class="line">passwd nginx   # 设置nginx用户密码</span><br><span class="line">su - nginx</span><br></pre></td></tr></table></figure><h3 id="三、安装PCRE"><a href="#三、安装PCRE" class="headerlink" title="三、安装PCRE"></a>三、安装PCRE</h3><p>PCRE 作用是让 Nginx 支持 Rewrite 功能。</p><ol><li>下载PCRE安装包：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;downloads.sourceforge.net&#x2F;project&#x2F;pcre&#x2F;pcre&#x2F;8.35&#x2F;pcre-8.35.tar.gz</span><br></pre></td></tr></table></figure></li><li>解压安装包：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf pcre-8.35.tar.gz</span><br></pre></td></tr></table></figure></li><li>进入安装包目录：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd pcre-8.35</span><br></pre></td></tr></table></figure></li><li>编译安装<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;configure</span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure></li><li>安装完成，查看pcre版本：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pcre-config --version</span><br></pre></td></tr></table></figure></li></ol><h3 id="四、安装Nginx"><a href="#四、安装Nginx" class="headerlink" title="四、安装Nginx"></a>四、安装Nginx</h3><ol><li>下载Nignx<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;nginx.org&#x2F;download&#x2F;nginx-1.6.2.tar.gz</span><br></pre></td></tr></table></figure></li><li>解压安装包<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf nginx-1.6.2.tar.gz</span><br></pre></td></tr></table></figure></li><li>进入安装包目录<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd nginx-1.6.2</span><br></pre></td></tr></table></figure></li><li>编译安装<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;configure --prefix&#x3D;&#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx --with-http_stub_status_module --with-http_ssl_module --with-pcre&#x3D;&#x2F;home&#x2F;nginx&#x2F;pcre-8.35</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure></li><li>查看Nginx版本<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;sbin&#x2F;nginx -v</span><br></pre></td></tr></table></figure>至此，Nginx安装完成</li></ol><h3 id="Nginx配置"><a href="#Nginx配置" class="headerlink" title="Nginx配置"></a>Nginx配置</h3><ol><li>配置<code>nginx.conf</code>，将<code>/home/nginx/webserver/nginx/conf/nginx.conf</code>替换为以下内容:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">user nginx users;  # user &lt;username&gt; &lt;groupname&gt;</span><br><span class="line">worker_processes 2; #设置值和CPU核心数一致</span><br><span class="line">error_log &#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;logs&#x2F;nginx_error.log crit; #日志位置和日志级别</span><br><span class="line">pid &#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;nginx.pid;</span><br><span class="line">#Specifies the value for maximum file descriptors that can be opened by this process.</span><br><span class="line">worker_rlimit_nofile 65535;</span><br><span class="line">events</span><br><span class="line">&#123;</span><br><span class="line">  use epoll;</span><br><span class="line">  worker_connections 65535;</span><br><span class="line">&#125;</span><br><span class="line">http</span><br><span class="line">&#123;</span><br><span class="line">  include mime.types;</span><br><span class="line">  default_type application&#x2F;octet-stream;</span><br><span class="line">  log_format main  &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39;</span><br><span class="line">               &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39;</span><br><span class="line">               &#39;&quot;$http_user_agent&quot; $http_x_forwarded_for&#39;;</span><br><span class="line">  </span><br><span class="line">#charset gb2312;</span><br><span class="line">     </span><br><span class="line">  server_names_hash_bucket_size 128;</span><br><span class="line">  client_header_buffer_size 32k;</span><br><span class="line">  large_client_header_buffers 4 32k;</span><br><span class="line">  client_max_body_size 8m;</span><br><span class="line">     </span><br><span class="line">  sendfile on;</span><br><span class="line">  tcp_nopush on;</span><br><span class="line">  keepalive_timeout 60;</span><br><span class="line">  tcp_nodelay on;</span><br><span class="line">  fastcgi_connect_timeout 300;</span><br><span class="line">  fastcgi_send_timeout 300;</span><br><span class="line">  fastcgi_read_timeout 300;</span><br><span class="line">  fastcgi_buffer_size 64k;</span><br><span class="line">  fastcgi_buffers 4 64k;</span><br><span class="line">  fastcgi_busy_buffers_size 128k;</span><br><span class="line">  fastcgi_temp_file_write_size 128k;</span><br><span class="line">  gzip on; </span><br><span class="line">  gzip_min_length 1k;</span><br><span class="line">  gzip_buffers 4 16k;</span><br><span class="line">  gzip_http_version 1.0;</span><br><span class="line">  gzip_comp_level 2;</span><br><span class="line">  gzip_types text&#x2F;plain application&#x2F;x-javascript text&#x2F;css application&#x2F;xml;</span><br><span class="line">  gzip_vary on;</span><br><span class="line"> </span><br><span class="line">  #limit_zone crawler $binary_remote_addr 10m;</span><br><span class="line"> #下面是server虚拟主机的配置</span><br><span class="line"> server</span><br><span class="line">  &#123;</span><br><span class="line">    listen 80;#监听端口</span><br><span class="line">    server_name localhost;#域名</span><br><span class="line">    index index.html index.htm index.php;</span><br><span class="line">    root &#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;html;#站点目录</span><br><span class="line">      location ~ .*\.(php|php5)?$</span><br><span class="line">    &#123;</span><br><span class="line">      #fastcgi_pass unix:&#x2F;tmp&#x2F;php-cgi.sock;</span><br><span class="line">      fastcgi_pass 127.0.0.1:9000;</span><br><span class="line">      fastcgi_index index.php;</span><br><span class="line">      include fastcgi.conf;</span><br><span class="line">    &#125;</span><br><span class="line">    location ~ .*\.(gif|jpg|jpeg|png|bmp|swf|ico)$</span><br><span class="line">    &#123;</span><br><span class="line">      expires 30d;</span><br><span class="line">  # access_log off;</span><br><span class="line">    &#125;</span><br><span class="line">    location ~ .*\.(js|css)?$</span><br><span class="line">    &#123;</span><br><span class="line">      expires 15d;</span><br><span class="line">   # access_log off;</span><br><span class="line">    &#125;</span><br><span class="line">    access_log off;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>检查配置文件nginx.conf的正确性：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;sbin&#x2F;nginx -t</span><br></pre></td></tr></table></figure></li></ol><h3 id="启动Nginx"><a href="#启动Nginx" class="headerlink" title="启动Nginx"></a>启动Nginx</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;sbin&#x2F;nginx</span><br></pre></td></tr></table></figure><h3 id="访问站点"><a href="#访问站点" class="headerlink" title="访问站点"></a>访问站点</h3><p>从浏览器访问我们配置的站点IP，如果出现了：<code>Welcome to nginx</code>。就表示部署成功。</p><h3 id="Nginx其他常用命令"><a href="#Nginx其他常用命令" class="headerlink" title="Nginx其他常用命令"></a>Nginx其他常用命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;sbin&#x2F;nginx -s reload            # 重新载入配置文件</span><br><span class="line">&#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;sbin&#x2F;nginx -s reopen            # 重启 Nginx</span><br><span class="line">&#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;sbin&#x2F;nginx -s stop              # 停止 Nginx</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Nginx快速部署指南&quot;&gt;&lt;a href=&quot;#Nginx快速部署指南&quot; class=&quot;headerlink&quot; title=&quot;Nginx快速部署指南&quot;&gt;&lt;/a&gt;Nginx快速部署指南&lt;/h1&gt;&lt;p&gt;转自：&lt;a href=&quot;https://www.runoob.com
      
    
    </summary>
    
    
      <category term="CS Soft" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/"/>
    
      <category term="Server" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"/>
    
      <category term="Nginx" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Nginx/"/>
    
    
      <category term="Tools" scheme="https://winyter.github.io/MyBlog/tags/Tools/"/>
    
      <category term="Nginx" scheme="https://winyter.github.io/MyBlog/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>Hexo部署指南</title>
    <link href="https://winyter.github.io/MyBlog/2019/12/16/Hexo_install_guide/"/>
    <id>https://winyter.github.io/MyBlog/2019/12/16/Hexo_install_guide/</id>
    <published>2019-12-16T11:25:37.000Z</published>
    <updated>2020-04-19T07:42:33.634Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hexo部署指南"><a href="#Hexo部署指南" class="headerlink" title="Hexo部署指南"></a>Hexo部署指南</h1><hr><h2 id="什么是Hexo"><a href="#什么是Hexo" class="headerlink" title="什么是Hexo"></a>什么是Hexo</h2><p>“Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。”</p><p>这段话，是Hexo中文官网中关于什么是Hexo的解释。</p><p>实际上，Hexo首先是一个博客框架，和WordPress等博客框架是一样的作用——给用户写博客，而使用博客框架，可以免除我们每写一篇文章还需要自己手动构建一个网页，这些博客框架能够自动将你写成的文章转化为一个网页。</p><p>这些博客框架之间的区别，就在于使用的技术、呈现的效果上有所区别，Hexo采用了Markdown解析文章（也可以更换其他渲染引擎，但非常不建议），Hexo主打简洁、扁平的现代网页风格，所以，这也吸引了越来越多的人，使用Hexo构建自己的博客。</p><p>作为一个没使用过其他博客框架的人，我也没有能力点评这些博客框架的优劣，但对我来说，选择哪个框架只是其次，重要的，是我写在其中的文章和知识，是否足够优秀，足够出彩，而这也希望给正在纠结选择哪个博客框架的你们一点idea。</p><h2 id="写在动手前"><a href="#写在动手前" class="headerlink" title="写在动手前"></a>写在动手前</h2><p>下面，我会开始讲述如何一步一步从0开始，搭建自己的博客平台，并让她帮助你分享你的心情。</p><p>你可以打开这篇文章，按照文章中的步骤，一边阅读，一边部署，而基于这样的阅读方式，在开始正式动手前，我希望你能确定几件事情：</p><ol><li>你是否有一个GitHub账户，并且有GitHub基础的操作知识，Hexo本身以及很多Hexo主题的开发者，都把GitHub作为他们源代码的分享地，为了顺利的使用Hexo，你可能需要在使用Hexo之前，建立一个GitHub账户，并且掌握其基础的操作。由于本文主要讲述Hexo的使用，关于GitHub的使用，可能需要你通过下面的两个链接来了解。</li></ol><ul><li><a href="http://github.com" target="_blank" rel="noopener">GitHub官网地址</a>：你可以在这里注册、登录、使用GitHub。</li><li><a href="https://www.liaoxuefeng.com/wiki/896043488029600" target="_blank" rel="noopener">GitHub教程</a>：这是廖雪峰老师的GitHub教程，非常详实，也非常好用，你可以在这里学到几乎所有关于GitHub的基础知识。</li></ul><ol start="2"><li>你是否拥有一个线上环境（云服务器、连接了互联网的服务器……），这并不是必须的，Hexo可以部署在GitHub上，但如果你出于各种原因，想要部署在自己的线上环境中，那么你可能需要“整”一个线上环境，本文中会有线上环境的部署步骤，但是如何获取并搭建一个线上环境，这需要你自己的努力。</li><li>你是想在 Windows / Linux / MacOS 上部署你的本地或线上环境？本文会涉及Linux和MacOS操作系统中的部署（Windows会在后期补充上），你在阅读时，可能需要按照你的操作系统，选取相应的步骤执行，否则，你的部署会遇到问题。</li></ol><p>最后，需要声明，本文灵感来源于诸多网上公开博客和网址，在这里感谢各位大牛的分享，下面会放出部分链接，文中如果有侵犯了您版权的地方，可以与我联系，我会在核实后第一时间更改。</p><ul><li><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">Hexo官方文档(中文)</a>：这是Hexo官方的中文文档，Hexo的中文文档写的比较好，可以直接阅读中文文档，如果你需要阅读英文文档，可以<a href="https://hexo.io/docs/" target="_blank" rel="noopener">点击这里</a>。</li><li><a href="https://www.jianshu.com/p/1c888a6b8297?utm_source=oschina-app" target="_blank" rel="noopener">https://www.jianshu.com/p/1c888a6b8297?utm_source=oschina-app</a></li><li><a href="https://blog.csdn.net/u010075335/article/details/82861322" target="_blank" rel="noopener">https://blog.csdn.net/u010075335/article/details/82861322</a></li></ul><h2 id="本地环境部署"><a href="#本地环境部署" class="headerlink" title="本地环境部署"></a>本地环境部署</h2><p>Hexo需要部署一个本地环境，这允许你在这个环境中进行调试、修改，直到页面的展示效果满足你上线的要求。</p><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><ol><li>Nodejs (官方建议使用 Node.js 10.0 及以上版本)</li><li>Git<h4 id="Nodejs安装"><a href="#Nodejs安装" class="headerlink" title="Nodejs安装"></a>Nodejs安装</h4>Mac用户：<br>推荐直接在官网下载最新LTS安装包直接安装，如果有需要，也可以使用官网推荐的<a href="https://brew.sh/" target="_blank" rel="noopener">Homebrew</a>和<a href="https://www.macports.org/" target="_blank" rel="noopener">MacPorts</a>安装</li></ol><p>Linux用户：推荐使用Source Code方式安装，下面也会讲解Source Code安装步骤：</p><ul><li>前往<a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">官网</a>，下载最新的LTS for Linux Source Code 包。或者使用wget获取安装包：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;npm.taobao.org&#x2F;mirrors&#x2F;node&#x2F;v10.14.1&#x2F;node-v10.14.1-linux-x64.tar.gz</span><br></pre></td></tr></table></figure></li><li>然后放到你想安装的文件夹中。</li><li>解压安装包：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf  node-v8.0.0-linux-x64.tar.gz</span><br></pre></td></tr></table></figure></li><li>配置环境变量。需要注意：你如果需要配置全局环境变量，需要修改系统的环境变量文件：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure>如果只想在本用户下(非root用户)安装，则只需修改用户根目录下的环境变量文件，下面以CentOS为例 (其他操作系统的环境变量文件，可以自行查询) ：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~&#x2F;.bash_profile</span><br></pre></td></tr></table></figure></li><li>在文件末尾添加：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export NODE_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;node  </span><br><span class="line">export PATH&#x3D;$NODE_HOME&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure></li><li>生效配置文件：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~&#x2F;.bash_profile</span><br></pre></td></tr></table></figure></li><li>测试安装效果：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm -v</span><br><span class="line">node -v</span><br></pre></td></tr></table></figure>nodejs和npm的安装有很多方法，上面写的方法不一定是你最喜欢的，也不一定是最方便的，但是最通用的，由于Linux Distributions较多，每个发布版本安装软件的方式不尽相同，如果有需要，你可以自行网上搜索安装方式。<h4 id="Git的安装"><a href="#Git的安装" class="headerlink" title="Git的安装"></a>Git的安装</h4>Git需要在本地安装一个客户端，详细的安装方法不再赘述，可以参照<a href="https://www.liaoxuefeng.com/wiki/896043488029600/896067074338496" target="_blank" rel="noopener">廖雪峰老师的教程</a>，亦可参照<a href="https://hexo.io/zh-cn/docs/index.html#%E4%BB%80%E4%B9%88%E6%98%AF-Hexo%EF%BC%9F" target="_blank" rel="noopener">Hexo官方文档</a>中的安装方法，当然，你也可以自行上网搜索。</li></ul><h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><p>Linux与MacOS都适用于以下的安装步骤。</p><ul><li><p>全局安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure></li><li><p>仅当前用户下安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &#39;PATH&#x3D;&quot;$PATH:.&#x2F;node_modules&#x2F;.bin&quot;&#39; &gt;&gt; ~&#x2F;.profile</span><br></pre></td></tr></table></figure><p>需要注意，上面的~/.profile文件不一定适用于所有系统，请视自己系统内的环境变量文件而定。</p><h3 id="建站"><a href="#建站" class="headerlink" title="建站"></a>建站</h3><p>安装完成后，你需要选择部署Hexo本地站点的文件夹，执行：</p><p>  hexo init <folder><br>  cd <folder><br>  npm install</p></li></ul><p>完成后，<code>folder</code>文件夹下，会有如下目录结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml</span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds</span><br><span class="line">├── source</span><br><span class="line">|   ├── _drafts</span><br><span class="line">|   └── _posts</span><br><span class="line">└── themes</span><br></pre></td></tr></table></figure><h4 id="config-yml"><a href="#config-yml" class="headerlink" title="_config.yml"></a>_config.yml</h4><p>这是网站的配置文件，绝大部分配置均可在这个文件中设置，但需要注意，如果你使用了第三方的主题，部分主题下，也会有<code>_config.yml</code>文件，这时，这两个同名文件是有区别的，根目录下的<code>_config.yml</code>文件是全局配置文件，服务于整个网站，而主题下的<code>_config.yml</code>文件仅用来设置本主题下的一些配置，仅服务于该主题，而且这个配置文件内的配置项，基本不会和全局<code>_config.yml</code>文件中的配置项重合。</p><h4 id="package-json"><a href="#package-json" class="headerlink" title="package.json"></a>package.json</h4><p>应用程序的信息。非Hexo高级操作，不需要修改这个文件，如果需要修改，可以参考官方文档和网上资料。</p><h4 id="scaffolds"><a href="#scaffolds" class="headerlink" title="scaffolds"></a>scaffolds</h4><p>“模版 文件夹。当您新建文章时，Hexo 会根据 scaffold 来建立文件。</p><p>Hexo的模板是指在新建的文章文件中默认填充的内容。例如，如果您修改scaffold/post.md中的Front-matter内容，那么每次新建一篇文章时都会包含这个修改。”</p><p>以上是官方文档中对该文件夹作用的解释，如非高级操作，该文件夹一般也不需要打开进行修改。</p><h4 id="source"><a href="#source" class="headerlink" title="source"></a>source</h4><p>存放用户资源的地方。一般默认情况下，你写的文章在发布之前，都会存在在这个文件夹下的<code>_post</code>目录中，当你发布网站时，Hexo会根据<code>source</code>目录中的相关资源，生成静态页面，存放到<code>public</code>文件夹下，供页面展示。</p><h4 id="themes"><a href="#themes" class="headerlink" title="themes"></a>themes</h4><p>主题文件夹。Hexo 会根据主题来生成静态页面。这个文件夹也是你在下载了第三方Hexo主题后，这些文件的存放处。</p><h3 id="选择主题"><a href="#选择主题" class="headerlink" title="选择主题"></a>选择主题</h3><p>Hexo有丰富的主题提供选择，如果你有需要，请点击<a href="https://hexo.io/themes/" target="_blank" rel="noopener">官网主题页面</a>，进行挑选。</p><p>选择完成后，下载这个主题的所有文件，将这些文件，存放到<code>themes</code>文件夹下。</p><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>在选择完主题后，你需要配置你的网站和主题，以更好的服务于你，这时候，你主要会对全局<code>_config.yml</code>文件和主题的<code>_config.yml</code>文件进行修改。</p><h4 id="全局-config-yml修改方法"><a href="#全局-config-yml修改方法" class="headerlink" title="全局_config.yml修改方法"></a>全局<code>_config.yml</code>修改方法</h4><p>全局<code>_config.yml</code>文件的各个配置项，在官网都有详细的说明，建议先通读官方文档中关于配置项的说明后，再着手开始配置，这里给出<a href="https://hexo.io/zh-cn/docs/configuration" target="_blank" rel="noopener">文档链接</a>，本文中就不再过多赘述，仅简单讲解一些关键配置。<br><code>language</code> ：网站语言配置项，该项与你选择的主题有密切关系，例如，如果你的主题不支持中文，那么你将不能使你的网站显示中文。配置此项的时候建议你进入主题的语言资源目录（一般在你主题根目录下的<code>language</code>文件夹下），查看支持的语言文件，再进行该项的配置。<br><code>url</code> &amp; <code>root</code>：这两项是息息相关的，但在本地站点部署阶段，这两个配置不需要进行配置，因为它们仅与线上站点有关，此处不详细讲解这两个配置，后面在讲解线上站点配置时，会进行说明。</p><h4 id="主题-config-yml修改方法"><a href="#主题-config-yml修改方法" class="headerlink" title="主题_config.yml修改方法"></a>主题<code>_config.yml</code>修改方法</h4><p>你下载的主题中，基本都会带有一个<code>Readme</code>文件，你需要详细阅读该文件，该文件中一般会注明主题的<code>_config.yml</code>文件如何配置，由于每个主题的配置方法，不尽相同，这里不再进行讲解。需要注意，部分主题可能会需要你根据需求特殊修改全局<code>_config.yml</code>文件，请根据<code>Readme</code>指示，进行操作。</p><h3 id="发布本地站点"><a href="#发布本地站点" class="headerlink" title="发布本地站点"></a>发布本地站点</h3><p>在你配置完你的站点后，你需要在本地发布你的站点，发布的过程就是Hexo根据你的主题、资源自动生成静态页面，供你查看。<br>发布本地站点你需要执行：</p><pre><code>hexo generate</code></pre><p>在官方文档中，对这个命令解释为生成静态文件，我在此处将其理解为发布本地站点，这并没有冲突。</p><h3 id="启动本地服务"><a href="#启动本地服务" class="headerlink" title="启动本地服务"></a>启动本地服务</h3><p>这是为了你能在本地的浏览器中查看你的站点，这能方便的让你对站点进行调试、操作，而不会影响线上站点。</p><pre><code>hexo server</code></pre><p>启动服务器。默认情况下，访问网址为： <code>http://localhost:4000/</code>。这时，你就可以通过这个网址，访问你的本地站点了。</p><h2 id="线上环境部署"><a href="#线上环境部署" class="headerlink" title="线上环境部署"></a>线上环境部署</h2><p>当你完成了你本地站点的调试后，你就需要开始部署你的线上站点，毕竟，开通博客，最终是为了分享。</p><h3 id="选择线上平台"><a href="#选择线上平台" class="headerlink" title="选择线上平台"></a>选择线上平台</h3><p>首先，你需要选择线上平台，目前主流有两种方式：<code>GitHub部署</code>和<code>自建站点部署</code>，本文也会对这两种主流方式的部署进行讲解。</p><h3 id="GitHub部署"><a href="#GitHub部署" class="headerlink" title="GitHub部署"></a>GitHub部署</h3><p>GitHub提供了一个名为GitHub Pages的网页服务，它允许每个GitHub账户能够部署一个基于GitHub的站点服务，每个账户仅能部署一个站点，且所有站点源代码会被公开，当然，仅能提供基础的访问量服务，但好处在于完全免费，这一特性使得这个服务很适合部署个人博客站点。而本文也将带你一起，学习如何在GitHub上免费部署你的Hexo站点。<br>需要注意，官方文档中，建议使用<code>Travis CI</code>进行GitHub部署，但实际上大可不必如此麻烦。</p><h4 id="第一步：新建Repository"><a href="#第一步：新建Repository" class="headerlink" title="第一步：新建Repository"></a>第一步：新建Repository</h4><p>你需要在你的GitHub上，新建一个Repository，在命名时，有两种选择方式，这两种方式取决于你喜欢哪种URL。</p><p>如果你喜欢形如：<code>https://yourname.github.io</code>，那么，你需要将这个Repo的名称命名为：<code>&lt;your_github_name&gt;.github.io</code>，请务必保证你的命名没有错误字符，否则站点无法部署成功。</p><p>如果你喜欢形如：<code>https://yourname.github.io/xxx</code>，那么，你可以对这个Repo任意命名，最后只需要在<code>https://yourname.github.io</code>后面加上你命名的名称即可。（官网中，将这个方法叫做<a href="https://hexo.io/zh-cn/docs/github-pages#Project-page" target="_blank" rel="noopener">Project page</a>）</p><h4 id="第二步：配置全局-config-yml文件"><a href="#第二步：配置全局-config-yml文件" class="headerlink" title="第二步：配置全局_config.yml文件"></a>第二步：配置全局_config.yml文件</h4><p>如果你采用了上面的第一个Repo命名方式，那么，你需要确保以下配置：</p><pre><code>url: https://&lt;your_github_name&gt;.github.io/root: /</code></pre><p>如果你采用了第二种Repo命名方式，那么，你需要确保以下配置：</p><pre><code>url: https://&lt;your_github_name&gt;.github.io/&lt;your_repo_name&gt;root: /&lt;your_repo_name&gt;</code></pre><p>接着，你需要修改以下配置项，从而使你能直接使用Hexo命令，将站点部署到GitHub上：</p><pre><code>deploy:  type: git  repo: &lt;your_repo_url&gt;  branch: &lt;your_branch_name&gt;</code></pre><p>repo的链接地址你可以使用<code>ssh</code>形式或者<code>https</code>形式，在你repo页面中的左上角，绿色的<code>clone or download</code>按钮中可以查看地址。</p><h4 id="第三步：上线站点"><a href="#第三步：上线站点" class="headerlink" title="第三步：上线站点"></a>第三步：上线站点</h4><p>在上线站点前，首先需要确保你的本地GitHub仓库已经建好。<br>配置GitHub仓库：</p><pre><code>cd &lt;your_hexo_dir&gt;git init git remote add origin &lt;your_repo_url&gt;</code></pre><p>检查git省略文件，需要保证public不会被上传到GitHub：</p><pre><code>vi .gitignore</code></pre><p>上线站点：</p><pre><code>hexo deploy</code></pre><p>此时，如果之前的操作均没有问题的话，稍等几分钟，你就能访问你的线上站点了。</p><h3 id="自建站点部署"><a href="#自建站点部署" class="headerlink" title="自建站点部署"></a>自建站点部署</h3><h4 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h4><p>你需要拥有一个连接了互联网的服务器，具有独立的IP地址。可以租用一个云服务器。</p><p>本文使用了Nginx作为代理服务，如果你想使用其他工具，可以参考<a href="https://hexo.io/zh-cn/docs/one-command-deployment" target="_blank" rel="noopener">官方文档</a>或者上网搜索。</p><h4 id="安装Nginx"><a href="#安装Nginx" class="headerlink" title="安装Nginx"></a>安装Nginx</h4><p><a href="http://176.122.163.119/2019/12/18/Nginx-install-guide/" target="_blank" rel="noopener">点击这里</a>，查看Nginx快速部署指南。<br>需要注意，如果你给你的服务器配置了域名，你需要修改<code>nginx.conf</code>文件中的以下内容：</p><pre><code>server_name localhost;     #域名</code></pre><p>将localhost修改为你的域名。</p><h4 id="配置本地站点"><a href="#配置本地站点" class="headerlink" title="配置本地站点"></a>配置本地站点</h4><p>进入站点目录，打开全局<code>_config.yml</code>文件：</p><pre><code>vi _config.yml</code></pre><p>修改以下配置：</p><pre><code>url: 1.1.1.1    #填写你的IP地址或域名root: /    #填写网站根目录，如无特殊情况，默认 &quot;/&quot; 即可permalink: :year/:month/:day/:title/permalink_defaults:pretty_urls:  trailing_index: false # Set to false to remove trailing index.html from permalinks</code></pre><p>生成静态文件：</p><pre><code>hexo generate</code></pre><h4 id="上传静态网页文件"><a href="#上传静态网页文件" class="headerlink" title="上传静态网页文件"></a>上传静态网页文件</h4><p>拷贝<code>public</code>目录下的内容，上传到线上服务器的<code>~/webserver/nginx/html</code>目录下，浏览器输入访问地址，页面即可显示。</p><h4 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h4><p>使用Nginx部署时，有几率出现一个小问题：中文命名的文章，由于Nginx可能不识别中文，使得含有中文的文件无法被Nginx解析，导致页面报404错误。<br>解决方法：将文章文件名修改为英文即可，在你新建文章时，有两种方法实现这个效果：</p><ol><li>在<code>hexo new &quot;title_name&quot;</code>时，将<code>title_name</code>命名为英文，然后打开文章文件编辑时，将文件抬头的<code>title</code>属性修改为你想命名的中文名；</li><li><code>hexo new &quot;title_name&quot;</code>时，正常命名，然后使用<code>mv</code>命令，将文件名修改为中文。</li></ol><h2 id="Hexo使用"><a href="#Hexo使用" class="headerlink" title="Hexo使用"></a>Hexo使用</h2><p>Hexo有很多高级操作，本文仅介绍基础的写作，如果对其他操作感兴趣，可以访问<a href="https://hexo.io/zh-cn/docs/writing" target="_blank" rel="noopener">官方文档</a>，其中有详实的操作指导.</p><h3 id="写作-1"><a href="#写作-1" class="headerlink" title="写作"></a>写作</h3><pre><code>hexo new [layout] &lt;title&gt;</code></pre><p>您可以在命令中指定文章的布局（layout），默认为 post，可以通过修改 _config.yml 中的 default_layout 参数来指定默认布局。</p><p>如果你没有修改配置的话，一般默认会新建一个名为<code>title.md</code>的文件在<code>source/_post</code>文件夹中，这时，你需要编辑这个文件，使你的文章能够上传到站点：</p><pre><code>vim &lt;title&gt;.md</code></pre><p>在你打开站点后，你可以看到抬头有三个配置项可以配置：</p><pre><code>title: Hexo部署指南date: 2019-12-16 19:25:37tags:</code></pre><p>一般只需要修改<code>tags:</code>即可，这个配置设置的是这篇文章的标签信息（如果你的主题中启用了标签服务的话）。</p><p>配置完标签后，你就可以将你的文章放在下方，保存后，发布到本地站点：</p><pre><code>hexo generate</code></pre><p>确认文章没有问题后，发布到线上站点：</p><pre><code>hexo deploy</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hexo部署指南&quot;&gt;&lt;a href=&quot;#Hexo部署指南&quot; class=&quot;headerlink&quot; title=&quot;Hexo部署指南&quot;&gt;&lt;/a&gt;Hexo部署指南&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;什么是Hexo&quot;&gt;&lt;a href=&quot;#什么是Hexo&quot; class=&quot;h
      
    
    </summary>
    
    
      <category term="CS Soft" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/"/>
    
      <category term="Client" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/Client/"/>
    
      <category term="Hexo" scheme="https://winyter.github.io/MyBlog/categories/CS-Soft/Client/Hexo/"/>
    
    
      <category term="Hexo" scheme="https://winyter.github.io/MyBlog/tags/Hexo/"/>
    
      <category term="Tools" scheme="https://winyter.github.io/MyBlog/tags/Tools/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://winyter.github.io/MyBlog/2019/12/12/hello-world/"/>
    <id>https://winyter.github.io/MyBlog/2019/12/12/hello-world/</id>
    <published>2019-12-11T19:09:49.000Z</published>
    <updated>2020-04-19T07:37:21.965Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h1><hr><blockquote><p>这是我的博客，欢迎大家来潜水，我会在这里分享我的心得、心声、心爱</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hello-World&quot;&gt;&lt;a href=&quot;#Hello-World&quot; class=&quot;headerlink&quot; title=&quot;Hello World&quot;&gt;&lt;/a&gt;Hello World&lt;/h1&gt;&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;这是我的博客，欢迎大家来潜水，我
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
