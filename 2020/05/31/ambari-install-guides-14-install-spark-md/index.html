<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>基于 Ambari 的大数据集群安装指导——篇十四：手动安装Spark | 花不醉的小花园 | 携一生所爱，看遍世间美好</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="Ambari,Bigdata">
    <meta name="description" content="1、Spark 简介Apache Spark 是一个分布式计算框架，旨在简化运行于计算机集群上的并行程序的编写。该框架对资源调度，任务的提交、执行和跟踪，节点间的通信以及数据并行处理的内在底层操作都进行了抽象。它提供了一个更高级别的 API 用于处理分布式数据。从这方面说，它与 Apache Hadoop 等分布式处理框架类似。但在底层架构上，Spark 与它们有所不同。 运行模式上，Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="基于 Ambari 的大数据集群安装指导——篇十四：手动安装Spark">
<meta property="og:url" content="https:&#x2F;&#x2F;winyter.github.io&#x2F;MyBlog&#x2F;2020&#x2F;05&#x2F;31&#x2F;ambari-install-guides-14-install-spark-md&#x2F;">
<meta property="og:site_name" content="花不醉的小花园">
<meta property="og:description" content="1、Spark 简介Apache Spark 是一个分布式计算框架，旨在简化运行于计算机集群上的并行程序的编写。该框架对资源调度，任务的提交、执行和跟踪，节点间的通信以及数据并行处理的内在底层操作都进行了抽象。它提供了一个更高级别的 API 用于处理分布式数据。从这方面说，它与 Apache Hadoop 等分布式处理框架类似。但在底层架构上，Spark 与它们有所不同。 运行模式上，Spark">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http:&#x2F;&#x2F;cdn.winyter.cn&#x2F;ambari-install-guide_image229.png">
<meta property="article:published_time" content="2020-05-31T09:54:08.000Z">
<meta property="article:modified_time" content="2020-05-31T09:54:35.809Z">
<meta property="article:author" content="winyter">
<meta property="article:tag" content="Ambari">
<meta property="article:tag" content="Bigdata">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;cdn.winyter.cn&#x2F;ambari-install-guide_image229.png">
    
        <link rel="alternate" type="application/atom+xml" title="花不醉的小花园" href="/MyBlog/atom.xml">
    
    <link rel="shortcut icon" href="/MyBlog/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

<meta name="generator" content="Hexo 4.1.0"></head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/MyBlog/img/brand.jpg)">
      <div class="brand">
        <a href="/MyBlog/" class="avatar waves-effect waves-circle waves-light">
          <img src="/MyBlog/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">winyter</h5>
          <a href="mailto:mrwinterwei@outlook.com" title="mrwinterwei@outlook.com" class="mail">mrwinterwei@outlook.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/MyBlog/"  >
                <i class="icon icon-lg icon-home"></i>
                Home
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/MyBlog/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/MyBlog/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/MyBlog/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/winyter" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">基于 Ambari 的大数据集群安装指导——篇十四：手动安装Spark</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">基于 Ambari 的大数据集群安装指导——篇十四：手动安装Spark</h1>
        <h5 class="subtitle">
            
                <time datetime="2020-05-31T09:54:08.000Z" itemprop="datePublished" class="page-time">
  2020-05-31
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/MyBlog/categories/CS-Soft/">CS Soft</a><ul class="article-category-list-child"><li class="article-category-list-item"><a class="article-category-list-link" href="/MyBlog/categories/CS-Soft/Server/">Server</a><ul class="article-category-list-child"><li class="article-category-list-item"><a class="article-category-list-link" href="/MyBlog/categories/CS-Soft/Server/Ambari/">Ambari</a></li></ul></li></ul></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1、Spark-简介"><span class="post-toc-number">1.</span> <span class="post-toc-text">1、Spark 简介</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2、主节点安装-Scala【在-Spark-主节点的-rhino-用户下执行】"><span class="post-toc-number">2.</span> <span class="post-toc-text">2、主节点安装 Scala【在 Spark 主节点的 rhino 用户下执行】</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3、主节点安装-Spark-【在-Spark-主节点的-rhino-用户下执行】"><span class="post-toc-number">3.</span> <span class="post-toc-text">3、主节点安装 Spark 【在 Spark 主节点的 rhino 用户下执行】</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-1-修改-Spark-配置文件"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">3.1 修改 Spark 配置文件</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-2-拷贝-hadoop-的-jar-包【在-Spark-主节点的-root-用户下执行】"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">3.2 拷贝 hadoop 的 jar 包【在 Spark 主节点的 root 用户下执行】</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4、分发安装包至-slaves-节点【在-Spark-主节点的-rhino-用户下执行】"><span class="post-toc-number">4.</span> <span class="post-toc-text">4、分发安装包至 slaves 节点【在 Spark 主节点的 rhino 用户下执行】</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5、加载环境变量及创建软链接【在-Spark-所有节点的-rhino-用户下执行】"><span class="post-toc-number">5.</span> <span class="post-toc-text">5、加载环境变量及创建软链接【在 Spark 所有节点的 rhino 用户下执行】</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6、上传-jar-包到-HDFS-【在-Spark-主节点的-rhino-用户下执行】"><span class="post-toc-number">6.</span> <span class="post-toc-text">6、上传 jar 包到 HDFS 【在 Spark 主节点的 rhino 用户下执行】</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#7、启动thriftserver【在-Spark-主节点的-rhino-用户下执行】"><span class="post-toc-number">7.</span> <span class="post-toc-text">7、启动thriftserver【在 Spark 主节点的 rhino 用户下执行】</span></a></li></ol>
        </nav>
    </aside>


<article id="post-ambari-install-guides-14-install-spark-md"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">基于 Ambari 的大数据集群安装指导——篇十四：手动安装Spark</h1>
        <div class="post-meta">
            <time class="post-time" title="2020-05-31 17:54:08" datetime="2020-05-31T09:54:08.000Z"  itemprop="datePublished">2020-05-31</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/MyBlog/categories/CS-Soft/">CS Soft</a><ul class="article-category-list-child"><li class="article-category-list-item"><a class="article-category-list-link" href="/MyBlog/categories/CS-Soft/Server/">Server</a><ul class="article-category-list-child"><li class="article-category-list-item"><a class="article-category-list-link" href="/MyBlog/categories/CS-Soft/Server/Ambari/">Ambari</a></li></ul></li></ul></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <hr>
<h2 id="1、Spark-简介"><a href="#1、Spark-简介" class="headerlink" title="1、Spark 简介"></a>1、Spark 简介</h2><p>Apache Spark 是一个分布式计算框架，旨在简化运行于计算机集群上的并行程序的编写。该框架对资源调度，任务的提交、执行和跟踪，节点间的通信以及数据并行处理的内在底层操作都进行了抽象。它提供了一个更高级别的 API 用于处理分布式数据。从这方面说，它与 Apache Hadoop 等分布式处理框架类似。但在底层架构上，Spark 与它们有所不同。</p>
<p>运行模式上，Spark 支持四种运行模式：</p>
<ul>
<li>本地单机模式：所有 Spark 进程都运行在同一个 Java 虚拟机（Java Vitural Machine，JVM）中。</li>
<li>集群单机模式：使用 Spark 自己内置的任务调度框架。</li>
<li>基于 Mesos：Mesos 是一个流行的开源集群计算框架。</li>
<li>基于 YARN：即 Hadoop 2，它是一个与 Hadoop 关联的集群计算和资源调度框架。</li>
</ul>
<p>本文将讲述基于 YARN 的 Spark 分布式集群部署指导。</p>
<blockquote>
<p>注意：本次安装以3台节点为例，其中 node1 为主节点，node2、node3 为 slaves 节点，本次安装采取主节点安装配置，然后分发到各个 slaves 的方式来进行部署</p>
</blockquote>
<hr>
<h2 id="2、主节点安装-Scala【在-Spark-主节点的-rhino-用户下执行】"><a href="#2、主节点安装-Scala【在-Spark-主节点的-rhino-用户下执行】" class="headerlink" title="2、主节点安装 Scala【在 Spark 主节点的 rhino 用户下执行】"></a>2、主节点安装 Scala【在 Spark 主节点的 rhino 用户下执行】</h2><p>以 rhino 用户安装，将安装包 <code>scala-2.11.7.tar.gz</code> 上传至 <code>/home/rhino</code> 目录下。</p>
<p>解压 <code>scala-2.11.7.tar.gz</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino</span><br><span class="line">[rhino@node1 ~]$ tar -zxvf scala-2.11.7.tar.gz</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="3、主节点安装-Spark-【在-Spark-主节点的-rhino-用户下执行】"><a href="#3、主节点安装-Spark-【在-Spark-主节点的-rhino-用户下执行】" class="headerlink" title="3、主节点安装 Spark 【在 Spark 主节点的 rhino 用户下执行】"></a>3、主节点安装 Spark 【在 Spark 主节点的 rhino 用户下执行】</h2><p>以 rhino 用户安装，将安装包 <code>spark-2.2.0-bin-hadoop2.6.tgz</code> 上传至 <code>/home/rhino</code> 目录下。<br>这里以安装3台为例。</p>
<p>解压 spark 安装包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino</span><br><span class="line">[rhino@node1 ~]$tar -zxvf spark-2.2.0-bin-hadoop2.6.tgz</span><br></pre></td></tr></table></figure>
<p>修改环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[rhino@node1 ~]$ vim .bashrc</span><br></pre></td></tr></table></figure>
<p>添加如下内容（如果有已经添加的，可以不需要再添加；实际路径根据实际情况填写）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181</span><br><span class="line">export SCALA_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;scala-2.11.7</span><br><span class="line">export LD_LIBRARY_PATH&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;lib:$LD_LIBRARY_PATH</span><br><span class="line">export SPARK_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6</span><br><span class="line">export SPARK_PID_DIR&#x3D;&#x2F;home&#x2F;rhino&#x2F;pids</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;usr&#x2F;hdp&#x2F;2.6.3.0-235&#x2F;hadoop</span><br><span class="line">export PATH&#x3D;$JAVA_HOME&#x2F;bin:$SCALA_HOME&#x2F;bin:$SPARK_HOME&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="3-1-修改-Spark-配置文件"><a href="#3-1-修改-Spark-配置文件" class="headerlink" title="3.1 修改 Spark 配置文件"></a>3.1 修改 Spark 配置文件</h3><ul>
<li>配置 slaves 节点主机名 （注意：主节点主机名不要填写，只填写 slaves 节点的主机名。本文集群机器总共3台，一台主节点，两台 slaves 节点）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[rhino@rhino001 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;conf </span><br><span class="line">[rhino@rhino001 conf]$ cp slaves.template slaves</span><br><span class="line">[rhino@rhino001 conf]$ vim slaves</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">node2</span><br><span class="line">node3</span><br></pre></td></tr></table></figure>
<ul>
<li>配置 <code>spark-en.sh</code> 文件</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;conf</span><br><span class="line">[rhino@node1 conf]$ cp spark-env.sh.template spark-env.sh</span><br><span class="line">[rhino@node1 conf]$ vim spark-env.sh</span><br></pre></td></tr></table></figure>

<p>在spark-env.sh文件末添加所需的环境变量配置（实际路径根据实际情况填写）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181</span><br><span class="line">export SCALA_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;scala-2.11.7</span><br><span class="line">export SPARK_MASTER_IP&#x3D;192.168.50.213</span><br><span class="line">export SPARK_WORKER_MEMORY&#x3D;2g</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;&#x2F;usr&#x2F;hdp&#x2F;2.6.3.0-235&#x2F;hadoop&#x2F;etc&#x2F;hadoop</span><br><span class="line">export YARN_CONF_DIR&#x3D;&#x2F;usr&#x2F;hdp&#x2F;2.6.3.0-235&#x2F;hadoop-yarn&#x2F;etc&#x2F;hadoop</span><br><span class="line">export SPARK_CONF_DIR&#x3D;&#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;conf</span><br><span class="line">export SPARK_CLASSPATH&#x3D;$SPARK_CLASSPATH:&#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;jars&#x2F;mysql-connector-java-5.1.35.jar</span><br></pre></td></tr></table></figure>

<ul>
<li><p>配置 <code>spark-defaults.conf</code> 文件，在 <code>spark-defaults.conf</code> 文件末添加所需的配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;conf </span><br><span class="line">[rhino@node2 conf]$ cp spark-defaults.conf.template spark-defaults.conf </span><br><span class="line">[rhino@node3 conf]$ vim spark-defaults.conf</span><br></pre></td></tr></table></figure>
<p>添加如下内容（实际路径根据实际情况进行配置）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#spark临时目录</span><br><span class="line">spark.local.dir &#x2F;home&#x2F;rhino&#x2F;tmp</span><br><span class="line">spark.sql.warehouse.dir hdfs:&#x2F;&#x2F;mycluster&#x2F;user&#x2F;hive&#x2F;warehouse</span><br><span class="line">spark.yarn.jars hdfs:&#x2F;&#x2F;mycluster&#x2F;sharkjars-2.0&#x2F;*</span><br></pre></td></tr></table></figure>
</li>
<li><p>新建配置文件 <code>hive-site.xml</code></p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[rhino@node1 ]$ vim &#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;conf&#x2F;hive-site.xml</span><br></pre></td></tr></table></figure>
<p>添加如下内容，注意根据实际情况，修改相关配置参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.metastore.client.socket.timeout&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;3600&lt;&#x2F;value&gt;</span><br><span class="line">&lt;description&gt;MetaStore Client socket timeout in seconds&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;jdbc:mysql:&#x2F;&#x2F;172.168.1.1:3306&#x2F;hive?createDatabaseIfNotExist&#x3D;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;rhino&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;xxxxxxxx&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;jdbc:mysql:&#x2F;&#x2F;172.168.1.1:3306&#x2F;hive?createDatabaseIfNotExist&#x3D;true&lt;&#x2F;value&gt;     # 这个value里面写的是hive存在mysql上的元数据库的地址，如果是使用ambari安装的hive，则ip应该为ambari使用的mysql的ip，后面的hive_rhino为hive生产环境的库名，如果不知道怎么配，直接登录ambari，点击Hive &gt; configs &gt; DATABASE，将Database URL 配置项的内容直接拷贝到&lt;value&gt; &lt;&#x2F;value&gt;之间即可</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">扩展：spark需要连接hive的元数据库，通过获取元数据库内的元数据，才能建立spark与hive的链接，从而能够使用sparksql来操作hive。</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="3-2-拷贝-hadoop-的-jar-包【在-Spark-主节点的-root-用户下执行】"><a href="#3-2-拷贝-hadoop-的-jar-包【在-Spark-主节点的-root-用户下执行】" class="headerlink" title="3.2 拷贝 hadoop 的 jar 包【在 Spark 主节点的 root 用户下执行】"></a>3.2 拷贝 hadoop 的 jar 包【在 Spark 主节点的 root 用户下执行】</h3><p>从 <code>/usr/hdp/2.6.3.0-235/hadoop/client</code> 中拷贝 <code>jersey-client-1.9.jar</code> 和 <code>jersey-core-1.9.jar</code> 到 spark 的 jars 中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">su - root</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# cd &#x2F;usr&#x2F;hdp&#x2F;2.6.3.0-235&#x2F;hadoop&#x2F;client&#x2F;</span><br><span class="line">[root@node1 client] cp jersey-client-1.9.jar &#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;jars</span><br><span class="line">[root@node1 client]# cp jersey-core-1.9.jar &#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;jars</span><br><span class="line">[root@node1 client]# cd &#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;jars</span><br><span class="line">[root@node1 jars]# chmod 755 *</span><br><span class="line">[root@node1 jars]# chown rhino:root *</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="4、分发安装包至-slaves-节点【在-Spark-主节点的-rhino-用户下执行】"><a href="#4、分发安装包至-slaves-节点【在-Spark-主节点的-rhino-用户下执行】" class="headerlink" title="4、分发安装包至 slaves 节点【在 Spark 主节点的 rhino 用户下执行】"></a>4、分发安装包至 slaves 节点【在 Spark 主节点的 rhino 用户下执行】</h2><p>拷贝 <code>scala-2.11.7</code> 及环境变量到 slave 机器。（这里以安装3台为例）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[rhino@node1 ~]$ scp -r ~&#x2F;scala-2.11.7 rhino@node2:~</span><br><span class="line">[rhino@node1 ~]$ scp -r ~&#x2F;scala-2.11.7 rhino@node3:~</span><br></pre></td></tr></table></figure>

<p>拷贝 <code>spark-2.2.0-bin-hadoop2.6</code> 及环境变量到 slave 机器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[rhino@node1 ~]$ scp -r ~&#x2F; spark-2.2.0-bin-hadoop2.6 rhino@node2:~</span><br><span class="line">[rhino@node1 ~]$ scp -r ~&#x2F; spark-2.2.0-bin-hadoop2.6 rhino@node3:~</span><br><span class="line">[rhino@node1 ~]$ scp ~&#x2F;.bashrc rhino@node2:~</span><br><span class="line">[rhino@node1 ~]$ scp ~&#x2F;.bashrc rhino@node3:~</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="5、加载环境变量及创建软链接【在-Spark-所有节点的-rhino-用户下执行】"><a href="#5、加载环境变量及创建软链接【在-Spark-所有节点的-rhino-用户下执行】" class="headerlink" title="5、加载环境变量及创建软链接【在 Spark 所有节点的 rhino 用户下执行】"></a>5、加载环境变量及创建软链接【在 Spark 所有节点的 rhino 用户下执行】</h2><p>使每台机器环境变量生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[rhino@node1 ~]$ source ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure>
<p>在每台机器上创建软链接</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino</span><br><span class="line">[rhino@node1 ~]$ ln -s spark-2.2.0-bin-hadoop2.6 spark</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="6、上传-jar-包到-HDFS-【在-Spark-主节点的-rhino-用户下执行】"><a href="#6、上传-jar-包到-HDFS-【在-Spark-主节点的-rhino-用户下执行】" class="headerlink" title="6、上传 jar 包到 HDFS 【在 Spark 主节点的 rhino 用户下执行】"></a>6、上传 jar 包到 HDFS 【在 Spark 主节点的 rhino 用户下执行】</h2><p>将 spark 环境中的 jar 包上传到 HDFS</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[rhino@node1 ~]$hdfs dfs -mkdir hdfs:&#x2F;&#x2F;sinovatiocluster&#x2F;sharkjars-2.0&#x2F;</span><br><span class="line">[rhino@node1 ~]$ hdfs dfs -put ~&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;jars&#x2F;* hdfs:&#x2F;&#x2F;mycluster&#x2F;sharkjars-2.0&#x2F;</span><br></pre></td></tr></table></figure>
<p>验证 jar 包上传是否成功<br>通过浏览器登录 HDFS 界面：<code>http://172.168.1.1:50070/explorer.html</code><br>查看 Utilities 项，并选择 <code>browse the file system</code>，结果如下图：<br><img src="http://cdn.winyter.cn/ambari-install-guide_image229.png" alt="image-229.png"></p>
<hr>
<h2 id="7、启动thriftserver【在-Spark-主节点的-rhino-用户下执行】"><a href="#7、启动thriftserver【在-Spark-主节点的-rhino-用户下执行】" class="headerlink" title="7、启动thriftserver【在 Spark 主节点的 rhino 用户下执行】"></a>7、启动thriftserver【在 Spark 主节点的 rhino 用户下执行】</h2><p>即启动 sparksql，需要时启动。</p>
<blockquote>
<p>如果不用 sparksql 就不要启动。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~&#x2F;spark&#x2F;sbin&#x2F;start-thriftserver.sh --master yarn-client --executor-memory 1g --executor-cores 2 --queue root.spark-mid</span><br></pre></td></tr></table></figure>
<blockquote>
<p>–executor-memory: 指定内存<br>–executor-cores: 指定核数<br>–queue: 指定 yarn 资源队列</p>
</blockquote>
<hr>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2020-05-31T09:54:35.809Z" itemprop="dateUpdated">2020-05-31 17:54:35</time>
</span><br>


        
        Copyright Info
        
    </div>
    
    <footer>
        <a href="https://winyter.github.io/MyBlog">
            <img src="/MyBlog/img/avatar.jpg" alt="winyter">
            winyter
        </a>
    </footer>
</blockquote>

        


        <div class="post-footer">
            
	<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/MyBlog/tags/Ambari/" rel="tag">Ambari</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/MyBlog/tags/Bigdata/" rel="tag">Bigdata</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-14-install-spark-md/&title=《基于 Ambari 的大数据集群安装指导——篇十四：手动安装Spark》 — 花不醉的小花园&pic=https://winyter.github.io/MyBlog/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-14-install-spark-md/&title=《基于 Ambari 的大数据集群安装指导——篇十四：手动安装Spark》 — 花不醉的小花园&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-14-install-spark-md/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《基于 Ambari 的大数据集群安装指导——篇十四：手动安装Spark》 — 花不醉的小花园&url=https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-14-install-spark-md/&via=https://winyter.github.io/MyBlog" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-14-install-spark-md/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/MyBlog/2020/05/31/ambari-install-guides-15-install-kafka-manager-md/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">基于 Ambari 的大数据集群安装指导——篇十五：手动安装 Kafka Manager</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/MyBlog/2020/05/31/ambari-install-guides-13-install-impala-md/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">基于 Ambari 的大数据集群安装指导——篇十三：Ambari 安装 Impala</h4>
      </a>
    </div>
  
</nav>



    




















</article>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/MyBlog/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>winyter &copy; 2019 - 2020</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-14-install-spark-md/&title=《基于 Ambari 的大数据集群安装指导——篇十四：手动安装Spark》 — 花不醉的小花园&pic=https://winyter.github.io/MyBlog/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-14-install-spark-md/&title=《基于 Ambari 的大数据集群安装指导——篇十四：手动安装Spark》 — 花不醉的小花园&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-14-install-spark-md/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《基于 Ambari 的大数据集群安装指导——篇十四：手动安装Spark》 — 花不醉的小花园&url=https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-14-install-spark-md/&via=https://winyter.github.io/MyBlog" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-14-install-spark-md/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADMUlEQVR42u3awW7qQAwF0P7/T/O2fSok13aQiDlZoZZMcmaB5ev5+Ymvx6/r+L9/v3P89+NvHj/31TqXXdjY2Ng3YT8Or+MXylf7e9er9fMtztd5sho2Njb2OvZxYUgeU8VUC1tSMhMLNjY2NnaO78Emq2FjY2NjVwtY3khMymEeV2FjY2N/MzsPg5INyrcs4VVDq4uzNGxsbOyPZ+dT0c///Jb5NjY2NvYHsx/Fq3qIpzckqI6QywpsbGzsRey8AOTHaPJQKRn6VludwvtjY2Njr2Anxaka4iSMHNDjnXwfGxsbeym7OnDNqdc2Nkmrc/JcbGxs7KXs/Bd+MnCdFJ7eCieTbWxsbOx17Hxoeu2xnvkoN9nilwUMGxsb+7bsXvievHRenOaD3mZRxMbGxl7BnsfxFxSSVjBULX5PQiVsbGzspexeNF8dDycFrBppFdoebGxs7K9hJ81G3sbkg+R3jC6eZGnY2NjYi9jVwzrVwGjyudfYnERg2NjY2IvYvbi/2pbksdH8WYWNw8bGxl7Brv70916xWmx6I4TCXdjY2NhL2UnLkX8/P5Qzb2CSKv1yPICNjY29gl24IQlriod7klfP8VGxxMbGxl7Eri5ULWm96GcSUZVnI9jY2Njr2Pkk9NrjNZOtaW4KNjY29gp2tWBMWoheFFVtn6JxMjY2NvZq9qQIzeOearx1We+FjY2NfXN2NaBJWoi8Seghe43NfwUMGxsbewU7/+nvlahJqJQPlZMNwsbGxt7NPn7YpHQlQ4UkqOoV0ZMNwsbGxl7EzkP/SRswOXzTC5iaU25sbGzsG7Lz7qQ6DJ687hx/0uRgY2Njr2NXH1MdA+Tx0OSgT9LkYGNjY29l52PR6qjgHedkLrgLGxsbewX7UbyuzbGqB3p64deTe7GxsbEXsfMrH+JOwqbesLa3KdjY2Nib2NWyUX3dXvBUPbiTHxXCxsbG3sqeRDnVopIXyzcOCbCxsbG/mH3tCCEvThNk+cgONjY29pex81Fx9bDOfOh7EiphY2NjL2InoVKv2CTrVMOmaukqH9zBxsbGvhU7D3p6kVDvIE4+uM23AxsbG3sd+x/DoGkE76Ix9wAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/MyBlog/', SHARE: true, REWARD: false };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>






<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '死鬼去哪里了！';
            clearTimeout(titleTime);
        } else {
            document.title = '(つェ⊂)咦!又好了!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
