<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/MyBlog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/MyBlog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/MyBlog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/MyBlog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/MyBlog/css/main.css">


<link rel="stylesheet" href="/MyBlog/lib/font-awesome/css/all.min.css">
  
  <link rel="stylesheet" href="/MyBlog/lib/animate-css/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"winyter.github.io","root":"/MyBlog/","scheme":"Gemini","version":"8.0.0-rc.4","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"path":"search.json"};
  </script>

  <meta name="description" content="1、基础环境准备">
<meta property="og:type" content="article">
<meta property="og:title" content="基于 Ambari 的大数据集群安装指导——篇二：环境准备">
<meta property="og:url" content="https:&#x2F;&#x2F;winyter.github.io&#x2F;MyBlog&#x2F;2020&#x2F;05&#x2F;31&#x2F;ambari-install-guides-2-environment-md&#x2F;index.html">
<meta property="og:site_name" content="花不醉的小花园">
<meta property="og:description" content="1、基础环境准备">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http:&#x2F;&#x2F;static.zybuluo.com&#x2F;winyter&#x2F;ydzffqqiw3kvorqt8k7wfk9n&#x2F;image.png">
<meta property="og:image" content="http:&#x2F;&#x2F;static.zybuluo.com&#x2F;winyter&#x2F;2x6xwi4lytp2xkykyp39iihj&#x2F;image.png">
<meta property="og:image" content="http:&#x2F;&#x2F;static.zybuluo.com&#x2F;winyter&#x2F;c5wag9wgptk6b7gnlb7r57u1&#x2F;image.png">
<meta property="og:image" content="http:&#x2F;&#x2F;static.zybuluo.com&#x2F;winyter&#x2F;qex2g3kui8jx7f9qnw128ig0&#x2F;image.png">
<meta property="og:image" content="http:&#x2F;&#x2F;static.zybuluo.com&#x2F;winyter&#x2F;vwbcgjs0yltlzumljju68ohi&#x2F;image.png">
<meta property="og:image" content="http:&#x2F;&#x2F;cdn.winyter.cn&#x2F;ambari-install-guide_image6.png">
<meta property="og:image" content="http:&#x2F;&#x2F;cdn.winyter.cn&#x2F;ambari-install-guide_image7.png">
<meta property="og:image" content="http:&#x2F;&#x2F;cdn.winyter.cn&#x2F;ambari-install-guide_image8.png">
<meta property="article:published_time" content="2020-05-31T09:19:20.000Z">
<meta property="article:modified_time" content="2020-07-25T11:12:56.248Z">
<meta property="article:author" content="winyter">
<meta property="article:tag" content="Ambari">
<meta property="article:tag" content="Bigdata">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;static.zybuluo.com&#x2F;winyter&#x2F;ydzffqqiw3kvorqt8k7wfk9n&#x2F;image.png">

<link rel="canonical" href="https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-2-environment-md/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>基于 Ambari 的大数据集群安装指导——篇二：环境准备 | 花不醉的小花园</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<link rel="alternate" href="/MyBlog/atom.xml" title="花不醉的小花园" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <main class="main">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader">
        <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/MyBlog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">花不醉的小花园</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">携一生所爱，看遍世间美好</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/MyBlog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/MyBlog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/MyBlog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/MyBlog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、基础环境准备"><span class="nav-text">1、基础环境准备</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-基础版本包的上传【在每个节点-root-用户下执行】"><span class="nav-text">1.1 基础版本包的上传【在每个节点 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-操作系统安装及数据盘格式化挂载"><span class="nav-text">1.2 操作系统安装及数据盘格式化挂载</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-操作系统安装"><span class="nav-text">1.2.1 操作系统安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-2-数据盘格式化挂载【每个节点root用户下执行】"><span class="nav-text">1.2.2 数据盘格式化挂载【每个节点root用户下执行】</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-3-修改数据目录权限【每个节点-root-用户下执行】"><span class="nav-text">1.2.3 修改数据目录权限【每个节点 root 用户下执行】</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-操作系统参数设置【每个节点root用户下执行】"><span class="nav-text">1.3 操作系统参数设置【每个节点root用户下执行】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-修改-etc-security-limits-conf文件"><span class="nav-text">1.3.1 修改&#x2F;etc&#x2F;security&#x2F;limits.conf文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-2-修改-etc-sysctl-conf文件"><span class="nav-text">1.3.2 修改&#x2F;etc&#x2F;sysctl.conf文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-3-关闭监控程序的服务端和客户端"><span class="nav-text">1.3.3 关闭监控程序的服务端和客户端</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-4系统自启动文件修改"><span class="nav-text">1.3.4系统自启动文件修改</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-禁用-SELinux-和-Firewall【每个节点-root-用户下执行】"><span class="nav-text">1.4 禁用 SELinux 和 Firewall【每个节点 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-配置主机名及-hosts【每个节点-root-用户下执行】"><span class="nav-text">1.5 配置主机名及 hosts【每个节点 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-6-修改ssh参数配置【每个节点-root-用户下执行】"><span class="nav-text">1.6 修改ssh参数配置【每个节点 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-7-重启操作系统【每个节点-root-用户下执行】"><span class="nav-text">1.7 重启操作系统【每个节点 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-8-配置本地源【在-Ambari-Server-所在节点的-root-用户下执行】"><span class="nav-text">1.8 配置本地源【在 Ambari Server 所在节点的 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-9-安装-httpd-服务【在-Ambari-Server-所在节点的-root-用户下执行】"><span class="nav-text">1.9 安装 httpd 服务【在 Ambari Server 所在节点的 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-10-安装-wget-服务【在所有部署-Ambari-Agent-节点的-root-用户下执行】"><span class="nav-text">1.10 安装 wget 服务【在所有部署 Ambari Agent 节点的 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-11-统一服务器时区【所有节点的-root-用户下执行】"><span class="nav-text">1.11 统一服务器时区【所有节点的 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-12-安装-NTP-服务端【在-ambari-server-节点的-root-用户下执行】"><span class="nav-text">1.12 安装 NTP 服务端【在 ambari server 节点的 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-13-安装-NTP-客户端【在除-ambari-server-节点以外的其他所有节点的-root-用户下执行】"><span class="nav-text">1.13 安装 NTP 客户端【在除 ambari server 节点以外的其他所有节点的 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-14-配置root用户无密码访问【所有节点-root-用户执行】"><span class="nav-text">1.14 配置root用户无密码访问【所有节点 root 用户执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-15-配置rhino用户无密码访问【所有节点-rhino-用户执行】"><span class="nav-text">1.15 配置rhino用户无密码访问【所有节点 rhino 用户执行】</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-安装-JDK"><span class="nav-text">1.2 安装 JDK</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-JDK安装及环境配置【选择任一节点的-root-用户下执行】"><span class="nav-text">1.2.1 JDK安装及环境配置【选择任一节点的 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-2-环境配置及修改权限【选择任一节点的-root-用户下执行】"><span class="nav-text">1.2.2 环境配置及修改权限【选择任一节点的 root 用户下执行】</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-MySQL数据库安装"><span class="nav-text">1.3 MySQL数据库安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-1-节点规划及端口检查"><span class="nav-text">1.3.1 节点规划及端口检查</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-2-安装部署Mysql双主备【在规划安装-MySQL-的两台节点的-rhino-用户下操作】"><span class="nav-text">1.3.2 安装部署Mysql双主备【在规划安装 MySQL 的两台节点的 rhino 用户下操作】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-3-初始化及启动Mysql数据库【两台-MySQL-节点的-rhino-用户下执行】"><span class="nav-text">1.3.3 初始化及启动Mysql数据库【两台 MySQL 节点的 rhino 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-4-登录Mysql设置双主备模式【两台-MySQL-节点的-rhino-用户下执行】"><span class="nav-text">1.3.4 登录Mysql设置双主备模式【两台 MySQL 节点的 rhino 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-5-验证Mysql双主备模式【两台节点-MySQL-上执行】"><span class="nav-text">1.3.5 验证Mysql双主备模式【两台节点 MySQL 上执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-6-为Mysql添加rhino用户【主-MySQL-节点-rhino-用户下执行】"><span class="nav-text">1.3.6 为Mysql添加rhino用户【主 MySQL 节点 rhino 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-7-创建hive-rhino数据库【主-MySQL-节点-rhino-用户下执行】"><span class="nav-text">1.3.7 创建hive_rhino数据库【主 MySQL 节点 rhino 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-8-添加Mysql开机自启【在-MySQL-主节点-root-用户下执行】"><span class="nav-text">1.3.8 添加Mysql开机自启【在 MySQL 主节点 root 用户下执行】</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-部署-Keepalived（MySQL双机切换）"><span class="nav-text">1.4 部署 Keepalived（MySQL双机切换）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-1-安装部署-MySQL-双机切换【在-MySQL-所在两个节点的-root-用户下执行】"><span class="nav-text">1.4.1 安装部署 MySQL 双机切换【在 MySQL 所在两个节点的 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-2-配置Mysql双机切换【在-MySQL-所在两个节点的-root-用户下执行】"><span class="nav-text">1.4.2 配置Mysql双机切换【在 MySQL 所在两个节点的 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-3-启停keepalived进程【在-MySQL-所在两个节点的-root-用户下执行】"><span class="nav-text">1.4.3 启停keepalived进程【在 MySQL 所在两个节点的 root 用户下执行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-4-验证-MySQL-双机切换"><span class="nav-text">1.4.4 验证 MySQL 双机切换</span></a></li></ol></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="winyter"
      src="/MyBlog/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">winyter</p>
  <div class="site-description" itemprop="description">IT,Photograph,Travel,Love</div>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/winyter" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;winyter" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:mrwinterwei@outlook.com" title="E-Mail → mailto:mrwinterwei@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </section>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </header>

      
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/winyter" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div id="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


      <div class="main-inner">
        

        <div class="content post posts-expand">
          

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-2-environment-md/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/MyBlog/images/avatar.jpg">
      <meta itemprop="name" content="winyter">
      <meta itemprop="description" content="IT,Photograph,Travel,Love">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="花不醉的小花园">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          基于 Ambari 的大数据集群安装指导——篇二：环境准备
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-05-31 17:19:20" itemprop="dateCreated datePublished" datetime="2020-05-31T17:19:20+08:00">2020-05-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-07-25 19:12:56" itemprop="dateModified" datetime="2020-07-25T19:12:56+08:00">2020-07-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/MyBlog/categories/CS-Soft/" itemprop="url" rel="index"><span itemprop="name">CS Soft</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/MyBlog/categories/CS-Soft/Server/" itemprop="url" rel="index"><span itemprop="name">Server</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/MyBlog/categories/CS-Soft/Server/Ambari/" itemprop="url" rel="index"><span itemprop="name">Ambari</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <hr>
<h2 id="1、基础环境准备"><a href="#1、基础环境准备" class="headerlink" title="1、基础环境准备"></a>1、基础环境准备</h2><hr>
<a id="more"></a>

<h3 id="1-1-基础版本包的上传【在每个节点-root-用户下执行】"><a href="#1-1-基础版本包的上传【在每个节点-root-用户下执行】" class="headerlink" title="1.1 基础版本包的上传【在每个节点 root 用户下执行】"></a>1.1 基础版本包的上传【在每个节点 root 用户下执行】</h3><p>基础版本包即一些基础的，支撑业务的服务包，本文档 ambari 安装包含了以下的必装服务，实际安装时，按照自己的实际需求做增删：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apache-tomcat-8.0.35.zip       # tomcat 安装包</span><br><span class="line">jdk1.8.0_181.tar.gz            # jdk 安装包</span><br><span class="line">kafka-manager-1.3.3.17.zip     # kafka manager 安装包</span><br><span class="line">keepalived-1.1.20.tar.gz       # mysql 的 keepalived 安装包</span><br><span class="line">mysql-connector-java-5.1.40-bin.jar     # mysql 的 JDBC 驱动 jar 包</span><br><span class="line">parted.sh                      # 自动挂载脚本</span><br><span class="line">mysql-5.6.25-linux-x86_64.tar.gz        # mysql 安装包</span><br></pre></td></tr></table></figure>
<p>新建版本包存放目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# mkdir -p &#x2F;home&#x2F;package&#x2F;base</span><br></pre></td></tr></table></figure>
<p>将这些安装包上传至 <code>/home/package/base</code></p>
<blockquote>
<p>注意：本文档后续安装中，如果使用到这些基础包，将不再赘述将包拷贝至相应目录，在自己实际操作时，记得到该目录下拷贝服务包。</p>
</blockquote>
<hr>
<h3 id="1-2-操作系统安装及数据盘格式化挂载"><a href="#1-2-操作系统安装及数据盘格式化挂载" class="headerlink" title="1.2 操作系统安装及数据盘格式化挂载"></a>1.2 操作系统安装及数据盘格式化挂载</h3><blockquote>
<p>本节内容主要面向物理机的部署，对于使用了云平台等 PAAS 服务来部署集群的情况，可以不需要进行本节的操作</p>
</blockquote>
<h4 id="1-2-1-操作系统安装"><a href="#1-2-1-操作系统安装" class="headerlink" title="1.2.1 操作系统安装"></a>1.2.1 操作系统安装</h4><blockquote>
<p><a href="https://winyter.github.io/MyBlog/2020/06/09/centos7x-install-guide/" target="_blank" rel="noopener">CentOS 7x 操作系统安装</a></p>
</blockquote>
<h4 id="1-2-2-数据盘格式化挂载【每个节点root用户下执行】"><a href="#1-2-2-数据盘格式化挂载【每个节点root用户下执行】" class="headerlink" title="1.2.2 数据盘格式化挂载【每个节点root用户下执行】"></a>1.2.2 数据盘格式化挂载【每个节点root用户下执行】</h4><p>磁盘格式化可以使用脚本进行，下面提供的脚本是基于 xfs 文件格式的，通过 uuid 方式进行挂载的。<br>脚本解析详见文档：</p>
<blockquote>
<p>链接</p>
</blockquote>
<p>将脚本放入<code>/root</code>目录下，并赋予755权限：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# chmod 755 parted.sh</span><br></pre></td></tr></table></figure>
<p>执行脚本：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# sh parted.sh</span><br><span class="line"></span><br><span class="line">根据提示，执行安装：</span><br><span class="line">Step 1.No lock file, begin to create lock file and continue</span><br><span class="line">.&#x2F;parted.sh: line 201: -----------------------------------------------------MAIN---------------------------------------------: command not found</span><br><span class="line">Step 2.Begin to check free disk</span><br><span class="line">You have a free disk, Now will fdisk it and mount it</span><br><span class="line">You have a free disk, Now will fdisk it and mount it</span><br><span class="line">You have a free disk, Now will fdisk it and mount it</span><br><span class="line">You have a free disk, Now will fdisk it and mount it</span><br><span class="line">This system have free disk :</span><br><span class="line">&#x2F;dev&#x2F;sdb</span><br><span class="line">&#x2F;dev&#x2F;sdc</span><br><span class="line">&#x2F;dev&#x2F;sdd</span><br><span class="line">&#x2F;dev&#x2F;sde</span><br><span class="line">confirm free disk(yes&#x2F;no):yes</span><br><span class="line"></span><br><span class="line">Step 3.Begin to parted free disk &#x2F;dev&#x2F;sdb </span><br><span class="line">Step 3.Begin to parted free disk &#x2F;dev&#x2F;sdc </span><br><span class="line">Step 3.Begin to parted free disk &#x2F;dev&#x2F;sdd</span><br><span class="line">Step 3.Begin to parted free disk &#x2F;dev&#x2F;sde </span><br><span class="line"></span><br><span class="line">Step 4.Begin to mkfs on disk &#x2F;dev&#x2F;sdb </span><br><span class="line">meta-data&#x3D;&#x2F;dev&#x2F;sdb1              isize&#x3D;512    agcount&#x3D;160, agsize&#x3D;163837 blks</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   attr&#x3D;2, projid32bit&#x3D;1</span><br><span class="line">         &#x3D;                       crc&#x3D;1        finobt&#x3D;0, sparse&#x3D;0</span><br><span class="line">data     &#x3D;                       bsize&#x3D;4096   blocks&#x3D;26213888, imaxpct&#x3D;25</span><br><span class="line">         &#x3D;                       sunit&#x3D;0      swidth&#x3D;0 blks</span><br><span class="line">naming   &#x3D;version 2              bsize&#x3D;4096   ascii-ci&#x3D;0 ftype&#x3D;1</span><br><span class="line">log      &#x3D;internal log           bsize&#x3D;4096   blocks&#x3D;12799, version&#x3D;2</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   sunit&#x3D;0 blks, lazy-count&#x3D;1</span><br><span class="line">realtime &#x3D;none                   extsz&#x3D;4096   blocks&#x3D;0, rtextents&#x3D;0</span><br><span class="line"></span><br><span class="line">Step 4.Begin to mkfs on disk &#x2F;dev&#x2F;sdc </span><br><span class="line">meta-data&#x3D;&#x2F;dev&#x2F;sdc1              isize&#x3D;512    agcount&#x3D;160, agsize&#x3D;163837 blks</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   attr&#x3D;2, projid32bit&#x3D;1</span><br><span class="line">         &#x3D;                       crc&#x3D;1        finobt&#x3D;0, sparse&#x3D;0</span><br><span class="line">data     &#x3D;                       bsize&#x3D;4096   blocks&#x3D;26213888, imaxpct&#x3D;25</span><br><span class="line">         &#x3D;                       sunit&#x3D;0      swidth&#x3D;0 blks</span><br><span class="line">naming   &#x3D;version 2              bsize&#x3D;4096   ascii-ci&#x3D;0 ftype&#x3D;1</span><br><span class="line">log      &#x3D;internal log           bsize&#x3D;4096   blocks&#x3D;12799, version&#x3D;2</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   sunit&#x3D;0 blks, lazy-count&#x3D;1</span><br><span class="line">realtime &#x3D;none                   extsz&#x3D;4096   blocks&#x3D;0, rtextents&#x3D;0</span><br><span class="line"></span><br><span class="line">Step 4.Begin to mkfs on disk &#x2F;dev&#x2F;sdd </span><br><span class="line">meta-data&#x3D;&#x2F;dev&#x2F;sdd1              isize&#x3D;512    agcount&#x3D;160, agsize&#x3D;163837 blks</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   attr&#x3D;2, projid32bit&#x3D;1</span><br><span class="line">         &#x3D;                       crc&#x3D;1        finobt&#x3D;0, sparse&#x3D;0</span><br><span class="line">data     &#x3D;                       bsize&#x3D;4096   blocks&#x3D;26213888, imaxpct&#x3D;25</span><br><span class="line">         &#x3D;                       sunit&#x3D;0      swidth&#x3D;0 blks</span><br><span class="line">naming   &#x3D;version 2              bsize&#x3D;4096   ascii-ci&#x3D;0 ftype&#x3D;1</span><br><span class="line">log      &#x3D;internal log           bsize&#x3D;4096   blocks&#x3D;12799, version&#x3D;2</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   sunit&#x3D;0 blks, lazy-count&#x3D;1</span><br><span class="line">realtime &#x3D;none                   extsz&#x3D;4096   blocks&#x3D;0, rtextents&#x3D;0</span><br><span class="line"></span><br><span class="line">Step 4.Begin to mkfs on disk &#x2F;dev&#x2F;sde </span><br><span class="line">meta-data&#x3D;&#x2F;dev&#x2F;sde1              isize&#x3D;512    agcount&#x3D;160, agsize&#x3D;163837 blks</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   attr&#x3D;2, projid32bit&#x3D;1</span><br><span class="line">         &#x3D;                       crc&#x3D;1        finobt&#x3D;0, sparse&#x3D;0</span><br><span class="line">data     &#x3D;                       bsize&#x3D;4096   blocks&#x3D;26213888, imaxpct&#x3D;25</span><br><span class="line">         &#x3D;                       sunit&#x3D;0      swidth&#x3D;0 blks</span><br><span class="line">naming   &#x3D;version 2              bsize&#x3D;4096   ascii-ci&#x3D;0 ftype&#x3D;1</span><br><span class="line">log      &#x3D;internal log           bsize&#x3D;4096   blocks&#x3D;12799, version&#x3D;2</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   sunit&#x3D;0 blks, lazy-count&#x3D;1</span><br><span class="line">realtime &#x3D;none                   extsz&#x3D;4096   blocks&#x3D;0, rtextents&#x3D;0</span><br><span class="line"></span><br><span class="line">Step 5.Begin to mount disk &#x2F;dev&#x2F;sdb </span><br><span class="line">&#x2F;data01 not exist! make it                &#x2F;&#x2F;不需要自己创建&#x2F;data01,脚本自动创建</span><br><span class="line">Step 5.Begin to mount disk &#x2F;dev&#x2F;sdc </span><br><span class="line">&#x2F;data02 not exist! make it </span><br><span class="line">Step 5.Begin to mount disk &#x2F;dev&#x2F;sdd </span><br><span class="line">&#x2F;data03 not exist! make it </span><br><span class="line">Step 5.Begin to mount disk &#x2F;dev&#x2F;sde </span><br><span class="line">&#x2F;data04 not exist! make it</span><br></pre></td></tr></table></figure>
<p>查看挂载情况：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# df -h</span><br></pre></td></tr></table></figure>

<p>获取每块磁盘的uuid</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# lsblk -f</span><br></pre></td></tr></table></figure>
<p><img src="http://static.zybuluo.com/winyter/ydzffqqiw3kvorqt8k7wfk9n/image.png" alt="image.png-92.3kB"></p>
<p>修改挂载脚本中的uuid</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# vim &#x2F;root&#x2F;.mount.sh</span><br></pre></td></tr></table></figure>
<p><img src="http://static.zybuluo.com/winyter/2x6xwi4lytp2xkykyp39iihj/image.png" alt="image.png-35.6kB"></p>
<p>添加开机自动挂载</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# chmod +x &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br><span class="line">[root@node1 ~]# echo sh &#x2F;root&#x2F;.mount.sh &gt;&gt; &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="1-2-3-修改数据目录权限【每个节点-root-用户下执行】"><a href="#1-2-3-修改数据目录权限【每个节点-root-用户下执行】" class="headerlink" title="1.2.3 修改数据目录权限【每个节点 root 用户下执行】"></a>1.2.3 修改数据目录权限【每个节点 root 用户下执行】</h4><p>需要给所有节点的数据盘目录赋 777 权限，否则在集群安装时，会出现错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod -R 777 &#x2F;data*</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-3-操作系统参数设置【每个节点root用户下执行】"><a href="#1-3-操作系统参数设置【每个节点root用户下执行】" class="headerlink" title="1.3 操作系统参数设置【每个节点root用户下执行】"></a>1.3 操作系统参数设置【每个节点root用户下执行】</h3><h4 id="1-3-1-修改-etc-security-limits-conf文件"><a href="#1-3-1-修改-etc-security-limits-conf文件" class="headerlink" title="1.3.1 修改/etc/security/limits.conf文件"></a>1.3.1 修改<code>/etc/security/limits.conf</code>文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# vim &#x2F;etc&#x2F;security&#x2F;limits.conf</span><br><span class="line"></span><br><span class="line">在文件最下方添加如下内容：</span><br><span class="line"></span><br><span class="line">* soft nofile 200000</span><br><span class="line">* hard nofile 200000</span><br><span class="line">* soft nproc 131072</span><br><span class="line">* hard nproc 131072</span><br><span class="line">* soft core unlimited</span><br><span class="line">* - memlock unlimited</span><br><span class="line">* - as      unlimited</span><br><span class="line">* - data    unlimited</span><br><span class="line">* - fsize   unlimited</span><br><span class="line">* - rss     unlimited</span><br></pre></td></tr></table></figure>

<h4 id="1-3-2-修改-etc-sysctl-conf文件"><a href="#1-3-2-修改-etc-sysctl-conf文件" class="headerlink" title="1.3.2 修改/etc/sysctl.conf文件"></a>1.3.2 修改<code>/etc/sysctl.conf</code>文件</h4><p>修改以下配置参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vm.max_map_count&#x3D;262144</span><br></pre></td></tr></table></figure>
<p>该配置修改完需要重启操作系统，但目前不重启，后续基础配置完成后，文档会指导进行重启操作。</p>
<blockquote>
<p>可通过 <code>[root@node1]# sysctl -w vm.max_map_count=262144</code> 使得该配置临时生效。</p>
</blockquote>
<h4 id="1-3-3-关闭监控程序的服务端和客户端"><a href="#1-3-3-关闭监控程序的服务端和客户端" class="headerlink" title="1.3.3 关闭监控程序的服务端和客户端"></a>1.3.3 关闭监控程序的服务端和客户端</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# systemctl start tuned</span><br><span class="line">[root@node1]# tuned-adm off</span><br><span class="line">[root@node1]# tuned-adm list</span><br><span class="line">[root@node1]# systemctl stop tuned</span><br><span class="line">[root@node1]# systemctl disable tuned</span><br></pre></td></tr></table></figure>

<h4 id="1-3-4系统自启动文件修改"><a href="#1-3-4系统自启动文件修改" class="headerlink" title="1.3.4系统自启动文件修改"></a>1.3.4系统自启动文件修改</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# echo &quot;echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled&quot; &gt;&gt; &#x2F;etc&#x2F;rc.local</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-4-禁用-SELinux-和-Firewall【每个节点-root-用户下执行】"><a href="#1-4-禁用-SELinux-和-Firewall【每个节点-root-用户下执行】" class="headerlink" title="1.4 禁用 SELinux 和 Firewall【每个节点 root 用户下执行】"></a>1.4 禁用 SELinux 和 Firewall【每个节点 root 用户下执行】</h3><p>查看 SELinux 状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">getenforce</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果状态为<code>Disabled</code>，说明 selinux 已经被禁用；状态为<code>permissive</code>，说明 selinux 为临时关闭；状态为<code>enforcing</code>，说明 selinux 处于启动状态。</p>
</blockquote>
<p>永久修改 SELinux 状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# vim &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line"></span><br><span class="line">修改如下配置：</span><br><span class="line">SELINUX&#x3D;disabled</span><br></pre></td></tr></table></figure>
<blockquote>
<p>本步骤需要重启操作系统才能生效，但此时不需要重启，等其他基础参数修改完，文档会指导进行重启操作。<br>可通过 <code>[root@node1]# setenforce 0</code> 使得该配置临时生效。</p>
</blockquote>
<p>禁用 Firewall 和网络管理工具</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# systemctl disable firewalld</span><br><span class="line">[root@node1]# systemctl stop firewalld</span><br><span class="line">[root@node1]# systemctl statue firewalld</span><br><span class="line">返回结果如下即表示执行成功：</span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;firewalld.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:firewalld(1)</span><br><span class="line"></span><br><span class="line">[root@node1]# systemctl disable NetworkManager.service</span><br><span class="line">[root@node1]# systemctl stop NetworkManager.service</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-5-配置主机名及-hosts【每个节点-root-用户下执行】"><a href="#1-5-配置主机名及-hosts【每个节点-root-用户下执行】" class="headerlink" title="1.5 配置主机名及 hosts【每个节点 root 用户下执行】"></a>1.5 配置主机名及 hosts【每个节点 root 用户下执行】</h3><p>配置主机名（以下命令只适用于 CentOS7 及以上版本）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# hostnamectl set-hostname node1</span><br></pre></td></tr></table></figure>
<p>选择一台节点，修改 <code>/etc/hosts</code> 文件，先清空文件内容，然后添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">添加所有节点的IP和主机名对应关系</span><br><span class="line">示例：</span><br><span class="line">127.0.0.1 localhost</span><br><span class="line">172.168.1.1	node1</span><br><span class="line">172.168.1.2	node2</span><br><span class="line">172.168.1.3	node3</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：127.0.0.1 localhost 必须放在第一行</p>
</blockquote>
<p>复制该文件到其他所有节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">示例：</span><br><span class="line">[root@node1]# scp &#x2F;etc&#x2F;hosts root@172.168.1.2:&#x2F;etc</span><br><span class="line">[root@node1]# scp &#x2F;etc&#x2F;hosts root@172.168.1.3:&#x2F;etc</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-6-修改ssh参数配置【每个节点-root-用户下执行】"><a href="#1-6-修改ssh参数配置【每个节点-root-用户下执行】" class="headerlink" title="1.6 修改ssh参数配置【每个节点 root 用户下执行】"></a>1.6 修改ssh参数配置【每个节点 root 用户下执行】</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# vim &#x2F;etc&#x2F;ssh&#x2F;ssh_config</span><br><span class="line">修改配置参数：</span><br><span class="line">StrickHostKeyChecking no</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-7-重启操作系统【每个节点-root-用户下执行】"><a href="#1-7-重启操作系统【每个节点-root-用户下执行】" class="headerlink" title="1.7 重启操作系统【每个节点 root 用户下执行】"></a>1.7 重启操作系统【每个节点 root 用户下执行】</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# reboot</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-8-配置本地源【在-Ambari-Server-所在节点的-root-用户下执行】"><a href="#1-8-配置本地源【在-Ambari-Server-所在节点的-root-用户下执行】" class="headerlink" title="1.8 配置本地源【在 Ambari Server 所在节点的 root 用户下执行】"></a>1.8 配置本地源【在 Ambari Server 所在节点的 root 用户下执行】</h3><p>检查操作系统版本，务必和即将挂载的镜像版本一致，否则会报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# lsb_release -a</span><br><span class="line">Description:    CentOS Linux release 7.5.1804 (Core) </span><br><span class="line">Release:        7.5.1804</span><br><span class="line">Codename:       Core</span><br></pre></td></tr></table></figure>

<p>检查 8080 端口是否被占用，Ambari需要使用 8080 端口：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# netstat -an |grep 8080</span><br><span class="line">如果没有任何返回，即表示端口没有被占用</span><br></pre></td></tr></table></figure>

<p>新建目录，用于存放系统镜像：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# mkdir &#x2F;centos7.5</span><br></pre></td></tr></table></figure>
<p>上传系统镜像至新建目录</p>
<p>新建挂载目录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# mkdir -p &#x2F;var&#x2F;www&#x2F;html&#x2F;mnt</span><br></pre></td></tr></table></figure>
<p>挂载镜像：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# mount -o loop &#x2F;centos7.5&#x2F;CentOS-7-x86_64-Everything-1804.iso &#x2F;var&#x2F;www&#x2F;html&#x2F;mnt</span><br></pre></td></tr></table></figure>
<p>添加开机自动挂载：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# vim &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br><span class="line">添加如下内容，保存：</span><br><span class="line">mount -o loop &#x2F;7.5&#x2F;CentOS-7-x86_64-Everything-1804.iso &#x2F;var&#x2F;www&#x2F;html&#x2F;mnt</span><br><span class="line"></span><br><span class="line">[root@node1]# chmod +x &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br></pre></td></tr></table></figure>
<p>保留cdrom.repo，删除其它源文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# cd &#x2F;etc&#x2F;yum.repos.d&#x2F;</span><br><span class="line">[root@node1]# tar zcvf repo.tar.gz .&#x2F;*</span><br><span class="line">[root@node1]# rm -rf CentOS*</span><br></pre></td></tr></table></figure>
<p>修改yum源（如果没有直接新建）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# vim &#x2F;etc&#x2F;yum.repos.d&#x2F;cdrom.repo</span><br><span class="line"></span><br><span class="line">添加&#x2F;修改如下内容：</span><br><span class="line">[cdrom]</span><br><span class="line">name&#x3D;cdrom</span><br><span class="line">baseurl&#x3D;file:&#x2F;&#x2F;&#x2F;var&#x2F;www&#x2F;html&#x2F;mnt</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;0</span><br></pre></td></tr></table></figure>
<p>清空原先的 yum 缓存</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# yum clean all</span><br><span class="line">出现以下内容：</span><br><span class="line">Loaded plugins: fastestmirror, langpacks</span><br><span class="line">Cleaning repos: cdrom</span><br><span class="line">Cleaning up everything</span><br><span class="line">Maybe you want: rm -rf &#x2F;var&#x2F;cache&#x2F;yum, to also free up space taken by orphaned data from disabled or removed repos</span><br><span class="line">Cleaning up list of fastest mirrors</span><br><span class="line"></span><br><span class="line">[root@node1]# yum repolist</span><br><span class="line">出现以下内容：</span><br><span class="line">Loaded plugins: fastestmirror, langpacks</span><br><span class="line">Determining fastest mirrors</span><br><span class="line">cdrom                                                            | 3.6 kB  00:00:00     </span><br><span class="line">(1&#x2F;2): cdrom&#x2F;group_gz                                            | 166 kB  00:00:00     </span><br><span class="line">(2&#x2F;2): cdrom&#x2F;primary_db                                          | 5.9 MB  00:00:00     </span><br><span class="line">repo id                                   repo name                               status</span><br><span class="line">cdrom                                     cdrom                                   9,911</span><br><span class="line">repolist: 9,911</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-9-安装-httpd-服务【在-Ambari-Server-所在节点的-root-用户下执行】"><a href="#1-9-安装-httpd-服务【在-Ambari-Server-所在节点的-root-用户下执行】" class="headerlink" title="1.9 安装 httpd 服务【在 Ambari Server 所在节点的 root 用户下执行】"></a>1.9 安装 httpd 服务【在 Ambari Server 所在节点的 root 用户下执行】</h3><p>检查 httpd 是否已安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum info httpd</span><br><span class="line"></span><br><span class="line">显示以下内容：</span><br><span class="line">Installed Packages</span><br><span class="line">Name : httpd</span><br><span class="line">Arch : x86_64</span><br><span class="line">Version : 2.4.6</span><br><span class="line">Release : 31.el7.centos</span><br><span class="line">Size : 9.4 M</span><br><span class="line">Repo : installed</span><br><span class="line">From repo : anaconda</span><br><span class="line">Summary : Apache HTTP Server</span><br><span class="line">URL : http:&#x2F;&#x2F;httpd.apache.org&#x2F;</span><br><span class="line">License : ASL 2.0</span><br><span class="line">Description : The Apache HTTP Server is a powerful, efficient, and extensible</span><br><span class="line">: web server.</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果显示：<code>No matching Packages to list</code> 等信息，则表示没有安装</p>
</blockquote>
<p>安装服务（如果已经安装，则跳过该步）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# yum install -y httpd</span><br></pre></td></tr></table></figure>
<p>启动 httpd 服务，被设置开机自启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# systemctl start httpd</span><br><span class="line">[root@node1]# systemctl enable httpd</span><br><span class="line">[root@node1]# systemctl status httpd</span><br></pre></td></tr></table></figure>
<p><img src="http://static.zybuluo.com/winyter/c5wag9wgptk6b7gnlb7r57u1/image.png" alt="image.png-55.1kB"></p>
<hr>
<h3 id="1-10-安装-wget-服务【在所有部署-Ambari-Agent-节点的-root-用户下执行】"><a href="#1-10-安装-wget-服务【在所有部署-Ambari-Agent-节点的-root-用户下执行】" class="headerlink" title="1.10 安装 wget 服务【在所有部署 Ambari Agent 节点的 root 用户下执行】"></a>1.10 安装 wget 服务【在所有部署 Ambari Agent 节点的 root 用户下执行】</h3><p>保留cdrom.repo，删除其它源文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node2]# cd &#x2F;etc&#x2F;yum.repos.d&#x2F;</span><br><span class="line">[root@node2]# tar zcvf repo.tar.gz .&#x2F;*</span><br><span class="line">[root@node2]# rm -rf CentOS*</span><br></pre></td></tr></table></figure>
<p>修改 yum 源</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node2]# vim cdrom.repo</span><br><span class="line"></span><br><span class="line">[cdrom]</span><br><span class="line">name&#x3D;cdrom</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;10.0.0.1&#x2F;mnt&#x2F;    &#x2F;&#x2F; 10.0.0.1 为 ambari server 节点的 IP</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;0</span><br></pre></td></tr></table></figure>
<p>清空原先的yum缓存</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node2]# yum clean all</span><br><span class="line">Loaded plugins: fastestmirror, langpacks</span><br><span class="line">Cleaning repos: cdrom</span><br><span class="line">Cleaning up everything</span><br><span class="line">Maybe you want: rm -rf &#x2F;var&#x2F;cache&#x2F;yum, to also free up space taken by orphaned data from disabled or removed repos</span><br><span class="line">Cleaning up list of fastest mirrors</span><br><span class="line"></span><br><span class="line">[root@node2]# yum repolist</span><br><span class="line">Loaded plugins: fastestmirror, langpacks</span><br><span class="line">Determining fastest mirrors</span><br><span class="line">cdrom                                                            | 3.6 kB  00:00:00     </span><br><span class="line">(1&#x2F;2): cdrom&#x2F;group_gz                                            | 166 kB  00:00:00     </span><br><span class="line">(2&#x2F;2): cdrom&#x2F;primary_db                                          | 5.9 MB  00:00:00     </span><br><span class="line">repo id                                   repo name                               status</span><br><span class="line">cdrom                                     cdrom                                   9,911</span><br><span class="line">repolist: 9,911</span><br></pre></td></tr></table></figure>
<p>在ambari agent的所有服务器上安装wget服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node2]# yum install -y wget</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-11-统一服务器时区【所有节点的-root-用户下执行】"><a href="#1-11-统一服务器时区【所有节点的-root-用户下执行】" class="headerlink" title="1.11 统一服务器时区【所有节点的 root 用户下执行】"></a>1.11 统一服务器时区【所有节点的 root 用户下执行】</h3><p>注意点：此项为配置时间同步使用，确保每台机器在同一个时区，本手册统一时区在Asia/Shanghai，通过date命令查看。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@rhino141 ~]# date</span><br><span class="line">Fri Mar  1 09:03:55 CST 2019</span><br></pre></td></tr></table></figure>
<blockquote>
<p>出现&quot;CST&quot;字样说明时区是在“Asia/Shanghai”，不需要执行修改时区的操作。如果显示不是的话，则按下面步骤设置“Asia/Shanghai”时区。</p>
</blockquote>
<p>方法一：通过底层文件修改时区（推荐）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai &#x2F;etc&#x2F;localtime</span><br></pre></td></tr></table></figure>

<p>方法二：通过timedatectl修改时区（不推荐）<br>列出所有时区</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">timedatectl list-timezones</span><br></pre></td></tr></table></figure>
<p>将硬件时钟调整为与本地时钟一致, 0 为设置为 UTC 时间</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">timedatectl set-local-rtc 1</span><br></pre></td></tr></table></figure>
<p>设置系统时区为上海</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">timedatectl set-timezone Asia&#x2F;Shanghai</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-12-安装-NTP-服务端【在-ambari-server-节点的-root-用户下执行】"><a href="#1-12-安装-NTP-服务端【在-ambari-server-节点的-root-用户下执行】" class="headerlink" title="1.12 安装 NTP 服务端【在 ambari server 节点的 root 用户下执行】"></a>1.12 安装 NTP 服务端【在 ambari server 节点的 root 用户下执行】</h3><p>检查 ntp 服务是否已经安装（一般是已经安装好的）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# rpm -q ntp</span><br><span class="line">ntp-4.2.6p5-29.el7.centos.x86_64</span><br></pre></td></tr></table></figure>
<p>如果没有安装则执行安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# yum install ntp ntpdate –y</span><br></pre></td></tr></table></figure>
<p>修改配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# vim &#x2F;etc&#x2F;ntp.conf</span><br><span class="line"></span><br><span class="line">在 #Hosts on local network are less restricted. 下面添加如下语句：</span><br><span class="line">restrict 172.168.1.0 mask 255.255.255.0 nomodifynotrap</span><br><span class="line">&gt; 172.168.1.0 表示：只允许 172.168.1.x 网段内的机器使用本机的服务端同步时间</span><br><span class="line"></span><br><span class="line">在 #manycastclient 239.255.254.254 autokey # manycast client 下面添加如下语句：</span><br><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br><span class="line">&gt; 本段配置表示：外界同步源无法联系时，使用本地时间为同步服务。</span><br><span class="line"></span><br><span class="line">注释如下语句，并新增本机为同步源：</span><br><span class="line">#server 0.centos.pool.ntp.org iburst</span><br><span class="line">#server 1.centos.pool.ntp.org iburst</span><br><span class="line">#server 2.centos.pool.ntp.org iburst</span><br><span class="line">#server 3.centos.pool.ntp.org iburst</span><br><span class="line">server 172.168.1.1</span><br></pre></td></tr></table></figure>
<p>启动服务，配置开机自启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node2]# systemctl start ntpd </span><br><span class="line">[root@node2]# systemctl enable ntpd</span><br><span class="line">[root@node2]# systemctl status ntpd</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-13-安装-NTP-客户端【在除-ambari-server-节点以外的其他所有节点的-root-用户下执行】"><a href="#1-13-安装-NTP-客户端【在除-ambari-server-节点以外的其他所有节点的-root-用户下执行】" class="headerlink" title="1.13 安装 NTP 客户端【在除 ambari server 节点以外的其他所有节点的 root 用户下执行】"></a>1.13 安装 NTP 客户端【在除 ambari server 节点以外的其他所有节点的 root 用户下执行】</h3><p>检查 ntp 服务是否已经安装（一般是已经安装好的）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# rpm -q ntp</span><br><span class="line">ntp-4.2.6p5-29.el7.centos.x86_64</span><br></pre></td></tr></table></figure>
<p>如果没有安装则执行安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# yum install ntp ntpdate –y</span><br></pre></td></tr></table></figure>
<p>修改配置文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node2]# vim &#x2F;etc&#x2F;ntp.conf</span><br><span class="line"></span><br><span class="line">注释如下语句，并新增 ntp 服务端为同步源：</span><br><span class="line">#server 0.centos.pool.ntp.org iburst</span><br><span class="line">#server 1.centos.pool.ntp.org iburst</span><br><span class="line">#server 2.centos.pool.ntp.org iburst</span><br><span class="line">#server 3.centos.pool.ntp.org iburst</span><br><span class="line">server 172.168.1.1</span><br><span class="line">&gt; 172.168.1.1 即上一步配置的 ntp 服务端的节点 IP</span><br></pre></td></tr></table></figure>
<p>启动服务，配置开机自启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node2]# systemctl start ntpd </span><br><span class="line">[root@node2]# systemctl enable ntpd</span><br><span class="line">[root@node2]# systemctl status ntpd</span><br></pre></td></tr></table></figure>
<p>在客户端查看同步状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node2]# ntpq -p</span><br><span class="line">     remote           refid      st t when poll reach   delay   offset  jitter</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">*node1         LOCAL(0)        11 u   29  128  377    0.187   -0.009   0.009</span><br><span class="line"> LOCAL(0)        .LOCL.          10 l 112m   64    0    0.000    0.000   0.000</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-14-配置root用户无密码访问【所有节点-root-用户执行】"><a href="#1-14-配置root用户无密码访问【所有节点-root-用户执行】" class="headerlink" title="1.14 配置root用户无密码访问【所有节点 root 用户执行】"></a>1.14 配置root用户无密码访问【所有节点 root 用户执行】</h3><p>以root用户登录所有服务器，修改配置文件ssh_config，关闭首次无密码访问询问：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# vim &#x2F;etc&#x2F;ssh&#x2F;ssh_config</span><br><span class="line">StrictHostKeyChecking no</span><br></pre></td></tr></table></figure>
<p>设置各台服务器的root用户无密码访问，执行如下命令，出现提示后按回车:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>
<p>执行如下命令，设置各台机器的无密码访问，注意每台机器都要执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# ssh-copy-id root@172.168.1.1</span><br><span class="line">[root@node1]# ssh-copy-id root@172.168.1.2</span><br><span class="line">[root@node1]# ssh-copy-id root@172.168.1.3</span><br></pre></td></tr></table></figure>
<p>使用ssh命令，测试无密码访问是否成功</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">示例：</span><br><span class="line">[root@node1]# ssh root@172.168.1.2</span><br><span class="line">如果没有任何提示，直接登录到该节点上，即表示无密码访问成功</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-15-配置rhino用户无密码访问【所有节点-rhino-用户执行】"><a href="#1-15-配置rhino用户无密码访问【所有节点-rhino-用户执行】" class="headerlink" title="1.15 配置rhino用户无密码访问【所有节点 rhino 用户执行】"></a>1.15 配置rhino用户无密码访问【所有节点 rhino 用户执行】</h3><p>新建rhino用户并为它设定密码，<strong>root用户操作</strong>（如果已经新建了rhino，则可以跳过本步骤）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1]# useradd -g root -md &#x2F;home&#x2F;rhino  rhino</span><br><span class="line">[root@node1]# passwd rhino</span><br><span class="line">Changing password for user rhino.</span><br><span class="line">New password: xxxxxxxx</span><br><span class="line">BAD PASSWORD: The password is shorter than 8 characters</span><br><span class="line">Retype new password: xxxxxxxx</span><br><span class="line">passwd: all authentication tokens updated successfully.</span><br></pre></td></tr></table></figure>
<p>以rhino用户登录所有服务器，输入命令：ssh-keygen -t rsa生成public key(id_rsa.pub)和private key(id_rsa)文件。当出现”Enter Passphrase”的提示时直接按回车</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]# ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>
<p>执行如下命令，设置各台机器的无密码访问，注意每台机器都要执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]# ssh-copy-id root@172.168.1.1</span><br><span class="line">[rhino@node1 ~]# ssh-copy-id root@172.168.1.2</span><br><span class="line">[rhino@node1 ~]# ssh-copy-id root@172.168.1.3</span><br></pre></td></tr></table></figure>
<p>使用ssh命令，测试无密码访问是否成功</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">示例：</span><br><span class="line">[rhino@node1 ~]# ssh rhino@172.168.1.2</span><br><span class="line">如果没有任何提示，直接登录到该节点上，即表示无密码访问成功</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="1-2-安装-JDK"><a href="#1-2-安装-JDK" class="headerlink" title="1.2 安装 JDK"></a>1.2 安装 JDK</h2><hr>
<h3 id="1-2-1-JDK安装及环境配置【选择任一节点的-root-用户下执行】"><a href="#1-2-1-JDK安装及环境配置【选择任一节点的-root-用户下执行】" class="headerlink" title="1.2.1 JDK安装及环境配置【选择任一节点的 root 用户下执行】"></a>1.2.1 JDK安装及环境配置【选择任一节点的 root 用户下执行】</h3><p>将安装包 <code>jdk1.8.0_181.tar.gz</code> 上传至 <code>/home/rhino</code> 目录下</p>
<p>解压 <code>jdk1.8.0_181.tar.gz</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]# cd &#x2F;home&#x2F;rhino</span><br><span class="line">[rhino@node1 ~]# tar -zxvf jdk1.8.0_181.tar.gz</span><br></pre></td></tr></table></figure>
<p>配置 JAVA_HOME 环境变量 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]# vim ~&#x2F;.bashrc </span><br><span class="line">添加如下内容：</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181 </span><br><span class="line">export PATH&#x3D;$JAVA_HOME&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure>
<p>拷贝 JDK 及环境变量到其他节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]# scp -r ~&#x2F;jdk1.8.0_181 rhino@node2:~</span><br><span class="line">[rhino@node1 ~]# scp -r ~&#x2F;jdk1.8.0_181 rhino@node3:~</span><br><span class="line">[rhino@node1 ~]# scp ~&#x2F;.bashrc rhino@node2:~</span><br><span class="line">[rhino@node1 ~]# scp ~&#x2F;.bashrc rhino@node3:~</span><br></pre></td></tr></table></figure>
<p>使每台机器环境变量生效（每台节点都需要执行）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]# source ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-2-2-环境配置及修改权限【选择任一节点的-root-用户下执行】"><a href="#1-2-2-环境配置及修改权限【选择任一节点的-root-用户下执行】" class="headerlink" title="1.2.2 环境配置及修改权限【选择任一节点的 root 用户下执行】"></a>1.2.2 环境配置及修改权限【选择任一节点的 root 用户下执行】</h3><p>root用户下的 <code>~/.bashrc</code> 文件需要配置JDK相关的环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# vim ~&#x2F;.bashrc </span><br><span class="line">添加如下内容：</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181</span><br><span class="line">export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin</span><br><span class="line">export LD_LIBRARY_PATH&#x3D;$LD_LIBRARY_PATH:$JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;amd64&#x2F;server&#x2F;</span><br></pre></td></tr></table></figure>
<p>root用户下的 <code>/etc/profile</code> 文件添加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# vim &#x2F;etc&#x2F;profile </span><br><span class="line">添加如下内容：</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181</span><br><span class="line">export JRE_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181&#x2F;jre</span><br><span class="line">export CLASSPATH&#x3D;.:&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181&#x2F;lib:&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181&#x2F;jre&#x2F;lib</span><br><span class="line">export PATH&#x3D;&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181&#x2F;bin:&#x2F;bin:&#x2F;usr&#x2F;lib64&#x2F;qt-3.3&#x2F;bin:&#x2F;root&#x2F;perl5&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;bin&#x2F;:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;root&#x2F;bin</span><br></pre></td></tr></table></figure>
<p>拷贝JDK及环境变量到其他节点 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# scp &#x2F;etc&#x2F;profile root@node2:&#x2F;etc</span><br><span class="line">[root@node1 ~]# scp &#x2F;etc&#x2F;profile root@node3:&#x2F;etc</span><br><span class="line">[root@node1 ~]# scp ~&#x2F;.bashrc root@node2:~</span><br><span class="line">[root@node1 ~]# scp ~&#x2F;.bashrc root@node3:~</span><br></pre></td></tr></table></figure>
<p>使每台节点环境变量生效（每台节点都需要执行）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]$ source ~&#x2F;.bashrc</span><br><span class="line">[root@node1 ~]$ source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure>
<p>为 <code>/home/</code> 目录下的 rhino 目录加 755 权限, 而不是 777，否则无密码访问不正常（每台节点都需要执行）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# cd &#x2F;home&#x2F;</span><br><span class="line">[root@node1 home]# chmod 755 rhino</span><br></pre></td></tr></table></figure>
<p>为 <code>rhino/jdk1.8.0_181/bin/java</code> 加 777 权限（每台节点都需要执行）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# cd &#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181&#x2F;bin</span><br><span class="line">[root@node1 bin]# chmod 777 java</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="1-3-MySQL数据库安装"><a href="#1-3-MySQL数据库安装" class="headerlink" title="1.3 MySQL数据库安装"></a>1.3 MySQL数据库安装</h2><blockquote>
<p>为了方便部署，这里将 Hive元数据库/ambari/业务 所使用的 MySQL 集中在一个 MySQL 上，使用 keepalive 和 mysql 主备保证 MySQL 数据库的高可用和数据安全。</p>
</blockquote>
<h3 id="1-3-1-节点规划及端口检查"><a href="#1-3-1-节点规划及端口检查" class="headerlink" title="1.3.1 节点规划及端口检查"></a>1.3.1 节点规划及端口检查</h3><blockquote>
<p><strong>节点规划</strong>：建议在 Ambari Server 机器上安装 MySQL 主节点，选择另一台机器安装 MySQL 备节点；端口默认 3306。本文档假设在：node1和node2两台节点上部署</p>
</blockquote>
<p>检查 3306 端口是否被占用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# netstat -anp|grep 3306</span><br></pre></td></tr></table></figure>
<p>若3306端口被占用，则使用3307端口，以此类推；服务端口被占用的情况下，端口循序向上叠加+1。<br>另外还需要规划 MySQL 的密码，主要配置 root 和 rhino 两个数据库用户的密码，这里需要注意，数据库用户：root和rhino，与操作系统用户：root和rhino是不一样的概念，不要搞混。</p>
<h3 id="1-3-2-安装部署Mysql双主备【在规划安装-MySQL-的两台节点的-rhino-用户下操作】"><a href="#1-3-2-安装部署Mysql双主备【在规划安装-MySQL-的两台节点的-rhino-用户下操作】" class="headerlink" title="1.3.2 安装部署Mysql双主备【在规划安装 MySQL 的两台节点的 rhino 用户下操作】"></a>1.3.2 安装部署Mysql双主备【在规划安装 MySQL 的两台节点的 rhino 用户下操作】</h3><p>将<code>mysql-5.6.25-linux-x86_64.tar.gz</code>安装包上传到 node1 机器的<code>/home/rhino</code>目录下<br>解压，拷贝至 node2 节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino</span><br><span class="line">[rhino@node1 ~]$ tar -zxvf mysql-5.6.25-linux-x86_64.tar.gz</span><br><span class="line">[rhino@node1 ~]$ scp -r ~&#x2F;mysql-5.6.25-linux-x86_64 rhino@node2:~</span><br></pre></td></tr></table></figure>
<p>新建数据目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]$ mkdir -p &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;lock&#x2F;subsys</span><br><span class="line">[rhino@node1 ~]$ mkdir -p &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp</span><br></pre></td></tr></table></figure>
<p>修改配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]$ vim &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;my.cnf</span><br></pre></td></tr></table></figure>
<p>配置文件如下，按照说明，进行修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[client]</span><br><span class="line">port&#x3D;3306    # mysql端口号，填入规划的端口号即可</span><br><span class="line"></span><br><span class="line">default-character-set&#x3D;utf8     # 字符集，默认为utf8即可</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">join_buffer_size&#x3D;16M</span><br><span class="line"></span><br><span class="line">sort_buffer_size&#x3D;16M</span><br><span class="line"></span><br><span class="line">read_rnd_buffer_size&#x3D;16M</span><br><span class="line"></span><br><span class="line">max_allowed_packet&#x3D;200M     # 该配置按照此处的修改</span><br><span class="line"></span><br><span class="line">max_connections&#x3D;1000      # 该配置按照此处的修改</span><br><span class="line"></span><br><span class="line">port&#x3D;3306        # mysql端口号，填入规划的端口号即可</span><br><span class="line"></span><br><span class="line">socket&#x3D; &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock</span><br><span class="line"></span><br><span class="line">character-set-server&#x3D;utf8  # 字符集，默认为utf8即可</span><br><span class="line"></span><br><span class="line">basedir&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64    # 基础目录，为mysql安装目录，需要确认实际目录路径</span><br><span class="line"></span><br><span class="line">datadir&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata   # 数据目录，一般在mysql安装目录下的mydata目录，需要确认实际目录路径</span><br><span class="line"></span><br><span class="line">explicit_defaults_for_timestamp&#x3D;true</span><br><span class="line"></span><br><span class="line">binlog_format&#x3D;MIXED</span><br><span class="line"></span><br><span class="line">log-bin&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;master-bin   # 需要确认目录路径</span><br><span class="line"></span><br><span class="line">server_id&#x3D;131 #两台主备mysql id 需要不一样,在一个网段内如果有其他mysql做主备，那么它们的id值都不能相同。</span><br><span class="line"></span><br><span class="line">auto_increment_increment&#x3D;1</span><br><span class="line"></span><br><span class="line">auto_increment_offset&#x3D;1</span><br><span class="line"></span><br><span class="line">#binlog-do-db&#x3D;test</span><br><span class="line"></span><br><span class="line">#binlog-ignore-db&#x3D;mysql</span><br><span class="line"></span><br><span class="line">#replicate-do-db&#x3D;test</span><br><span class="line"></span><br><span class="line">#replicate-ignore-db&#x3D;mysql</span><br><span class="line"></span><br><span class="line">log-slave-updates</span><br><span class="line"></span><br><span class="line">slave-skip-errors&#x3D;all</span><br><span class="line"></span><br><span class="line">sql_mode&#x3D;NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES</span><br></pre></td></tr></table></figure>

<p>修改<code>mysql.server</code>配置文件，在配置项<code>basedir</code> 、<code>datadir</code>、<code>lockdir</code>第一次出现的位置修改</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;support-files</span><br><span class="line">[rhino@node1 ~]$ vim mysql.server</span><br><span class="line"></span><br><span class="line">参照如下内容修改，实际路径需要根据实际情况确认：</span><br><span class="line">basedir&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;</span><br><span class="line">datadir&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata</span><br><span class="line">lockdir&#x3D;&#39;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;lock&#x2F;subsys&#39;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-3-3-初始化及启动Mysql数据库【两台-MySQL-节点的-rhino-用户下执行】"><a href="#1-3-3-初始化及启动Mysql数据库【两台-MySQL-节点的-rhino-用户下执行】" class="headerlink" title="1.3.3 初始化及启动Mysql数据库【两台 MySQL 节点的 rhino 用户下执行】"></a>1.3.3 初始化及启动Mysql数据库【两台 MySQL 节点的 rhino 用户下执行】</h3><p>为两台机器分别用mysql用户初始化数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64</span><br><span class="line">[rhino@node1 ~]$ </span><br><span class="line">scripts&#x2F;mysql_install_db  --defaults-file&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;my.cnf --datadir&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata  --basedir&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64;</span><br></pre></td></tr></table></figure>
<p>为两台机器上启动mysql</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]$ </span><br><span class="line">nohup bin&#x2F;mysqld_safe --defaults-file&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;my.cnf  --user&#x3D;rhino &amp;</span><br></pre></td></tr></table></figure>
<p>为两台机器上设置root密码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]$ bin&#x2F;mysqladmin --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock -P3306  -uroot  password</span><br><span class="line">输入root用户密码：xxxxxxxx</span><br></pre></td></tr></table></figure>
<p>进入mysql shell，验证mysql是否启动成功。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]$ cd mysql-5.6.25-linux-x86_64&#x2F;</span><br><span class="line">[rhino@node1 ~]$ </span><br><span class="line">bin&#x2F;mysql --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock -P3306 -uroot -p</span><br><span class="line"></span><br><span class="line">输入上一步修改完成后的root用户的密码。</span><br></pre></td></tr></table></figure>

<p>停止mysql命令(如果需要停止mysql时执行，不需要停止可以跳过本步骤)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]$ bin&#x2F;mysqladmin --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock -P3306 -uroot  -p  shutdown</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-3-4-登录Mysql设置双主备模式【两台-MySQL-节点的-rhino-用户下执行】"><a href="#1-3-4-登录Mysql设置双主备模式【两台-MySQL-节点的-rhino-用户下执行】" class="headerlink" title="1.3.4 登录Mysql设置双主备模式【两台 MySQL 节点的 rhino 用户下执行】"></a>1.3.4 登录Mysql设置双主备模式【两台 MySQL 节点的 rhino 用户下执行】</h3><p>登录 node1 节点的 MySQL</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~] cd mysql-5.6.25-linux-x86_64</span><br><span class="line">[rhino@node1 ~]$ bin&#x2F;mysql --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock -P3306 -uroot -pxxxxxxxx</span><br></pre></td></tr></table></figure>
<p>执行如下语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &#39;backup&#39;@&#39;172.168.1.2&#39; IDENTIFIED BY &#39;backup&#39;;</span><br><span class="line">mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &#39;root&#39;@&#39;*&#39; IDENTIFIED BY &#39;xxxxxxxx&#39;;</span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;</span><br><span class="line">mysql&gt; show master status;</span><br><span class="line"></span><br><span class="line">&gt; 172.168.1.2 即另外一台 mysql 节点的 IP</span><br></pre></td></tr></table></figure>
<p>执行完毕后，会有以下的回显，后面的步骤需要使用到这个回显里的内容：<br><img src="http://static.zybuluo.com/winyter/qex2g3kui8jx7f9qnw128ig0/image.png" alt="image.png-37.1kB"></p>
<p>登录 node2 的 MySQL</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node2 ~] cd mysql-5.6.25-linux-x86_64</span><br><span class="line">[rhino@node2 ~]$ bin&#x2F;mysql --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock -P3306 -uroot -pxxxxxxxx</span><br></pre></td></tr></table></figure>
<p>执行如下语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &#39;backup&#39;@&#39;172.168.1.1&#39; IDENTIFIED BY &#39;backup&#39;;</span><br><span class="line">mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &#39;root&#39;@&#39;*&#39; IDENTIFIED BY &#39;xxxxxxxx&#39;;</span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;</span><br><span class="line">mysql&gt; show master status;</span><br><span class="line"></span><br><span class="line">&gt; 172.168.1.1 即另外一台 mysql 节点的 IP</span><br></pre></td></tr></table></figure>
<p>执行完毕后，会有以下的回显，后面的步骤需要使用到这个回显里的内容：<br><img src="http://static.zybuluo.com/winyter/vwbcgjs0yltlzumljju68ohi/image.png" alt="image.png-33.5kB"></p>
<p>为 node1 设置主备模式，在 node1 节点的 mysql 内执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt;</span><br><span class="line">CHANGE MASTER TO MASTER_HOST&#x3D;&#39;172.168.1.2&#39;,  MASTER_USER&#x3D;&#39;backup&#39;,MASTER_PASSWORD&#x3D;&#39;backup&#39;, MASTER_LOG_FILE&#x3D;&#39;master-bin.000005&#39;,MASTER_LOG_POS&#x3D;772;</span><br><span class="line"></span><br><span class="line">&gt; 此处 MASTER_LOG_FILE 和 MASTER_LOG_POS 的值都是从上面得到的 node2 的状态表格中获取。</span><br><span class="line"></span><br><span class="line">mysql&gt; start slave;</span><br></pre></td></tr></table></figure>
<p>为 node2 设置主备模式，在 node2 节点的 mysql 内执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt;</span><br><span class="line">CHANGE MASTER TO MASTER_HOST&#x3D;&#39;172.168.1.1&#39;,  MASTER_USER&#x3D;&#39;backup&#39;,MASTER_PASSWORD&#x3D;&#39;backup&#39;, MASTER_LOG_FILE&#x3D;&#39;master-bin.000004&#39;,MASTER_LOG_POS&#x3D;625;</span><br><span class="line"></span><br><span class="line">&gt; 此处 MASTER_LOG_FILE 和 MASTER_LOG_POS 的值都是从上面得到的 node1 的状态表格中获取。</span><br><span class="line"></span><br><span class="line">mysql&gt; start slave;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-3-5-验证Mysql双主备模式【两台节点-MySQL-上执行】"><a href="#1-3-5-验证Mysql双主备模式【两台节点-MySQL-上执行】" class="headerlink" title="1.3.5 验证Mysql双主备模式【两台节点 MySQL 上执行】"></a>1.3.5 验证Mysql双主备模式【两台节点 MySQL 上执行】</h3><p>在 node1 上创建数据库及表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; create database mysqltest;use  mysqltest;</span><br><span class="line">mysql&gt; create table testtab(id int);insert into testtab values (1), (2);</span><br></pre></td></tr></table></figure>
<p>在 node2 上查看数据库及表是否已经同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; show databases;use mysqltest;</span><br><span class="line">mysql&gt; show tables;select * from testtab;</span><br></pre></td></tr></table></figure>
<p>在 node2 上删除mysqltest数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; drop database mysqltest;</span><br></pre></td></tr></table></figure>
<p>查看 node1 上mysqltest数据库是否也已经删除成功</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-3-6-为Mysql添加rhino用户【主-MySQL-节点-rhino-用户下执行】"><a href="#1-3-6-为Mysql添加rhino用户【主-MySQL-节点-rhino-用户下执行】" class="headerlink" title="1.3.6 为Mysql添加rhino用户【主 MySQL 节点 rhino 用户下执行】"></a>1.3.6 为Mysql添加rhino用户【主 MySQL 节点 rhino 用户下执行】</h3><p>登陆rhino213的mysql并为其创建rhino用户、登录密码及设置权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &#39;rhino&#39;@&#39;%&#39; IDENTIFIED BY &#39;xxxxxxxx&#39; WITH GRANT OPTION;</span><br><span class="line">mysql&gt;GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;xxxxxxxx&#39; WITH GRANT OPTION;</span><br><span class="line">mysql&gt; delete from mysql.user where user&#x3D;&#39;&#39;;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>
<p>验证能否用新建的rhino用户登陆</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]$&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;bin&#x2F;mysql --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock -urhino -pxxxxxxxx</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-3-7-创建hive-rhino数据库【主-MySQL-节点-rhino-用户下执行】"><a href="#1-3-7-创建hive-rhino数据库【主-MySQL-节点-rhino-用户下执行】" class="headerlink" title="1.3.7 创建hive_rhino数据库【主 MySQL 节点 rhino 用户下执行】"></a>1.3.7 创建hive_rhino数据库【主 MySQL 节点 rhino 用户下执行】</h3><p>使用rhino用户登录mysql</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[rhino@node1 ~]$&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;bin&#x2F;mysql --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock -urhino -pxxxxxxxx</span><br></pre></td></tr></table></figure>
<p>创建hive_rhino数据库，字符集为latin1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; create database hive_rhino CHARACTER SET latin1;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-3-8-添加Mysql开机自启【在-MySQL-主节点-root-用户下执行】"><a href="#1-3-8-添加Mysql开机自启【在-MySQL-主节点-root-用户下执行】" class="headerlink" title="1.3.8 添加Mysql开机自启【在 MySQL 主节点 root 用户下执行】"></a>1.3.8 添加Mysql开机自启【在 MySQL 主节点 root 用户下执行】</h3><p>在rc.local配置文件中增加配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# vim &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br></pre></td></tr></table></figure>
<p>在文件中添加以下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">su - rhino -c &quot;cd &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64; bin&#x2F;mysqld_safe --defaults-file&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;my.cnf  --user&#x3D;rhino &amp;&quot;</span><br></pre></td></tr></table></figure>
<p>赋予可执行权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# chmod +x &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="1-4-部署-Keepalived（MySQL双机切换）"><a href="#1-4-部署-Keepalived（MySQL双机切换）" class="headerlink" title="1.4 部署 Keepalived（MySQL双机切换）"></a>1.4 部署 Keepalived（MySQL双机切换）</h2><p>元数据mysql需要做双机切换，从而当主 mysql机器宕机后可以立即切换到备份的 mysql 上。  </p>
<p>该章节以元数据 mysql 为例介绍了如何通过 keepalived 进程进行双机切换。最终 mysql 客户端连接的是虚拟出来的 ip 地址。但是需要注意，这个虚拟出来的 IP 地址也必须在整个网络环境中，未被使用。  </p>
<p><strong>节点规划</strong>：在 ambari server 机器上安装 keepalived (master)，选择另一台机器安装备 keepalived (slave)。虚拟后的ip地址 172.168.1.4。</p>
<blockquote>
<p>keepalived的安装机器和mysql的安装机器保持一致。</p>
</blockquote>
<hr>
<h3 id="1-4-1-安装部署-MySQL-双机切换【在-MySQL-所在两个节点的-root-用户下执行】"><a href="#1-4-1-安装部署-MySQL-双机切换【在-MySQL-所在两个节点的-root-用户下执行】" class="headerlink" title="1.4.1 安装部署 MySQL 双机切换【在 MySQL 所在两个节点的 root 用户下执行】"></a>1.4.1 安装部署 MySQL 双机切换【在 MySQL 所在两个节点的 root 用户下执行】</h3><blockquote>
<p>根据上面 MySQL 的部署情况，MySQL 分别部署在 node1 和 node2 两个节点</p>
</blockquote>
<p>将 keepalived-1.1.20.tar.gz 安装包上传到 node1 和 node2 的 /home/rhino 目录下  </p>
<p>分别在 node1 和 node2 上执行如下操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# cd &#x2F;root</span><br><span class="line">[root@node1 ~]# tar -zxvf keepalived-1.1.20.tar.gz</span><br><span class="line">[root@node1 ~]# cd keepalived-1.1.20</span><br><span class="line">[root@node1 ~]# chmod 700 sbin&#x2F;keepalived</span><br><span class="line">[root@node1 ~]# chmod 755 etc&#x2F;rc.d&#x2F;init.d&#x2F;keepalived</span><br><span class="line">[root@node1 ~]# chmod 755 etc&#x2F;sysconfig&#x2F;keepalived</span><br><span class="line">[root@node1 ~]# chmod 644 etc&#x2F;keepalived&#x2F;keepalived.conf</span><br><span class="line">[root@node1 ~]# chmod 644 etc&#x2F;keepalived&#x2F;samples&#x2F;*</span><br><span class="line">[root@node1 ~]# chmod 644 share&#x2F;man&#x2F;man5</span><br><span class="line">[root@node1 ~]# chmod 644 share&#x2F;man&#x2F;man8</span><br><span class="line">[root@node1 ~]# chmod 755 bin&#x2F;genhash</span><br><span class="line">[root@node1 ~]# chmod 644 share&#x2F;man&#x2F;man1</span><br><span class="line">[root@node1 ~]# cp etc&#x2F;rc.d&#x2F;init.d&#x2F;keepalived &#x2F;etc&#x2F;init.d&#x2F;keepalived</span><br><span class="line">[root@node1 ~]# cp sbin&#x2F;keepalived &#x2F;usr&#x2F;sbin&#x2F;</span><br><span class="line">[root@node1 ~]# cp etc&#x2F;sysconfig&#x2F;keepalived &#x2F;etc&#x2F;sysconfig&#x2F;</span><br><span class="line">[root@node1 ~]# mkdir -p &#x2F;etc&#x2F;keepalived&#x2F;</span><br><span class="line">[root@node1 ~]# cp &#x2F;root&#x2F;keepalived-1.1.20&#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf &#x2F;etc&#x2F;keepalived</span><br></pre></td></tr></table></figure>
<p>为两台节点的 keepalived 进程添加开机自启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# systemctl enable keepalived.service</span><br><span class="line">[root@node1 ~]# systemctl start keepalived.service</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-4-2-配置Mysql双机切换【在-MySQL-所在两个节点的-root-用户下执行】"><a href="#1-4-2-配置Mysql双机切换【在-MySQL-所在两个节点的-root-用户下执行】" class="headerlink" title="1.4.2 配置Mysql双机切换【在 MySQL 所在两个节点的 root 用户下执行】"></a>1.4.2 配置Mysql双机切换【在 MySQL 所在两个节点的 root 用户下执行】</h3><ul>
<li><p>配置主节点 node1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# vim &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf</span><br></pre></td></tr></table></figure>
<p>参照以下内容配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#! Configuration File for keepalived</span><br><span class="line">#check_all puts all check in one process: including check jboss, vip, mysql, timeout, status</span><br><span class="line">vrrp_script chk_all &#123;</span><br><span class="line"></span><br><span class="line">script &quot;&#x2F;etc&#x2F;keepalived&#x2F;check_mysql.sh&quot; # connects and exits</span><br><span class="line">interval 2 # check every second</span><br><span class="line">weight -2 # default prio: -2 if connect fails</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line"></span><br><span class="line">router_id CM-HA</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line"></span><br><span class="line"># state MASTER</span><br><span class="line">state BACKUP</span><br><span class="line">nopreempt</span><br><span class="line"># 通过ifconfig查看可以绑定的网卡名，是eth0还是eth1要根据实际来选择相应的网卡名称，下面同理。</span><br><span class="line"></span><br><span class="line">interface eth1</span><br><span class="line"></span><br><span class="line"># 同一网段内如果有另外一组keepalived进程，那么virtual_router_id值不能相同；同一组keepalived进程virtual_router_id相同。</span><br><span class="line">virtual_router_id 13</span><br><span class="line">priority 100</span><br><span class="line">advert_int 1</span><br><span class="line">authentication &#123;</span><br><span class="line"></span><br><span class="line">auth_type PASS</span><br><span class="line">auth_pass 8888</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">virtual_ipaddress &#123;</span><br><span class="line"></span><br><span class="line">172.168.1.4&#x2F;24 dev eth1 brd 172.168.1.255 label eth1:1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">track_interface &#123;</span><br><span class="line"></span><br><span class="line">eth1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># notify_master &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh master&quot;</span><br><span class="line"># notify_backup &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh slave&quot;</span><br><span class="line"># notify_fault &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh fault&quot;</span><br><span class="line"># notify_stop &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh stop&quot;</span><br><span class="line">track_script &#123;</span><br><span class="line"></span><br><span class="line">chk_all</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此处的 172.168.1.4 为虚拟的 IP 地址。172.168.1.255 为网关地址，按照实际配置。</p>
</li>
<li><p>配置备节点 node2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@rhino214 ~]# vim &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf</span><br></pre></td></tr></table></figure>
<p>参照以下内容配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#! Configuration File for keepalived</span><br><span class="line">#check_all puts all check in one process: including check jboss, vip, mysql, timeout, status</span><br><span class="line">vrrp_script chk_all &#123;</span><br><span class="line"></span><br><span class="line">script &quot;&#x2F;etc&#x2F;keepalived&#x2F;check_mysql.sh&quot; # connects and exits</span><br><span class="line">interval 2 # check every second</span><br><span class="line">weight -2 # default prio: -2 if connect fails</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line"></span><br><span class="line">router_id CM-HA</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line"></span><br><span class="line"># state MASTER</span><br><span class="line">state BACKUP</span><br><span class="line">nopreempt</span><br><span class="line"># 通过ifconfig查看绑定的网卡名</span><br><span class="line"></span><br><span class="line">interface eth1</span><br><span class="line"></span><br><span class="line"># 同一网段内如果有另外一组keepalived进程，那么virtual_router_id值不能相同；同一组keepalived进程virtual_router_id相同。</span><br><span class="line">virtual_router_id 13</span><br><span class="line">priority 90</span><br><span class="line">advert_int 1</span><br><span class="line">authentication &#123;</span><br><span class="line"></span><br><span class="line">auth_type PASS</span><br><span class="line">auth_pass 8888</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">virtual_ipaddress &#123;</span><br><span class="line"></span><br><span class="line">172.168.1.4&#x2F;24 dev eth1 brd 172.168.1.255 label eth1:1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">track_interface &#123;</span><br><span class="line"></span><br><span class="line">eth1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># notify_master &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh master&quot;</span><br><span class="line"># notify_backup &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh slave&quot;</span><br><span class="line"># notify_fault &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh fault&quot;</span><br><span class="line"># notify_stop &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh stop&quot;</span><br><span class="line">track_script &#123;</span><br><span class="line"></span><br><span class="line">chk_all</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此处的 172.168.1.4 为虚拟的 IP 地址。172.168.1.255 为网关地址，按照实际配置。</p>
</li>
<li><p>配置检测脚本</p>
</li>
</ul>
<blockquote>
<p>分别为主备服务器配置检测脚本。 （两台机器都要操作）<br>若需要检测haproxy，需要添加其中注释的部分，并按照注释修改。</p>
</blockquote>
<p>下面以 node1 节点配置作为示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]#vim &#x2F;etc&#x2F;keepalived&#x2F;check_mysql.sh</span><br></pre></td></tr></table></figure>
<p>参照以下内容配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">MYSQL&#x3D;&quot;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;bin&#x2F;mysql --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock&quot;</span><br><span class="line"></span><br><span class="line">MYSQL_HOST&#x3D;localhost</span><br><span class="line"></span><br><span class="line">MYSQL_USER&#x3D;root # mysql用户名</span><br><span class="line"></span><br><span class="line">MYSQL_PASSWORD&#x3D;root@34#$ # mysql密码</span><br><span class="line"></span><br><span class="line">CHECK_TIME&#x3D;2</span><br><span class="line"></span><br><span class="line">#mysql is working MYSQL_OK is 1 , mysql down MYSQL_OK is 0</span><br><span class="line"></span><br><span class="line">MYSQL_OK&#x3D;1</span><br><span class="line"></span><br><span class="line">#检测haproxy时定义haproxy状态</span><br><span class="line"></span><br><span class="line"># HAPROXY_OK&#x3D;1</span><br><span class="line"></span><br><span class="line">function check_mysql_helth ()&#123;</span><br><span class="line"></span><br><span class="line">$MYSQL -h $MYSQL_HOST -u $MYSQL_USER -p$&#123;MYSQL_PASSWORD&#125; -e &quot;show status;&quot;&gt;&#x2F;dev&#x2F;null 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line">if [ $? -eq 0 ] ;then</span><br><span class="line"></span><br><span class="line">MYSQL_OK&#x3D;1</span><br><span class="line">else</span><br><span class="line">MYSQL_OK&#x3D;0</span><br><span class="line">fi</span><br><span class="line">return</span><br><span class="line">$MYSQL_OK</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#检测haproxy状态的函数</span><br><span class="line"></span><br><span class="line"># function check_haproxy_helth ()&#123;</span><br><span class="line"># haproxyprocessnum&#x3D;ps -ef | grep haproxy | grep -v grep | awk &#39;&#123;print $2&#125;&#39; | wc -l</span><br><span class="line"># if [ $haproxyprocessnum -eq 0 ] ;then</span><br><span class="line"># HAPROXY_OK&#x3D;0</span><br><span class="line"># else</span><br><span class="line"># HAPROXY_OK&#x3D;1</span><br><span class="line"># fi</span><br><span class="line"># return $HAPROXY_OK</span><br><span class="line">#&#125;</span><br><span class="line"></span><br><span class="line">while [ $CHECK_TIME -ne 0 ]</span><br><span class="line"></span><br><span class="line">do</span><br><span class="line"></span><br><span class="line">let &quot;CHECK_TIME -&#x3D; 1&quot;</span><br><span class="line">check_mysql_helth</span><br><span class="line">if [ $MYSQL_OK -eq 1 ] ; then</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CHECK_TIME&#x3D;0</span><br><span class="line">exit 0</span><br><span class="line"></span><br><span class="line">fi</span><br><span class="line">if [ $MYSQL_OK -eq 0 ] &amp;&amp; [ $CHECK_TIME -eq 0 ]</span><br><span class="line">then</span><br><span class="line"></span><br><span class="line">&#x2F;etc&#x2F;init.d&#x2F;keepalived stop</span><br><span class="line">sleep 1</span><br><span class="line">#此处可以添加需要拉起的进程，根据实际需求来定</span><br><span class="line">&#x2F;etc&#x2F;init.d&#x2F;keepalived start</span><br><span class="line">exit 1</span><br><span class="line"></span><br><span class="line">fi</span><br><span class="line">sleep 1</span><br><span class="line"></span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"># 检测haproxy时上面的while…done部分替换为以下注释内</span><br><span class="line"></span><br><span class="line"># while [ $CHECK_TIME -ne 0 ]</span><br><span class="line"># do</span><br><span class="line"></span><br><span class="line"># let &quot;CHECK_TIME -&#x3D; 1&quot;</span><br><span class="line"># check_mysql_helth</span><br><span class="line"># check_haproxy_helth</span><br><span class="line"># if [ $MYSQL_OK -eq 1 ] &amp;&amp; [ $HAPROXY_OK -eq 1 ]; then</span><br><span class="line"></span><br><span class="line"># CHECK_TIME&#x3D;0</span><br><span class="line"># exit 0</span><br><span class="line"></span><br><span class="line"># fi </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># if [ $MYSQL_OK -eq 0 ] || [ $HAPROXY_OK -eq 0 ] &amp;&amp; [ $CHECK_TIME -eq 0 ] </span><br><span class="line"># then</span><br><span class="line"></span><br><span class="line"># &#x2F;etc&#x2F;init.d&#x2F;keepalived stop</span><br><span class="line"># sleep 1</span><br><span class="line"># &#x2F;etc&#x2F;init.d&#x2F;keepalived start</span><br><span class="line"># exit 1</span><br><span class="line"></span><br><span class="line"># fi</span><br><span class="line"># sleep 1</span><br><span class="line"></span><br><span class="line"># done</span><br></pre></td></tr></table></figure>

<ul>
<li>为主备服务器检测脚本添加执行权限 （两台机器都要操作）<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# chmod +x &#x2F;etc&#x2F;keepalived&#x2F;check_mysql.sh [root@rhino214 ~]# chmod +x &#x2F;etc&#x2F;keepalived&#x2F;check_mysql.sh</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="1-4-3-启停keepalived进程【在-MySQL-所在两个节点的-root-用户下执行】"><a href="#1-4-3-启停keepalived进程【在-MySQL-所在两个节点的-root-用户下执行】" class="headerlink" title="1.4.3 启停keepalived进程【在 MySQL 所在两个节点的 root 用户下执行】"></a>1.4.3 启停keepalived进程【在 MySQL 所在两个节点的 root 用户下执行】</h3><p>主备服务器启动keepalived</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# systemctl start keepalived.service</span><br><span class="line">[root@node1 ~]# systemctl status keepalived.service</span><br></pre></td></tr></table></figure>

<p>keepalived 停止命令（需要停止该进程时执行，正常安装启动后不需要执行）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# systemctl stop keepalived.service</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="1-4-4-验证-MySQL-双机切换"><a href="#1-4-4-验证-MySQL-双机切换" class="headerlink" title="1.4.4 验证 MySQL 双机切换"></a>1.4.4 验证 MySQL 双机切换</h3><p><img src="http://cdn.winyter.cn/ambari-install-guide_image6.png" alt="image6.png"><br>主备服务器启动后，可以通过ifconfig命令来查看网卡的状态，其中一台服务器包含一个虚拟网卡，另外一台机器没有虚拟网卡，如下图所示。<br><img src="http://cdn.winyter.cn/ambari-install-guide_image7.png" alt="image7.png"><br><img src="http://cdn.winyter.cn/ambari-install-guide_image8.png" alt="image8.png"></p>
<p>当将 node1 上的 mysql 停止时，虚拟网卡应该切换到 node2 上。</p>
<hr>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/MyBlog/tags/Ambari/" rel="tag"># Ambari</a>
              <a href="/MyBlog/tags/Bigdata/" rel="tag"># Bigdata</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/MyBlog/2020/05/31/ambari-install-guides-1-introduction-md/" rel="prev" title="基于 Ambari 的大数据集群安装指导——篇一：基本介绍">
      <i class="fa fa-chevron-left"></i> 基于 Ambari 的大数据集群安装指导——篇一：基本介绍
    </a></div>
      <div class="post-nav-item">
    <a href="/MyBlog/2020/05/31/ambari-install-guides-5-install-yarn-md/" rel="next" title="基于 Ambari 的大数据集群安装指导——篇五：Ambari 安装 YARN">
      基于 Ambari 的大数据集群安装指导——篇五：Ambari 安装 YARN <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



        </div>
        

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">winyter</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/MyBlog/lib/anime.min.js"></script>

<script src="/MyBlog/js/utils.js"></script>

<script src="/MyBlog/js/motion.js"></script>


<script src="/MyBlog/js/next-boot.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  















  

  

</body>
</html>
