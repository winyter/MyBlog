{"meta":{"title":"花不醉的小花园","subtitle":"携一生所爱，看遍世间美好","description":"IT,Photograph,Travel,Love","author":"winyter","url":"https://winyter.github.io/MyBlog","root":"/MyBlog/"},"pages":[{"title":"categories","date":"2019-12-10T16:55:18.000Z","updated":"2020-04-19T07:34:30.173Z","comments":true,"path":"categories/index.html","permalink":"https://winyter.github.io/MyBlog/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2019-12-10T16:54:17.000Z","updated":"2020-04-19T07:32:00.512Z","comments":true,"path":"tags/index.html","permalink":"https://winyter.github.io/MyBlog/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"CentOS 7 修改网卡配置","slug":"CentOS7x-Network-Config","date":"2020-07-22T14:30:33.000Z","updated":"2020-07-25T10:57:03.563Z","comments":true,"path":"2020/07/22/CentOS7x-Network-Config/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/07/22/CentOS7x-Network-Config/","excerpt":"本文档仅提供傻瓜式的配置模板，没有对网卡配置项进行详细的解读","text":"本文档仅提供傻瓜式的配置模板，没有对网卡配置项进行详细的解读 配置模板 TYPE&#x3D;EthernetPROXY_METHOD&#x3D;noneBROWSER_ONLY&#x3D;noBOOTPROTO&#x3D;staticDEFROUTE&#x3D;yesIPV4_FAILURE_FATAL&#x3D;noIPV6INIT&#x3D;yesIPV6_AUTOCONF&#x3D;yesIPV6_DEFROUTE&#x3D;yesIPV6_FAILURE_FATAL&#x3D;noIPV6_ADDR_GEN_MODE&#x3D;stable-privacyNAME&#x3D;eno3UUID&#x3D;c5a9ccc6-a5a1-44e2-bf62-f7af9b472d6bDEVICE&#x3D;eno3ONBOOT&#x3D;yesIPADDR&#x3D;162.168.1.100PREFIX&#x3D;24GATEWAY&#x3D;162.168.1.254 关闭界面网络管理服务 [root@node1]# systemctl stop NetworkManager.service[root@node1]# systemctl disable NetworkManager.service[root@node1]# systemctl stauts NetworkManager.service 重启网卡服务 [root@node1]# systemctl restart network[root@node1]# systemctl status network","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"OS","slug":"CS-Soft/OS","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/"},{"name":"Unix","slug":"CS-Soft/OS/Unix","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/Unix/"},{"name":"Linux","slug":"CS-Soft/OS/Unix/Linux","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/Unix/Linux/"}],"tags":[{"name":"OS","slug":"OS","permalink":"https://winyter.github.io/MyBlog/tags/OS/"},{"name":"Linux","slug":"Linux","permalink":"https://winyter.github.io/MyBlog/tags/Linux/"}]},{"title":"Ambari 安装 Hive 失败，报 “Table ‘hive.dbs’ doesn’t exist”","slug":"ERROR-ambari-install-hive-table-doesnt-exist","date":"2020-07-22T14:28:05.000Z","updated":"2020-07-25T10:59:55.159Z","comments":true,"path":"2020/07/22/ERROR-ambari-install-hive-table-doesnt-exist/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/07/22/ERROR-ambari-install-hive-table-doesnt-exist/","excerpt":"参考来源：https://blog.csdn.net/zheng911209/article/details/78718303 问题现象：使用 Ambari 安装 Hive，在启动 Hive 的时候，无法启动 HiveServer2，且 Ambari有报错：“ZooKeeper node /hiveserver2 is not ready yet”","text":"参考来源：https://blog.csdn.net/zheng911209/article/details/78718303 问题现象：使用 Ambari 安装 Hive，在启动 Hive 的时候，无法启动 HiveServer2，且 Ambari有报错：“ZooKeeper node /hiveserver2 is not ready yet” 问题定位：在查看 HiveServer2 的日志时，可以看到： 2017-12-05T11:52:50,446 WARN [ecd471e5-d4b9-40b4-bc9a-644fc411f415 main] metastore.MetaStoreDirectSql: Self-test query [select “DB_ID” from “DBS”] failed; direct SQL is disabledjavax.jdo.JDODataStoreException: Error executing SQL query “select “DB_ID” from “DBS”“.at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:543) ~[datanucleus-api-jdo-4.2.4.jar:?]……Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table ‘hive.dbs’ doesn’t exist……2017-12-05T11:52:50,457 ERROR [ecd471e5-d4b9-40b4-bc9a-644fc411f415 main] metastore.RetryingHMSHandler: MetaException(message:Version information not found in metastore. ) 问题原因：没生成源数据表 解决问题：1、打开 hive-site.xml，设置如下为 true： &lt;property&gt; &lt;name&gt;datanucleus.schema.autoCreateAll&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 2、再执行命令： schematool -dbType mysql -initSchema","categories":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/"},{"name":"Big Data","slug":"CS-Concept/Big-Data","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Big-Data/"}],"tags":[{"name":"ERROR","slug":"ERROR","permalink":"https://winyter.github.io/MyBlog/tags/ERROR/"},{"name":"Hive","slug":"Hive","permalink":"https://winyter.github.io/MyBlog/tags/Hive/"},{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"}]},{"title":"解决 Hive 重启时，Metastore 报：MetaException(message:Hive Schema version 2.3.0 does not match metastore's schema version 1.2.0","slug":"ERROR-Hive-restart-schema-version-unmatched","date":"2020-07-22T14:23:25.000Z","updated":"2020-07-25T10:59:32.479Z","comments":true,"path":"2020/07/22/ERROR-Hive-restart-schema-version-unmatched/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/07/22/ERROR-Hive-restart-schema-version-unmatched/","excerpt":"参考来源：https://blog.csdn.net/struggling_rong/article/details/82598277 问题现象：使用 Ambari 重启 Hive 时，遇到在启动 HiveServer2 失败，报错为： resource_management.core.exceptions.Fail: ZooKeeper node &#x2F;hiveserver2 is not ready yet","text":"参考来源：https://blog.csdn.net/struggling_rong/article/details/82598277 问题现象：使用 Ambari 重启 Hive 时，遇到在启动 HiveServer2 失败，报错为： resource_management.core.exceptions.Fail: ZooKeeper node &#x2F;hiveserver2 is not ready yet 问题定位：在查看 HiveServer2 的日志时，可以看到： 2020-07-21T17:55:41,864 INFO [main]: sqlstd.SQLStdHiveAccessController (SQLStdHiveAccessController.java:&lt;init&gt;(95)) - Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString&#x3D;53ee998b-175a-4bc8-985c-a27bfc138166, clientType&#x3D;HIVESERVER2]2020-07-21T17:55:41,869 WARN [main]: session.SessionState (SessionState.java:setAuthorizerV2Config(903)) - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.2020-07-21T17:55:42,352 INFO [main]: hive.metastore (HiveMetaStoreClient.java:open(407)) - Trying to connect to metastore with URI thrift:&#x2F;&#x2F;zz-hadoop2:90832020-07-21T17:55:42,381 WARN [main]: hive.metastore (HiveMetaStoreClient.java:open(490)) - Failed to connect to the MetaStore Server...2020-07-21T17:55:42,381 INFO [main]: hive.metastore (HiveMetaStoreClient.java:open(521)) - Waiting 5 seconds before next connection attempt...................2020-07-21T17:57:37,424 INFO [main]: hive.metastore (HiveMetaStoreClient.java:open(407)) - Trying to connect to metastore with URI thrift:&#x2F;&#x2F;zz-hadoop2:90832020-07-21T17:57:37,425 WARN [main]: hive.metastore (HiveMetaStoreClient.java:open(490)) - Failed to connect to the MetaStore Server...2020-07-21T17:57:37,425 INFO [main]: hive.metastore (HiveMetaStoreClient.java:open(521)) - Waiting 5 seconds before next connection attempt.2020-07-21T17:57:42,434 WARN [main]: metadata.Hive (Hive.java:registerAllFunctionsOnce(234)) - Failed to register all functions.java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1701) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:83) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3600) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3652) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3632) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3894) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:248) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:231) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.Hive.&lt;init&gt;(Hive.java:388) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:332) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:312) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:288) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.session.SessionState.setAuthorizerV2Config(SessionState.java:913) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:877) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.session.SessionState.applyAuthorizationPolicy(SessionState.java:1683) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hive.service.cli.CLIService.applyAuthorizationConfigPolicy(CLIService.java:130) ~[hive-service-2.3.3.jar:2.3.3] at org.apache.hive.service.cli.CLIService.init(CLIService.java:114) ~[hive-service-2.3.3.jar:2.3.3] at org.apache.hive.service.CompositeService.init(CompositeService.java:59) ~[hive-service-2.3.3.jar:2.3.3] at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:142) ~[hive-service-2.3.3.jar:2.3.3] at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:607) ~[hive-service-2.3.3.jar:2.3.3] at org.apache.hive.service.server.HiveServer2.access$700(HiveServer2.java:100) ~[hive-service-2.3.3.jar:2.3.3] at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:855) ~[hive-service-2.3.3.jar:2.3.3] at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:724) ~[hive-service-2.3.3.jar:2.3.3] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_181] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181] at org.apache.hadoop.util.RunJar.run(RunJar.java:233) ~[hadoop-common-2.7.3.2.6.3.0-235.jar:?] at org.apache.hadoop.util.RunJar.main(RunJar.java:148) ~[hadoop-common-2.7.3.2.6.3.0-235.jar:?]Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_181] at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_181] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_181] at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_181] at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1699) ~[hive-exec-2.3.3.jar:2.3.3] ... 30 moreCaused by: org.apache.hadoop.hive.metastore.api.MetaException: Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused (Connection refused) at org.apache.thrift.transport.TSocket.open(TSocket.java:226) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:480) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:247) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:70) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1699) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:83) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3600) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3652) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3632) at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3894) at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:248) at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:231) at org.apache.hadoop.hive.ql.metadata.Hive.&lt;init&gt;(Hive.java:388) at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:332) at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:312) at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:288) at org.apache.hadoop.hive.ql.session.SessionState.setAuthorizerV2Config(SessionState.java:913) at org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:877) at org.apache.hadoop.hive.ql.session.SessionState.applyAuthorizationPolicy(SessionState.java:1683) at org.apache.hive.service.cli.CLIService.applyAuthorizationConfigPolicy(CLIService.java:130) at org.apache.hive.service.cli.CLIService.init(CLIService.java:114) at org.apache.hive.service.CompositeService.init(CompositeService.java:59) at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:142) at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:607) at org.apache.hive.service.server.HiveServer2.access$700(HiveServer2.java:100) at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:855) at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:724) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:233) at org.apache.hadoop.util.RunJar.main(RunJar.java:148)Caused by: java.net.ConnectException: Connection refused (Connection refused) at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:589) at org.apache.thrift.transport.TSocket.open(TSocket.java:221) ... 38 more at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:529) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:247) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:70) ~[hive-exec-2.3.3.jar:2.3.3] at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_181] at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_181] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_181] at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_181] at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1699) ~[hive-exec-2.3.3.jar:2.3.3] ... 30 more2020-07-21T17:57:42,461 INFO [main]: server.HiveServer2 (HiveServer2.java:stop(518)) - Shutting down HiveServer22020-07-21T17:57:42,460 ERROR [main]: session.SessionState (SessionState.java:setupAuth(884)) - Error setting up authorization: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClientorg.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.ql.session.SessionState.setAuthorizerV2Config(SessionState.java:917) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:877) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.session.SessionState.applyAuthorizationPolicy(SessionState.java:1683) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hive.service.cli.CLIService.applyAuthorizationConfigPolicy(CLIService.java:130) ~[hive-service-2.3.3.jar:2.3.3] at org.apache.hive.service.cli.CLIService.init(CLIService.java:114) ~[hive-service-2.3.3.jar:2.3.3] at org.apache.hive.service.CompositeService.init(CompositeService.java:59) ~[hive-service-2.3.3.jar:2.3.3] at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:142) ~[hive-service-2.3.3.jar:2.3.3] at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:607) ~[hive-service-2.3.3.jar:2.3.3] at org.apache.hive.service.server.HiveServer2.access$700(HiveServer2.java:100) ~[hive-service-2.3.3.jar:2.3.3] at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:855) ~[hive-service-2.3.3.jar:2.3.3] at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:724) ~[hive-service-2.3.3.jar:2.3.3] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_181] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181] at org.apache.hadoop.util.RunJar.run(RunJar.java:233) ~[hadoop-common-2.7.3.2.6.3.0-235.jar:?] at org.apache.hadoop.util.RunJar.main(RunJar.java:148) ~[hadoop-common-2.7.3.2.6.3.0-235.jar:?]Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:236) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.Hive.&lt;init&gt;(Hive.java:388) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:332) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:312) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:288) ~[hive-exec-2.3.3.jar:2.3.3] at org.apache.hadoop.hive.ql.session.SessionState.setAuthorizerV2Config(SessionState.java:913) ~[hive-exec-2.3.3.jar:2.3.3] ... 16 more.................. 可以看到是因为 Hive 的 Metastore 无法访问导致 HiveServer2 启动失败，于是，再去看 Metastore 的日志： 2020-07-21T17:55:41,118 ERROR [main]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(204)) - MetaException(message:Hive Schema version 2.3.0 does not match metastore&#39;s schema version 1.2.0 Metastore is not upgraded or corrupt) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:7579) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:7542) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:101) at com.sun.proxy.$Proxy24.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:584) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:651) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:427) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:79) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6887) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6882) at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:7140) at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:7067) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:233) at org.apache.hadoop.util.RunJar.main(RunJar.java:148) 问题原因：Hive Schema version 2.3.0 does not match metastore&#39;s schema version 1.2.0 Metastore is not upgraded or corrupt问题的核心就在这一句报错，即 Hive 的 Schema 版本不兼容 解决问题： 临时解决：在 mysql（假定 metastore 的数据库为 mysql）中，切换到 hive 库，执行如下命令 UPDATE VERSION SET SCHEMA_VERSION&#x3D;&#39;2.3.0&#39;, VERSION_COMMENT&#x3D;&#39;fix conflict&#39; where VER_ID&#x3D;1; 这样解决后，当通过 spark 应用再创建新表时仍会报错。 更好的解决： metastore 模式使用remote方式 通过cdh来解决版本问题","categories":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/"},{"name":"Big Data","slug":"CS-Concept/Big-Data","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Big-Data/"}],"tags":[{"name":"ERROR","slug":"ERROR","permalink":"https://winyter.github.io/MyBlog/tags/ERROR/"},{"name":"Hive","slug":"Hive","permalink":"https://winyter.github.io/MyBlog/tags/Hive/"}]},{"title":"《数学之美》笔记系列——第二章","slug":"beauty-of-mathematics-notes-chapter2","date":"2020-07-22T14:18:08.000Z","updated":"2020-07-25T11:01:02.698Z","comments":true,"path":"2020/07/22/beauty-of-mathematics-notes-chapter2/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/07/22/beauty-of-mathematics-notes-chapter2/","excerpt":"第二章 自然语言处理——从规则到统计","text":"第二章 自然语言处理——从规则到统计 1、机器智能 图灵测试（Turing Test）：让人和机器进行交流，如果人无法判断自己交流的对象是人还是机器，那么就说明这个机器有智能。 1956 年，“达特茅斯夏季人工智能研究会议” 20 世纪 60 年代，分析语句和获取语义。句法分析树（Syntactic Parse Tree）、重写规则（Rewrite Rules）。这个方法的问题：文法规则全靠人工，而真实语句太多，人工无法完成，且会出现矛盾；即使能够写出涵盖所有自然语言现象的语法规则集合，也很难用计算机来解析，因为自然语言是上下文有关文法（Context Dependent Grammar），不是计算机高级语言的上下文无关文法（Context Independent Grammar），而上下文有关算法的计算复杂度（Computational Complexity）极大。 2、从规则到统计 上世纪 70 年代，基于规则的句法分析很快就走到了尽头，而对于语义的处理遇到了更大的麻烦。首先，自然语言中词的多义性很难用规则来描述，而是严重依赖于上下文，甚至是“世界的知识”（World Knowledge）或者常识。 小结 基于统计的自然语言处理方法，在数学模型上和通信是想通的，甚至就是相同的。因此，数学意义上的自然语言处理又和语言的初衷——通信联系在一起了。但是，科学家用了几十年才认识到这种联系。","categories":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/"},{"name":"Reading Notes","slug":"CS-Concept/Reading-Notes","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Reading-Notes/"}],"tags":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/tags/CS-Concept/"},{"name":"Math","slug":"Math","permalink":"https://winyter.github.io/MyBlog/tags/Math/"},{"name":"Reading","slug":"Reading","permalink":"https://winyter.github.io/MyBlog/tags/Reading/"},{"name":"Notes","slug":"Notes","permalink":"https://winyter.github.io/MyBlog/tags/Notes/"}]},{"title":"《数学之美》笔记系列——第一章","slug":"beauty-of-mathematics-notes-chapter1","date":"2020-07-12T14:37:05.000Z","updated":"2020-07-25T11:00:46.911Z","comments":true,"path":"2020/07/12/beauty-of-mathematics-notes-chapter1/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/07/12/beauty-of-mathematics-notes-chapter1/","excerpt":"第一章 文字和语言 vs 数字和信息 数字、文字、语言都是信息的载体，他们诞生的目的，是为了让信息更好的传播","text":"第一章 文字和语言 vs 数字和信息 数字、文字、语言都是信息的载体，他们诞生的目的，是为了让信息更好的传播 1、信息 早期人类利用声音进行通信，和现在最先进的通信，在原理上没有任何差别 信息[信息源] &gt;&gt;(怪叫声 编码)&gt;&gt; 信息[信道] &gt;&gt;(听到的声音 解码)&gt;&gt; 信息[接受者] 2、文字和数字 当语言和词汇多到一定程度，人类仅靠大脑已经及部署所有词汇了。于是，高效记录信息的需求就产生了，这就是文字的起源。 最早便是使用象形文字。但随着文明的进步，所需的文字越来越多，象形文字无法满足需求。于是，就开始了概念的第一次概括和归类（即一词多意）这种聚类，和如今自然语言处理或者机器学习的聚类有很大的相似性。 但是，文字按照意思来聚类，会带来一些歧义性。而为了消除歧义（Disambiguation），人们便根据上下文来判断词意，这也是今天自然语言处理时，采用的消歧方法。但是，即使根据上下文也总有做不到确定词意的时候，因此，自然语言处理的消歧，也不可能做到 100%判断出准确的词意 而在文明发展中，因为地域的原因，各个地方产生了不同的文字，而不同地方的人在交流时，就产生了翻译的需求。 【重要】翻译这件事之所以能达成，仅仅是因为不同的文字系统在记录信息上的能力是等价的。文字只是信息的载体，不同的文字可以描述相同的信息。这是现代通信的基础。 罗塞塔石碑带来的启示：1、信息的冗余是信息安全的保障；2、语言的数据，即语料，尤其是双语或者多语的对照语料对翻译至关重要，它是我们从事机器翻译研究的基础。 数字早期和文字一样，都是承载信息的工具。并不具备任何抽象的含义。 阿拉伯数字或者说印度数字的革命性，不仅在于它的简洁有效，而且标志着数字和文字的分离。这在客观上，让自然语言的研究和数学在几千年里没有重合的轨迹，而且越走越远。 3、文字和语言背后的数学 拼音文字(楔形文字)，起源于两河流域，每个形状不同的楔子实际上是不同的字母。如果把中文的笔画作为字母，它也可以看做是一种拼音文字，只不过是二维的。 随后拼音文字逐渐发展，古希腊人将字母的拼写和读音紧密结合，使得语言更容易学习。最终成为了今天西方拼音文字的起源——罗马式语言（Roman Languages） 从象形文字到拼音文字是一个飞跃，因为人类在描述物体的方式上，从物体的外表进化到了抽象的概念，同时不自觉的采用了对信息的编码。同时，文字的编码还非常合理，罗马体系文字中，常用字短，生僻字长，而类似汉字的意形文字也是如此，常用字笔画少，生僻字笔画多。这完全符合信息论中的最短编码原理。 在古代，书写不是一件容易的事，因此，古人说话和如今的白话差别不大，但书面文字却非常简洁。这非常符合如今信息科学和信息工程中的一些基本原理，即在通行中，如果信道较宽，信息不必压缩就可以直接进行传递；而如果信道较窄，则信息在传递前需要尽可能地压缩。 犹太人在抄写《圣经》时，为了防止抄错，发明了类似今天计算机和通信中校验码的方法，即每个字母对应一个数字，每行将这些数字相加，获得一个特殊的数字，每抄完一页，对这些数字进行校验，这既能防止抄错，也能在抄错时，快速定位错误位置。这和如今各种校验的原理是相同的。","categories":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/"},{"name":"Reading Notes","slug":"CS-Concept/Reading-Notes","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Reading-Notes/"}],"tags":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/tags/CS-Concept/"},{"name":"Math","slug":"Math","permalink":"https://winyter.github.io/MyBlog/tags/Math/"},{"name":"Reading","slug":"Reading","permalink":"https://winyter.github.io/MyBlog/tags/Reading/"},{"name":"Notes","slug":"Notes","permalink":"https://winyter.github.io/MyBlog/tags/Notes/"}]},{"title":"CentOS 7x 操作系统安装指南","slug":"centos7x-install-guide","date":"2020-06-09T15:07:00.000Z","updated":"2020-07-25T11:15:08.739Z","comments":true,"path":"2020/06/09/centos7x-install-guide/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/06/09/centos7x-install-guide/","excerpt":"1、选择安装介质","text":"1、选择安装介质 启动服务器，按F11进入BIOS设置，选择启动设备并回车。如下图所示： 识别操作系统源文件，选择Install CentOS 7并回车（第一个)： 2、配置语言配置语言为English点击右下角的continue。 3、配置时区进入操作系统设计界面 配置时区点击选择 DATA &amp; TIME，设置时区为 Asia-Shanghai。 配置完成后，点击左上角的done。 4、选择安装工具包 本节工具包的选择请根据实际需求选择，如果无法确定具体需要使用哪些，可以不做勾选，后期可通过镜像挂载的方式再进行补装 Software selection现举例选择以下的几个服务安装，如下图所示： 选择完成后，点击左上角的done。 5、系统分区 系统分区按照实际需求划分，下面提供一种比较通用的分区方法 Installation Destination 自定义配置安装，如下图所示： 注意，当磁盘数量较多时，请确认好哪个磁盘作为系统盘，不可装错，另外，生产环境下，建议系统盘配置 RAID1，提高安全性 根据引导进入如下界面，自动创建系统分区。 修改新增分区 对默认home目录空间进行减少，保证下面的目录空间分配。/boot分区保持默认。将根目录/ 空间增加至100G。新增/usr分区，分配60G。新增/var/log分区，分配60G~100G (视情况而定，若系统盘较大，则该分区给100g；较小则给60g。例如系统盘是600g，/var/log分60G,系统盘是900g，/var/log 分100G）swap分区调整到16G。文件系统选择xfs。 根据以上文字描述的建议进行分配。建议先将默认分好的 /home 的空间给释放出来。 接着点 +，按照需求来划分区域。注意填写单位。 分好的分区可以参考下图，但具体的分区请按照上面的文字要求进行，这里需注意的是根据盘的大小来划分 /var/log 的大小（600g 系统盘分 60G,900g 系统盘分 100G）。 设置完成后，点击 done。弹出框，继续点击 Accept Changer。 配置完成后点击 begin install 开始安装。 6、配置用户安装过程中可以配置root密码，同时可以添加配置用户 修改root用户密码 创建用户 7、完成安装等待安装等待 20-30 分钟后安装完成。点击右下角的 reboot。 开机后，进入 licensing 确认界面 选择第一项，进入如下界面 勾选最下面的 I accept… 的那个选项，点击左上角 done。 点击右下角后即可使用操作系统。","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"OS","slug":"CS-Soft/OS","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/"},{"name":"Unix","slug":"CS-Soft/OS/Unix","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/Unix/"},{"name":"Linux","slug":"CS-Soft/OS/Unix/Linux","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/Unix/Linux/"}],"tags":[{"name":"OS","slug":"OS","permalink":"https://winyter.github.io/MyBlog/tags/OS/"},{"name":"Linux","slug":"Linux","permalink":"https://winyter.github.io/MyBlog/tags/Linux/"}]},{"title":"Homebrew 安装指南","slug":"homebrew-install-guide","date":"2020-06-09T15:03:51.000Z","updated":"2020-07-25T11:17:05.885Z","comments":true,"path":"2020/06/09/homebrew-install-guide/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/06/09/homebrew-install-guide/","excerpt":"非正式文档，爬坑记录，将安装 Homebrew 的全流程都记录了下来，读完这篇，就能安装顺畅 xcode-select --install&#x2F;usr&#x2F;bin&#x2F;ruby -e &quot;$(curl -fsSL https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Homebrew&#x2F;install&#x2F;master&#x2F;install)&quot;","text":"非正式文档，爬坑记录，将安装 Homebrew 的全流程都记录了下来，读完这篇，就能安装顺畅 xcode-select --install&#x2F;usr&#x2F;bin&#x2F;ruby -e &quot;$(curl -fsSL https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Homebrew&#x2F;install&#x2F;master&#x2F;install)&quot; 如果遇到报错： curl: (7) Failed to connect to raw.githubusercontent.com port 443: Connection refused 换手机热点连接笔记本，无线可能被墙了 如果遇到报错： curl: (6) Could not resolve host: raw.githubusercontent.com 需要在电脑 hosts 文件里添加 IP 主机名映射，默认的 raw.githubusercontent.com 域名的 IP 是错误的 sudo vim &#x2F;etc&#x2F;hosts添加：199.232.68.133 githubusercontent.com# 请使用 https:&#x2F;&#x2F;www.ipaddress.com&#x2F; 查询一下该域名的 IP，这个 IP 不一定是一直不变的 然而，在国内环境下，你会发现，下载速度巨慢，这需要我们把下载源换到国内镜像上 根据网上的教程，你需要将 brew 的 install 文件下载本地 curl -fsSL https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Homebrew&#x2F;install&#x2F;master&#x2F;install &gt;&gt; brew_install 但是打开这个 brew_install 文件后，你会发现，里面并不是网上教程里所说的那些配置，而是这些东西： #!&#x2F;usr&#x2F;bin&#x2F;rubySTDERR.print &lt;&lt;EOSWarning: The Ruby Homebrew installer is now deprecated and has been rewritten inBash. Please migrate to the following command: &#x2F;bin&#x2F;bash -c &quot;$(curl -fsSL https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Homebrew&#x2F;install&#x2F;master&#x2F;install.sh)&quot;EOSKernel.exec &quot;&#x2F;bin&#x2F;bash&quot;, &quot;-c&quot;, &#39;&#x2F;bin&#x2F;bash -c &quot;$(curl -fsSL https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Homebrew&#x2F;install&#x2F;master&#x2F;install.sh)&quot;&#39; 这时候，你需要仔细的读一读这个文件内容，里面告诉你，原来的那个安装脚本现在已经不使用了，你需要换一个方式去执行，然后，给了你一个新的执行命令，这时候，你只需要把这个命令拿出来，然后把安装脚本下载下来： curl -fsSL https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Homebrew&#x2F;install&#x2F;master&#x2F;install.sh &gt; install.sh 然后，继续按照网上文档所说的，修改两个参数： BREW_REPO &#x3D; &quot;git:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;brew.git&quot;.freezeCORE_TAP_REPO &#x3D; &quot;git:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;homebrew-core.git&quot;.freeze 但是，这时候你需要注意，新版的 Homebrew 的安装脚本里，已经没有 CORE_TAP_REPO 这个配置项了，你不需要新增这个配置项，只要把 BREW_REPO 配置修改完即可 这时，就可以开始执行安装了 sh install.sh 注意：由于 Homebrew 默认的更新源还没有换，因此，在安装完成后，会出现如下报错 &#x3D;&#x3D;&gt; Tapping homebrew&#x2F;coreCloning into &#39;&#x2F;usr&#x2F;local&#x2F;Homebrew&#x2F;Library&#x2F;Taps&#x2F;homebrew&#x2F;homebrew-core&#39;...fatal: unable to access &#39;https:&#x2F;&#x2F;github.com&#x2F;Homebrew&#x2F;homebrew-core&#x2F;&#39;: LibreSSL SSL_read: SSL_ERROR_SYSCALL, errno 54Error: Failure while executing; &#96;git clone https:&#x2F;&#x2F;github.com&#x2F;Homebrew&#x2F;homebrew-core &#x2F;usr&#x2F;local&#x2F;Homebrew&#x2F;Library&#x2F;Taps&#x2F;homebrew&#x2F;homebrew-core&#96; exited with 128.Error: Failure while executing; &#96;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;brew tap homebrew&#x2F;core&#96; exited with 1.Failed during: &#x2F;usr&#x2F;local&#x2F;bin&#x2F;brew update --force 或者，很长时间，卡在Cloning into &#39;/usr/local/Homebrew/Library/Taps/homebrew/homebrew-core&#39;...这句话上 这时，继续执行以下语句： git clone git:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;homebrew-core.git&#x2F; &#x2F;usr&#x2F;local&#x2F;Homebrew&#x2F;Library&#x2F;Taps&#x2F;homebrew&#x2F;homebrew-core --depth&#x3D;1git clone git:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;homebrew-cask.git&#x2F; &#x2F;usr&#x2F;local&#x2F;Homebrew&#x2F;Library&#x2F;Taps&#x2F;caskroom&#x2F;homebrew-cask --depth&#x3D;1 至此，安装完成，下面开始替换更新源。 目前国内主流有两种源可以选择，中科大镜像和清华镜像，下面也会给出两种镜像的替换方法，任选其一即可 中科大镜像(方法缘于广大网友)： # 替换默认源cd &quot;$(brew --repo)&quot;git remote set-url origin https:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;brew.git # 替换核心仓库源cd &quot;$(brew --repo)&#x2F;Library&#x2F;Taps&#x2F;homebrew&#x2F;homebrew-core&quot;git remote set-url origin https:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;homebrew-core.git# 替换 cask 软件仓库（提供 macOS 应用和大型二进制文件）cd &quot;$(brew --repo)&#x2F;Library&#x2F;Taps&#x2F;caskroom&#x2F;homebrew-cask&quot;git remote set-url origin https:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;homebrew-cask.git# brew 更新brew update# 替换 Bottles 源（Homebrew 预编译二进制软件包）echo &#39;export HOMEBREW_BOTTLE_DOMAIN&#x3D;https:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;homebrew-bottles&#39; &gt;&gt; ~&#x2F;.bash_profilesource ~&#x2F;.bash_profile# 检查brew doctor 清华镜像(源自：清华镜像官方使用说明)git -C &quot;$(brew --repo)&quot; remote set-url origin https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;git&#x2F;homebrew&#x2F;brew.gitgit -C &quot;$(brew --repo homebrew&#x2F;core)&quot; remote set-url origin https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;git&#x2F;homebrew&#x2F;homebrew-core.gitgit -C &quot;$(brew --repo homebrew&#x2F;cask)&quot; remote set-url origin https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;git&#x2F;homebrew&#x2F;homebrew-cask.gitgit -C &quot;$(brew --repo homebrew&#x2F;cask-fonts)&quot; remote set-url origin https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;git&#x2F;homebrew&#x2F;homebrew-cask-fonts.gitgit -C &quot;$(brew --repo homebrew&#x2F;cask-drivers)&quot; remote set-url origin https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;git&#x2F;homebrew&#x2F;homebrew-cask-drivers.git 参考： Mac HomeBrew国内镜像安装方法 Mac下使用国内镜像安装Homebrew","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Homebrew","slug":"CS-Soft/Homebrew","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Homebrew/"}],"tags":[{"name":"MacOS","slug":"MacOS","permalink":"https://winyter.github.io/MyBlog/tags/MacOS/"},{"name":"Tools","slug":"Tools","permalink":"https://winyter.github.io/MyBlog/tags/Tools/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇十六：Ambari 管理","slug":"ambari-install-guides-16-ambari-manage-md","date":"2020-05-31T09:57:46.000Z","updated":"2020-07-25T11:14:44.348Z","comments":true,"path":"2020/05/31/ambari-install-guides-16-ambari-manage-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-16-ambari-manage-md/","excerpt":"1、Ambari 用户管理","text":"1、Ambari 用户管理 使用默认用户 admin/admin 登录 Ambari 页面，点击用户下的 Manage Ambari 按钮，进入 Ambari 管理界面。 Manage Users / Groups 模块可以创建 ambari 用户和用户组，并为用户赋予权限： 2、View 管理用于扩展 Ambari Web UI 中的框架。点击 Views 进入页面： 创建的views还可以通过主页中的视图菜单查看： 3、Hadoop集群管理进入仪表盘，可以管理安装的集群: Hadoop 集群管理包括： 集群扩容（新增机器） 添加、删除服务添加服务后，需要重启 ambari-server。 启动、停止服务 修改配置 监控数据并告警 自定义操作 点击 Hosts，可以分别按照服务、机器进行管理： 4、Ambari 清理 注意：Ambari清理需要对所有的机器都执行，不是只在主机上。 如果通过 ambari 安装 Hadoop 集群后，想重新再来一次的话，需要清理集群。1、通过 ambari 将集群中的所用组件都关闭，如果关闭不了，直接 kill -9 XXX。 2、关闭 ambari-server，ambari-agent ambari-server stop ambari-agent stop 3、卸载安装的软件（当出现 is this ok 的提示的时候输入 y 即可） yum remove hadoop_2* hdp-select* ranger_2* zookeeper_* bigtop* atlas-metadata* ambari* postgresql spark* slider* storm* snappy* 4、查看是否还有没有卸载的 yum list | grep @HDP 如果有，继续通过 yum remove XXX 卸载 5、删除 postgresql 的数据（如果使用其他数据库，可跳过此步骤）postgresql 软件卸载后，其数据还保留在硬盘中，需要把这部分数据删除掉，如果不删除掉，重新安装 ambari-server 后，有可能还应用以前的安装数据，而这些数据时错误数据，所以需要删除掉。 rm -rf &#x2F;var&#x2F;lib&#x2F;pgsql 6、删除用户和对应文件夹（复制下面表格全部的命令，在 xshell 界面中直接粘贴就可以全部执行。）ambari 安装 hadoop 集群会创建一些用户，清除集群时有必要清除这些用户，并删除对应的文件夹。这样做可以避免集群运行时出现的文件访问权限错误的问题。 rm -rf &#x2F;home&#x2F;atlasrm -rf &#x2F;home&#x2F;accumulo rm -rf &#x2F;home&#x2F;hbase rm -rf &#x2F;home&#x2F;hive rm -rf &#x2F;home&#x2F;oozie rm -rf &#x2F;home&#x2F;storm rm -rf &#x2F;home&#x2F;yarn rm -rf &#x2F;home&#x2F;ambari-qa rm -rf &#x2F;home&#x2F;falcon rm -rf &#x2F;home&#x2F;hcat rm -rf &#x2F;home&#x2F;kafka rm -rf &#x2F;home&#x2F;mahout rm -rf &#x2F;home&#x2F;spark rm -rf &#x2F;home&#x2F;tez rm -rf &#x2F;home&#x2F;zookeeper rm -rf &#x2F;home&#x2F;flume rm -rf &#x2F;home&#x2F;hdfsrm -rf &#x2F;hdfs&#x2F;hadoop rm -rf &#x2F;home&#x2F;knox rm -rf &#x2F;home&#x2F;mapred rm -rf &#x2F;home&#x2F;sqoop 7、删除ambari遗留数据 rm -rf &#x2F;var&#x2F;lib&#x2F;ambari*rm -rf &#x2F;usr&#x2F;lib&#x2F;python2.6&#x2F;site-packages&#x2F;ambari_* rm -rf &#x2F;usr&#x2F;lib&#x2F;python2.6&#x2F;site-packages&#x2F;resource_management rm -rf &#x2F;usr&#x2F;lib&#x2F;ambri-* 8、删除其他hadoop组件遗留数据 rm -rf &#x2F;etc&#x2F;falconrm -rf &#x2F;etc&#x2F;knoxrm -rf &#x2F;etc&#x2F;hive-webhcatrm -rf &#x2F;etc&#x2F;kafkarm -rf &#x2F;etc&#x2F;sliderrm -rf &#x2F;etc&#x2F;storm-slider-clientrm -rf &#x2F;etc&#x2F;sparkrm -rf &#x2F;var&#x2F;run&#x2F;sparkrm -rf &#x2F;var&#x2F;run&#x2F;hadooprm -rf &#x2F;var&#x2F;run&#x2F;hbaserm -rf &#x2F;var&#x2F;run&#x2F;zookeeperrm -rf &#x2F;var&#x2F;run&#x2F;flumerm -rf &#x2F;var&#x2F;run&#x2F;stormrm -rf &#x2F;var&#x2F;run&#x2F;webhcatrm -rf &#x2F;var&#x2F;run&#x2F;hadoop-yarnrm -rf &#x2F;var&#x2F;run&#x2F;hadoop-mapreducerm -rf &#x2F;var&#x2F;run&#x2F;kafkarm -rf &#x2F;var&#x2F;log&#x2F;hadooprm -rf &#x2F;var&#x2F;log&#x2F;hbaserm -rf &#x2F;var&#x2F;log&#x2F;flumerm -rf &#x2F;var&#x2F;log&#x2F;stormrm -rf &#x2F;var&#x2F;log&#x2F;hadoop-yarnrm -rf &#x2F;var&#x2F;log&#x2F;hadoop-mapreducerm -rf &#x2F;var&#x2F;log&#x2F;knoxrm -rf &#x2F;usr&#x2F;lib&#x2F;flumerm -rf &#x2F;usr&#x2F;lib&#x2F;stormrm -rf &#x2F;var&#x2F;lib&#x2F;hiverm -rf &#x2F;var&#x2F;lib&#x2F;oozierm -rf &#x2F;var&#x2F;lib&#x2F;flumerm -rf &#x2F;var&#x2F;lib&#x2F;hadoop-hdfsrm -rf &#x2F;var&#x2F;lib&#x2F;knoxrm -rf &#x2F;var&#x2F;log&#x2F;hiverm -rf &#x2F;var&#x2F;log&#x2F;oozierm -rf &#x2F;var&#x2F;log&#x2F;zookeeperrm -rf &#x2F;var&#x2F;log&#x2F;falconrm -rf &#x2F;var&#x2F;log&#x2F;webhcatrm -rf &#x2F;var&#x2F;log&#x2F;sparkrm -rf &#x2F;var&#x2F;tmp&#x2F;oozierm -rf &#x2F;tmp&#x2F;ambari-qarm -rf &#x2F;var&#x2F;hadooprm -rf &#x2F;hadoop&#x2F;falconrm -rf &#x2F;tmp&#x2F;hadooprm -rf &#x2F;tmp&#x2F;hadoop-hdfsrm -rf &#x2F;usr&#x2F;hdprm -rf &#x2F;usr&#x2F;hadooprm -rf &#x2F;opt&#x2F;hadooprm -rf &#x2F;opt&#x2F;hadoop2rm -rf &#x2F;tmp&#x2F;hadooprm -rf &#x2F;var&#x2F;hadooprm -rf &#x2F;hadooprm -rf &#x2F;etc&#x2F;ambari-metrics-collectorrm -rf &#x2F;etc&#x2F;ambari-metrics-monitorrm -rf &#x2F;var&#x2F;run&#x2F;ambari-metrics-collectorrm -rf &#x2F;var&#x2F;run&#x2F;ambari-metrics-monitorrm -rf &#x2F;var&#x2F;log&#x2F;ambari-metrics-collectorrm -rf &#x2F;var&#x2F;log&#x2F;ambari-metrics-monitorrm -rf &#x2F;var&#x2F;lib&#x2F;hadoop-yarnrm -rf &#x2F;var&#x2F;lib&#x2F;hadoop-mapreducerm -rf &#x2F;var&#x2F;lib&#x2F;hadoop-hdfsrm -rf &#x2F;etc&#x2F;hadooprm -rf &#x2F;etc&#x2F;hadoop-httpfsyum remove zookeeper*rm -rf &#x2F;var&#x2F;log&#x2F;zookeeperrm -rf &#x2F;etc&#x2F;zookeeperrm -rf &#x2F;var&#x2F;run&#x2F;zookeeperrm -rf zookeeper-clientrm -rf zookeeper-serverrm -rf zookeeper-server-cleanupuserdel -r zookeeperfind &#x2F; -name zookeeper 最后 find，输入这条命令后如果还有看到文件夹残留，则删除该文件夹。 9、删除数据（data01-data12的路径需要根据实际路径修改） rm -rf &#x2F;usr&#x2F;hdp&#x2F;*rm -rf &#x2F;data01&#x2F;*rm -rf &#x2F;data02&#x2F;*rm -rf &#x2F;data03&#x2F;*rm -rf &#x2F;data04&#x2F;*rm -rf &#x2F;data06&#x2F;*rm -rf &#x2F;data07&#x2F;*rm -rf &#x2F;data08&#x2F;*rm -rf &#x2F;data09&#x2F;*rm -rf &#x2F;data10&#x2F;*rm -rf &#x2F;data11&#x2F;*rm -rf &#x2F;data12&#x2F;*rm -rf &#x2F;hadoop&#x2F;* 10、清理 yum 数据源 yum clean all 11、删除ambari数据库登录到MySQL执行以下语句： drop DATABASE ambari; 12、清理smartsense yum remove smartsense-hstrm -rf &#x2F;var&#x2F;log&#x2F;smartsense然后再次使用yum命令安装：yum install smartsense-hst 此外，还需要删除在 /tmp 文件夹下的相关文件夹，如下图所示。 hsperfdata_rhino和hsperfdata_root文件夹不能删除，其他的文件夹都要删除。 命令如下： [root@node1 home]$ cd &#x2F;tmp 然后执行如下脚本： rm -rf hbase-hbase&#x2F;rm -rf hsperfdata_elastic&#x2F;rm -rf hsperfdata_hbase&#x2F;rm -rf hsperfdata_hdfs&#x2F;rm -rf hsperfdata_impala&#x2F;rm -rf hsperfdata_kafka&#x2F;rm -rf hsperfdata_redis&#x2F;rm -rf hsperfdata_testt&#x2F;rm -rf hsperfdata_yarn&#x2F;rm -rf hsperfdata_zookeeper&#x2F; 执行完以上脚本后最好自己检查一下是否删除对应的文件。 除此之外还需要删除home目录下面除了rhino文件夹之外的其他所有文件夹。 5、Ambari HDFS-HA 回滚 正常情况下，不需要执行该步骤，只有还是 hdfs 升级高可用失败时才需要执行回滚操作。 1、查看hdfs的信息 curl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X GET http:&#x2F;&#x2F;172.168.1.1:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;mycluster&#x2F;services&#x2F;HDFS 172.168.1.1 为 Ambari Server 的机器 IP（端口默认为 8080）；mycluster 为 cluster 名字，从 ambari 库的 clusters 表中查询，字段是 cluster_name ；HDFS 为 Service 的名字，从 ambari 库的 clusterservices 表中查询，字段是 service_name。 2、停止hdfs curl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X PUT -d &#39;&#123;&quot;RequestInfo&quot;: &#123;&quot;context&quot;:&quot;Stop Service&quot;&#125;,&quot;Body&quot;:&#123;&quot;ServiceInfo&quot;:&#123;&quot;state&quot;:&quot;INSTALLED&quot;&#125;&#125;&#125;&#39; http:&#x2F;&#x2F;172.168.1.1:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;mycluster&#x2F;services&#x2F;HDFS 3、查看各主机的组件角色查看 NAMENODE curl -u admin:admin -i http:&#x2F;&#x2F;172.168.1.1:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;mycluster&#x2F;host_components?HostRoles&#x2F;component_name&#x3D;NAMENODE 查看 SECONDARY_NAMENODE curl -u admin:admin -i http:&#x2F;&#x2F;172.168.1.1:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;mycluster&#x2F;host_components?HostRoles&#x2F;component_name&#x3D;SECONDARY_NAMENODE 查看JOURNALNODE curl -u admin:admin -i http:&#x2F;&#x2F;172.168.1.1:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;mycluster&#x2F;host_components?HostRoles&#x2F;component_name&#x3D;JOURNALNODE 查看ZKFC curl -u admin:admin -i http:&#x2F;&#x2F;172.168.1.1:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;mycluster&#x2F;host_components?HostRoles&#x2F;component_name&#x3D;ZKFC 4、删除zkfc curl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X DELETE http:&#x2F;&#x2F;172.168.1.1:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;mycluster&#x2F;hosts&#x2F;node2&#x2F;host_components&#x2F;ZKFCcurl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X DELETE http:&#x2F;&#x2F;172.168.1.1:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;mycluster&#x2F;hosts&#x2F;node3&#x2F;host_components&#x2F;ZKFC node2 和 node3 是安装 zkfc 机器的 hostname。 5、启用SECONDARY_NAMENODE curl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X POST -d &#39;&#123;&quot;host_components&quot; : [&#123;&quot;HostRoles&quot;:&#123;&quot;component_name&quot;:&quot;SECONDARY_NAMENODE&quot;&#125;&#125;] &#125;&#39; http:&#x2F;&#x2F;172.168.1.1:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;mycluster&#x2F;hosts?Hosts&#x2F;host_name&#x3D;node2curl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X PUT -d &#39;&#123;&quot;RequestInfo&quot;:&#123;&quot;context&quot;:&quot;Enable Secondary NameNode&quot;&#125;,&quot;Body&quot;:&#123;&quot;HostRoles&quot;:&#123;&quot;state&quot;:&quot;INSTALLED&quot;&#125;&#125;&#125;&#39; http:&#x2F;&#x2F;172.168.1.1:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;mycluster&#x2F;hosts&#x2F;node2&#x2F;host_components&#x2F;SECONDARY_NAMENODEcurl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X GET &quot;http:&#x2F;&#x2F;172.168.1.1:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;cluster&#x2F;host_components?HostRoles&#x2F;component_name&#x3D;SECONDARY_NAMENODE&amp;fields&#x3D;HostRoles&#x2F;state&quot; node2 是安装 secondary_namenode 机器的 hostname 6、删除 journalnode 先查看哪些机器上安装了 journalnode: curl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X GET http:&#x2F;&#x2F;10.10.111.11:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;&#x2F;rhino&#x2F;host_components?HostRoles&#x2F;component_name&#x3D;JOURNALNODE 删除journalnode： curl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X DELETE http:&#x2F;&#x2F;10.10.111.11:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;rhino&#x2F;hosts&#x2F;rhino011&#x2F;host_components&#x2F;JOURNALNODEcurl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X DELETE http:&#x2F;&#x2F;10.10.111.11:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;rhino&#x2F;hosts&#x2F;rhino012&#x2F;host_components&#x2F;JOURNALNODEcurl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X DELETE http:&#x2F;&#x2F;10.10.111.11:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;rhino&#x2F;hosts&#x2F;rhino051&#x2F;host_components&#x2F;JOURNALNODE 7、删除额外的 namenode查看 namenode： curl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X GET http:&#x2F;&#x2F;10.10.111.11:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;rhino&#x2F;host_components?HostRoles&#x2F;component_name&#x3D;NAMENODE 删除 namenode： curl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X DELETE http:&#x2F;&#x2F;10.10.111.11:8080&#x2F;api&#x2F;v1&#x2F;clusters&#x2F;rhino&#x2F;hosts&#x2F;rhino012&#x2F;host_components&#x2F;NAMENODE 8、恢复 hdfs 配置 在 ambari 的页面上将 hdfs 的配置版本信息改为 HA 之前的版本","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇十五：手动安装 Kafka Manager","slug":"ambari-install-guides-15-install-kafka-manager-md","date":"2020-05-31T09:55:31.000Z","updated":"2020-07-25T11:14:40.027Z","comments":true,"path":"2020/05/31/ambari-install-guides-15-install-kafka-manager-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-15-install-kafka-manager-md/","excerpt":"1、简介Kafka Manager 可用于监控和管理 kafka 集群。","text":"1、简介Kafka Manager 可用于监控和管理 kafka 集群。 主要作用有： 便捷的监控集群状态（topics，consumers，offsets，brokers，副本分布，分区分布）； 根据自定义配置创建 topic； 删除 topic，但是前提是配置 kafka 的 delete.topic.enable=true； 增加已存在 topic 的分区； 更新已存在 topic 的配置等等。 2、修改 Kafka 配置【在每个 Kafka broker 节点的 kafka 用户下执行】 修改 kafka 启动文件：kafka-server-start.sh 文件： [kafka@node1 ~]$ vim &#x2F;home&#x2F;kafka&#x2F;kafka-1.1.0&#x2F;bin&#x2F;kafka-server-start.sh#在 EXTRA_ARGS&#x3D;$&#123;EXTRA_ARGS-&#39;-name kafkaServer -loggc&#39;&#125;上面添加以下内容，如果已存在，修改端口号即可export JMX_PORT&#x3D;&quot;8999&quot; 注意：1.端口8999不能被占用；2.所有安装kafka的机器都需要修改 重启kafka登录 ambari 页面，先点击 Kafka，然后点击右上角 ACTION 里的 Restart All。 1.端口 8999 是监听 kafka 的。修改完该配置项后，需要重启 kafka，8999 端口才会被监听。2.该端口进程不要删。如果不小心删掉了，需要重启一下 kafka。 3、安装 Kafka Manager【rhino 用户下执行】先将 kafka-manager-1.3.3.4.zip 安装包上传至任意一台服务器 /home/rhino 目录下。（建议避开管理节点的机器）解压安装包 [rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino[rhino@node1 ~]$ unzip kafka-manager-1.3.3.4.zip 修改配置文件 [rhino@node1 ~]$ vim &#x2F;home&#x2F;rhino&#x2F;kafka-manager-1.3.3.4&#x2F;conf&#x2F;application.conf修改以下内容：增加所有zookeeper的地址kafka-manager.zkhosts&#x3D;&quot;node1:2181,node2:2181,node3:2181&quot; 启动kafka-manager [rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;kafka-manager-1.3.3.4[rhino@node1 ~]$ nohup bin&#x2F;kafka-manager -Dconfig.file&#x3D;conf&#x2F;application.conf -Dhttp.port&#x3D;9000 &amp; 1.9000是kafka-manager的默认端口，不要修改；2.确保9000端口没有被占用。 4、配置kafka manager登录管理界面 http://172.168.1.1:9000，对 kafka manager 进行配置。增加集群点击 Cluster → Add Cluster 配置集群信息修改图中红圈几处，点击保存即可。 配置完成后，即可使用 Kafka Manager 进行管理操作","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇十四：手动安装Spark","slug":"ambari-install-guides-14-install-spark-md","date":"2020-05-31T09:54:08.000Z","updated":"2020-07-25T11:14:34.624Z","comments":true,"path":"2020/05/31/ambari-install-guides-14-install-spark-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-14-install-spark-md/","excerpt":"1、Spark 简介Apache Spark 是一个分布式计算框架，旨在简化运行于计算机集群上的并行程序的编写。该框架对资源调度，任务的提交、执行和跟踪，节点间的通信以及数据并行处理的内在底层操作都进行了抽象。它提供了一个更高级别的 API 用于处理分布式数据。从这方面说，它与 Apache Hadoop 等分布式处理框架类似。但在底层架构上，Spark 与它们有所不同。","text":"1、Spark 简介Apache Spark 是一个分布式计算框架，旨在简化运行于计算机集群上的并行程序的编写。该框架对资源调度，任务的提交、执行和跟踪，节点间的通信以及数据并行处理的内在底层操作都进行了抽象。它提供了一个更高级别的 API 用于处理分布式数据。从这方面说，它与 Apache Hadoop 等分布式处理框架类似。但在底层架构上，Spark 与它们有所不同。 运行模式上，Spark 支持四种运行模式： 本地单机模式：所有 Spark 进程都运行在同一个 Java 虚拟机（Java Vitural Machine，JVM）中。 集群单机模式：使用 Spark 自己内置的任务调度框架。 基于 Mesos：Mesos 是一个流行的开源集群计算框架。 基于 YARN：即 Hadoop 2，它是一个与 Hadoop 关联的集群计算和资源调度框架。 本文将讲述基于 YARN 的 Spark 分布式集群部署指导。 注意：本次安装以3台节点为例，其中 node1 为主节点，node2、node3 为 slaves 节点，本次安装采取主节点安装配置，然后分发到各个 slaves 的方式来进行部署 2、主节点安装 Scala【在 Spark 主节点的 rhino 用户下执行】以 rhino 用户安装，将安装包 scala-2.11.7.tar.gz 上传至 /home/rhino 目录下。 解压 scala-2.11.7.tar.gz [rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino[rhino@node1 ~]$ tar -zxvf scala-2.11.7.tar.gz 3、主节点安装 Spark 【在 Spark 主节点的 rhino 用户下执行】以 rhino 用户安装，将安装包 spark-2.2.0-bin-hadoop2.6.tgz 上传至 /home/rhino 目录下。这里以安装3台为例。 解压 spark 安装包 [rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino[rhino@node1 ~]$tar -zxvf spark-2.2.0-bin-hadoop2.6.tgz 修改环境变量 [rhino@node1 ~]$ vim .bashrc 添加如下内容（如果有已经添加的，可以不需要再添加；实际路径根据实际情况填写） export JAVA_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181export SCALA_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;scala-2.11.7export LD_LIBRARY_PATH&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;lib:$LD_LIBRARY_PATHexport SPARK_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6export SPARK_PID_DIR&#x3D;&#x2F;home&#x2F;rhino&#x2F;pidsexport HADOOP_HOME&#x3D;&#x2F;usr&#x2F;hdp&#x2F;2.6.3.0-235&#x2F;hadoopexport PATH&#x3D;$JAVA_HOME&#x2F;bin:$SCALA_HOME&#x2F;bin:$SPARK_HOME&#x2F;bin:$PATH 3.1 修改 Spark 配置文件 配置 slaves 节点主机名 （注意：主节点主机名不要填写，只填写 slaves 节点的主机名。本文集群机器总共3台，一台主节点，两台 slaves 节点） [rhino@rhino001 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;conf [rhino@rhino001 conf]$ cp slaves.template slaves[rhino@rhino001 conf]$ vim slaves 添加如下内容 node2node3 配置 spark-en.sh 文件 [rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;conf[rhino@node1 conf]$ cp spark-env.sh.template spark-env.sh[rhino@node1 conf]$ vim spark-env.sh 在spark-env.sh文件末添加所需的环境变量配置（实际路径根据实际情况填写）： export JAVA_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181export SCALA_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;scala-2.11.7export SPARK_MASTER_IP&#x3D;192.168.50.213export SPARK_WORKER_MEMORY&#x3D;2gexport HADOOP_CONF_DIR&#x3D;&#x2F;usr&#x2F;hdp&#x2F;2.6.3.0-235&#x2F;hadoop&#x2F;etc&#x2F;hadoopexport YARN_CONF_DIR&#x3D;&#x2F;usr&#x2F;hdp&#x2F;2.6.3.0-235&#x2F;hadoop-yarn&#x2F;etc&#x2F;hadoopexport SPARK_CONF_DIR&#x3D;&#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;confexport SPARK_CLASSPATH&#x3D;$SPARK_CLASSPATH:&#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;jars&#x2F;mysql-connector-java-5.1.35.jar 配置 spark-defaults.conf 文件，在 spark-defaults.conf 文件末添加所需的配置： [rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;conf [rhino@node2 conf]$ cp spark-defaults.conf.template spark-defaults.conf [rhino@node3 conf]$ vim spark-defaults.conf 添加如下内容（实际路径根据实际情况进行配置） #spark临时目录spark.local.dir &#x2F;home&#x2F;rhino&#x2F;tmpspark.sql.warehouse.dir hdfs:&#x2F;&#x2F;mycluster&#x2F;user&#x2F;hive&#x2F;warehousespark.yarn.jars hdfs:&#x2F;&#x2F;mycluster&#x2F;sharkjars-2.0&#x2F;* 新建配置文件 hive-site.xml [rhino@node1 ]$ vim &#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;conf&#x2F;hive-site.xml 添加如下内容，注意根据实际情况，修改相关配置参数 &lt;configuration&gt;&lt;property&gt;&lt;name&gt;hive.metastore.client.socket.timeout&lt;&#x2F;name&gt;&lt;value&gt;3600&lt;&#x2F;value&gt;&lt;description&gt;MetaStore Client socket timeout in seconds&lt;&#x2F;description&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;&lt;value&gt;jdbc:mysql:&#x2F;&#x2F;172.168.1.1:3306&#x2F;hive?createDatabaseIfNotExist&#x3D;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt;&lt;value&gt;rhino&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt;&lt;value&gt;xxxxxxxx&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt; 注意： &lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;&lt;value&gt;jdbc:mysql:&#x2F;&#x2F;172.168.1.1:3306&#x2F;hive?createDatabaseIfNotExist&#x3D;true&lt;&#x2F;value&gt; # 这个value里面写的是hive存在mysql上的元数据库的地址，如果是使用ambari安装的hive，则ip应该为ambari使用的mysql的ip，后面的hive_rhino为hive生产环境的库名，如果不知道怎么配，直接登录ambari，点击Hive &gt; configs &gt; DATABASE，将Database URL 配置项的内容直接拷贝到&lt;value&gt; &lt;&#x2F;value&gt;之间即可&lt;&#x2F;property&gt;扩展：spark需要连接hive的元数据库，通过获取元数据库内的元数据，才能建立spark与hive的链接，从而能够使用sparksql来操作hive。 3.2 拷贝 hadoop 的 jar 包【在 Spark 主节点的 root 用户下执行】从 /usr/hdp/2.6.3.0-235/hadoop/client 中拷贝 jersey-client-1.9.jar 和 jersey-core-1.9.jar 到 spark 的 jars 中 su - root[root@node1 ~]# cd &#x2F;usr&#x2F;hdp&#x2F;2.6.3.0-235&#x2F;hadoop&#x2F;client&#x2F;[root@node1 client] cp jersey-client-1.9.jar &#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;jars[root@node1 client]# cp jersey-core-1.9.jar &#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;jars[root@node1 client]# cd &#x2F;home&#x2F;rhino&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;jars[root@node1 jars]# chmod 755 *[root@node1 jars]# chown rhino:root * 4、分发安装包至 slaves 节点【在 Spark 主节点的 rhino 用户下执行】拷贝 scala-2.11.7 及环境变量到 slave 机器。（这里以安装3台为例） [rhino@node1 ~]$ scp -r ~&#x2F;scala-2.11.7 rhino@node2:~[rhino@node1 ~]$ scp -r ~&#x2F;scala-2.11.7 rhino@node3:~ 拷贝 spark-2.2.0-bin-hadoop2.6 及环境变量到 slave 机器 [rhino@node1 ~]$ scp -r ~&#x2F;spark-2.2.0-bin-hadoop2.6 rhino@node2:~[rhino@node1 ~]$ scp -r ~&#x2F;spark-2.2.0-bin-hadoop2.6 rhino@node3:~[rhino@node1 ~]$ scp ~&#x2F;.bashrc rhino@node2:~[rhino@node1 ~]$ scp ~&#x2F;.bashrc rhino@node3:~ 5、加载环境变量及创建软链接【在 Spark 所有节点的 rhino 用户下执行】使每台机器环境变量生效 [rhino@node1 ~]$ source ~&#x2F;.bashrc 在每台机器上创建软链接 [rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino[rhino@node1 ~]$ ln -s spark-2.2.0-bin-hadoop2.6 spark 6、上传 jar 包到 HDFS 【在 Spark 主节点的 rhino 用户下执行】将 spark 环境中的 jar 包上传到 HDFS [rhino@node1 ~]$hdfs dfs -mkdir hdfs:&#x2F;&#x2F;mycluster&#x2F;sharkjars-2.0&#x2F;[rhino@node1 ~]$ hdfs dfs -put ~&#x2F;spark-2.2.0-bin-hadoop2.6&#x2F;jars&#x2F;* hdfs:&#x2F;&#x2F;mycluster&#x2F;sharkjars-2.0&#x2F; 验证 jar 包上传是否成功通过浏览器登录 HDFS 界面：http://172.168.1.1:50070/explorer.html查看 Utilities 项，并选择 browse the file system，结果如下图： 7、启动thriftserver【在 Spark 主节点的 rhino 用户下执行】即启动 sparksql，需要时启动。 如果不用 sparksql 就不要启动。 ~&#x2F;spark&#x2F;sbin&#x2F;start-thriftserver.sh --master yarn-client --executor-memory 1g --executor-cores 2 --queue root.spark-mid --executor-memory: 指定内存--executor-cores: 指定核数--queue: 指定 yarn 资源队列","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇十三：Ambari 安装 Impala","slug":"ambari-install-guides-13-install-impala-md","date":"2020-05-31T09:53:13.000Z","updated":"2020-07-25T11:14:27.489Z","comments":true,"path":"2020/05/31/ambari-install-guides-13-install-impala-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-13-install-impala-md/","excerpt":"1、准备工作 确认 GreenPlum 版本，以及 GreenPlum 部署规划 确认 GreenPlum 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 GreenPlum 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下","text":"1、准备工作 确认 GreenPlum 版本，以及 GreenPlum 部署规划 确认 GreenPlum 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 GreenPlum 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下 如果版本包未上传，则需要上传到指定的目录，并且重启 ambari server 2、安装部署Impala 点击 Add Service 选择 Impala 服务并点击 NEXT： 添加 impala 的 statestored/catalogd 节点（需要放在同一台服务器上），点击 NEXT 注意：statestored/catalogd 尽量不要安装在管理节点上。选择一台数据节点安装。且 statestored/catalogd 进程尽量不要和 impalad 合设。 选择 impalad 节点 impalad 进程和 datanode 保持一致。statestored/catalogd 进程所在的机器上不安装 impalad 进程。这里机器较少，所以全部都选择了。 Advanced hive-config content 中根据实际需求修改密码，并修改 URL 路径 根据实际情况修改 content 配置项中的 IP、端口、密码以及元数库名称。如果 mysql 是双击切换，则这里填写的是虚拟 IP；元数据库为 hive_rhino；密码修改为 xxxxxxxx 在 Advanced impala-env 配置项中修改 impala_download、catalog_service_host、state_store_host IP地址: http://172.168.1.1/ambari/extend/impala-2.12.0.tar.gz172.168.1.1 根据实际的 ambari server 地址填写。 3、确认安装 Impala点击 NEXT，直到 DEPOLY，安装部署 impala 启动 catalog_service：点击 IMPALA_CATALOG_SERVICE → Start 完成状态 4、启停点击右上角 ACTIONS 里的 Stop，即可执行停止服务操作；点击 Start ，即可执行启动服务操作。","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇十二：Ambari 安装 Neo4j","slug":"ambari-install-guides-12-install-neo4j-md","date":"2020-05-31T09:52:10.000Z","updated":"2020-07-25T11:07:38.470Z","comments":true,"path":"2020/05/31/ambari-install-guides-12-install-neo4j-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-12-install-neo4j-md/","excerpt":"1、准备工作 确认 Neo4j 版本，以及 Neo4j 部署规划 确认 Neo4j 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 Neo4j 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下","text":"1、准备工作 确认 Neo4j 版本，以及 Neo4j 部署规划 确认 Neo4j 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 Neo4j 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下 如果版本包未上传，则需要上传到指定的目录，并且重启 ambari server 2、安装部署Neo4j点击 Add Service 选择 Neo4j 版本的服务并点击 NEXT： 弹出如下图，点击 NEXT 按钮 修改 Advanced neo4j-env 中的 neo4j_download 配置http://172.168.1.1/ambari/extend/neo4j.tar.gz。（修改 IP 地址和 neo4j 包名） 注意：端口号不能被占用。 点击 NEXT 按钮，弹出如下图，点击红色按钮继续 3、确认安装点击 DEPLOY 按钮继续，等待安装完成。 安装完成后页面如下图，点击 NEXT 弹出如下图，点击 COMPLETE 按钮，完成结束。 完成后页面如下图，左边红色框内服务前面的圆点是绿色，中间红色框内显示为已启动，右边红色框内显示的是 neo4j 服务自带的 web 快速连接，点击可跳转。 4、启停 4.1 启动neo4j 前置状态是已停止登录 ambari 页面后，点击红色框内按钮进入 neo4j 主页面，如下图，点击右上角绿色的 ACTIONS 按钮 弹出如下图，点击红色框内按钮 弹出如下图，点击红色框内按钮，等待服务启动 弹出如下图，表示 neo4j 服务成功启动，点击绿色的 OK 按钮，结束 neo4j 服务启动后的主页面如下图 4.2 停止neo4j 前置状态是已启动登录 ambari 页面后，点击红色框内按钮进入 neo4j 主页面，如下图，点击右上角绿色的 ACTIONS 按钮 弹出如下图，点击红色框内按钮 弹出如下图，点击红色框内按钮，等待服务停止 弹出如下图，表示 neo4j 服务成功停止，点击绿色的 OK 按钮，结束 neo4j服务停止后的主页面如下图","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇十一：Ambari 安装 Kafka","slug":"ambari-install-guides-11-install-kafka-md","date":"2020-05-31T09:51:14.000Z","updated":"2020-07-25T11:07:24.651Z","comments":true,"path":"2020/05/31/ambari-install-guides-11-install-kafka-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-11-install-kafka-md/","excerpt":"1、准备工作 确认 Kafka 版本，以及 Kafka 部署规划 确认 Kafka 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 Kafka 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下","text":"1、准备工作 确认 Kafka 版本，以及 Kafka 部署规划 确认 Kafka 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 Kafka 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下 如果版本包未上传，则需要上传到指定的目录，并且重启 ambari server 2、安装部署点击 Add Service 选择 Kafka1_1 服务并点击 NEXT 添加 kafka1_1_broker 节点并点击 NEXT Kafka部署情况以规划部署方案为准。这里以3台为例 3、修改配置 Advanced kafka1.1-broker-server 配置 port, 填写 9092，如果端口被占用，可根据实际情况进行修改；kafka.metrics.reporters，填写 org.apache.hadoop.metrics2.sink.kafka.KafkaTimelineMetricsReporter ;zookeeper.connect，配置所有 zookeeper 节点的 主机名:端口；log.dir 具体填写 /data01/kafka-logs,/data02/kafka-logs,/data03/kafka-logs,/data04/kafka-logs,/data05/kafka-logs,/data06/kafka-logs,/data07/kafka-logs,/data08/kafka-logs,/data09/kafka-logs,/data010/kafka-logs （如果有多块磁盘，并对kafka的性能有较高要求，建议在每个磁盘下都配置一个目录，这里以10块盘为例） kafka 老化时间配置 log.rentention.hours: 默认 168 小时，可根据实际流量计硬盘容量情况配置 老化时间计算方法：假设现网实际场景中，8 台 kafka 服务器，每台服务器 12 块盘，单块容量 1.8T；现网流量数据，最大吞吐值为 10Gbps。那么 96X 1.8X 1024 X 1024 (M) ÷ （1.25 GB X 1024）÷ 3600(s) = 39.3 小时；即在峰值流量情况下，磁盘最多可保存 39.3 个小时的数据量。为了保证磁盘利用率，10Gbps 流量情况下，老化时间配置为 8 小时；15Gbps 流量下配置为 6 小时。 Advanced kafka1.1-env 配置 kafka_download，修改为：http://172.168.1.1/ambari/extend/kafka-1.1.0.tar.gz Advanced kafka1.1-broker-server优化配置修改 4、确认安装配置完成后一直点击下一步，执行安装部署： 5、手动修改部分参数【在所有 Kafka Broker 节点的 kafka 用户下执行】 kafka 在安装完成后，部分配置内会存在一些使用 localhost 的参数，这可能会导致业务应用启动报错，建议按本节内容修改。或者也可以跳过本节内容，届时如果业务应用启动时，报形如：Unexpected error fetching metadata for topic xxlww2 的错误时，再来修改这些参数。 修改 /home/kafka/kafka/kafka-1.1.0/conf 下 connect-distributed.properties、connect-standalone.properties、consumer.properties 和 producer.properties，将四个配置文件内所有的 localhost 全部修改为实际的 IP 地址 6、启停","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——番外一：Greenplum 恢复操作","slug":"ambari-install-guides-ex1-recover-greenplum-md","date":"2020-05-31T09:49:46.000Z","updated":"2020-07-25T11:12:31.910Z","comments":true,"path":"2020/05/31/ambari-install-guides-ex1-recover-greenplum-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-ex1-recover-greenplum-md/","excerpt":"当 Greenplum 的组件异常时，可进行恢复操作。Greenplum 恢复功能分为 2 大类：master 恢复和 primary(mirror) 恢复。","text":"当 Greenplum 的组件异常时，可进行恢复操作。Greenplum 恢复功能分为 2 大类：master 恢复和 primary(mirror) 恢复。 1、master 恢复当 master 主机出现故障时（monitor 依赖 master，如果 master 出故障，monitor 也出故障），如图： 点击 ACTIONS 中的 Activestandby 按钮，通过 standby 恢复 master，执行成功后如下图： 点击 GPDB Master 启动按钮，等待 master 状态恢复。 点击 GPDB Monitor 启动按钮，等待 monitor 状态恢复。 点击 GPDB Standby Master 启动按钮，等待 standby 状态恢复。 2、Standby Master 恢复 当 standby 出现故障时，关闭 master，等状态全部 down 之后，点击 ACTIONS 中的 start。 3、primary(mirror) 恢复 recover 当 host 出现状态 down 时，启动一下状态，如果还未恢复，使用 recover。检查失败的 segment，&#39;d&#39; 代表 &#39;down&#39; [rhino@node1 ~]$ psql -c &quot;SELECT * FROM gp_segment_configuration WHERE status&#x3D;&#39;d&#39;;&quot; 也可以通过 gpstate -c 查看 [rhino@hadoop134 ~]$ gpstate -c 上图为 node2 机器上 Mirror Failed(/data02/mirror/gpseg1)，(/data02/mirror/gpseg1)Primary Failed(/data02/gpdata/gpseg3) 。因为都是一台机器上的 /data02 磁盘出了问题，怀疑是否是磁盘问题，如果没有挂载，需要挂载： [root@node2 ~]# mount &#x2F;dev&#x2F;sdc1 &#x2F;data02&#x2F; 修复失败的 segment，点击 ACTIONS → Recover fullrecover 当 recover 失效或者 primary(mirror) 对应的磁盘出现文件缺失时，可以使用全量恢复. reblance 当 primary segment 宕机，mirror 成为了 primary segment。执行 gprecoverseg 命令恢复失败 segment 后，当前 primary segment 是原来的 mirror，原来失败的 primary segment 变成了现在的 mirror。并没有恢复到系统初始化时候的角色状态，造成一个主机上有多个 primary segment，会导致系统处于非平衡状态，消耗更多的系统资源。 gpstate -c ，确认所有 mirrors 都是同步的。 也可以通过 metrics 页面，查看 greenplum resyncing_mode 为 0 时，代表所有 mirrors 是同步的。 如果 mirrors 的状态是 Resynchronizing，等待完成。当状态都为 &#39;Synchronized&#39;，就可以进行 reblance 操作。 进行 reblance 切换的时候，建议不要做其他操作。reblance 成功后，greenplum unbalanced 为 0。","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇十：Ambari 安装 GreenPlum","slug":"ambari-install-guides-10-install-greenplum-md","date":"2020-05-31T09:47:55.000Z","updated":"2020-07-25T11:07:10.179Z","comments":true,"path":"2020/05/31/ambari-install-guides-10-install-greenplum-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-10-install-greenplum-md/","excerpt":"1、准备工作 确认 GreenPlum 版本，以及 GreenPlum 部署规划 确认 GreenPlum 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 GreenPlum 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下","text":"1、准备工作 确认 GreenPlum 版本，以及 GreenPlum 部署规划 确认 GreenPlum 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 GreenPlum 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下 如果版本包未上传，则需要上传到指定的目录，并且重启 ambari server 1.1 修改 Linux 内核参数【在需要安装 GP 的节点的 root 用户下执行】 注意：本节有部分参数已经在安装 Ambari 前的准备工作中做完，所以请先检查参数是否已经修改，没有修改的再进行修改 修改 sysctl.conf 文件 [root@node1 ~]# su - root[root@node1 ~]# vim &#x2F;etc&#x2F;sysctl.conf 添加如下内容： vm.max_map_count &#x3D; 262144vm.swappiness &#x3D; 0vm.min_free_kbytes &#x3D; 2097152vm.extra_free_kbytes &#x3D; 4194304fs.aio-max-nr &#x3D; 1048576fs.file-max &#x3D; 76724600kernel.sem &#x3D; 250 5120000 100 20480# 所有共享内存段加起来最大(例如，主机内存256G，则使用内存的80%，单位PAGE&#x3D;4096)# 计算方法为：256*0.8*1024*1024*1024&#x2F;4096kernel.shmall &#x3D; 53687092# 单个共享内存段最大(主机内存的一半，单位字节)# 计算方法为：256&#x2F;2*1024*1024*1024kernel.shmmax &#x3D; 137438953472kernel.shmmni &#x3D; 4096kernel.sysrq &#x3D; 1kernel.core_users_pid &#x3D; 1kernel.msgmnb &#x3D; 65536kernel.msgmax &#x3D; 65536kernel.msgmni &#x3D; 2048net.core.netdev_max_backlog &#x3D; 10000net.core.rmem_max &#x3D; 4194304net.core.wmem_max &#x3D; 4194304net.core.rmem_default &#x3D; 262144net.core.wmem_default &#x3D; 262144net.core.somaxconn &#x3D; 4096net.ipv4.tcp_max_syn_backlog &#x3D; 4096net.ipv4.tcp_keepalive_intvl &#x3D; 20net.ipv4.tcp_keepalive_probes &#x3D; 3net.ipv4.tcp_keepalive_time &#x3D; 60net.ipv4.tcp_mem &#x3D; 8388608 12582912 16777216net.ipv4.tcp_fin_timeout &#x3D; 5net.ipv4.tcp_synack_retries &#x3D; 2net.ipv4.tcp_syncookies &#x3D; 1net.ipv4.tcp_timestamps &#x3D; 1net.ipv4.tcp_rmem &#x3D; 8192 87380 16777216net.ipv4.tcp_wmem &#x3D; 8192 65536 16777216# net.ipv4.tcp_tw_recycle和net.ipv4.tcp_timestamps不建议同时开启net.ipv4.tcp_tw_recycle &#x3D; 1net.ipv4.tcp_tw_reuse &#x3D; 1net.ipv4.tcp_max_tw_buckets &#x3D; 200000net.ipv4.ip_local_port_range &#x3D; 10000 65535net.ipv4.ip_forward &#x3D; 0net.ipv4.conf.default.accept_source_route &#x3D; 0net.ipv4.conf.default.rp_filter &#x3D; 1net.ipv4.conf.default.arp_filter &#x3D; 1net.ipv4.conf.all.arp_filter &#x3D; 1# CentOS 6# net.nf_conntrack_max &#x3D; 1000000# net.netfilter.nf_conntrack_max &#x3D; 1000000# 根据实际IOPS能力以及内存大小设置vm.dirty_background_bytes &#x3D; 4096000000vm.dirty_expire_centisecs &#x3D; 6000vm.dirty_ratio &#x3D; 80vm.dirty_writeback_centisecs &#x3D; 50# 6xxxx 防止内核隐藏的BUG导致的问题# vm.mmap_min_addr &#x3D;# vm.overcommit_memory &#x3D;0时，vm.overcommit_ratio可以不设置vm.overcommit_memory &#x3D; 2vm.overcommit_ratio &#x3D; 95# 不使用NUMAvm.zone_reclaim_mode &#x3D; 0# 如果要使用PostgreSQL的huge page，建议设置它。大于数据库需要的共享内存即可。# vm.nr_hugepages &#x3D; 102352 注意：vm.overcommit_ratio=95，是通过网页地址http://greenplum.org/calc/, 输入服务器总内存，得出的结果之一。 执行 sysctl -p 使其配置生效。 [root@node1 ~]# sysctl -p 关闭监控程序的服务端和客户端 [root@node1 ~]# systemctl start tuned[root@node1 ~]# tuned-adm off[root@node1 ~]# tuned-adm list[root@node1 ~]# systemctl stop tuned[root@node1 ~]# systemctl disable tuned 修改 limits.conf 该文件中的参数，部分与此前修改的有重复，以之前修改的为准 [root@rhino131 ~]# vim &#x2F;etc&#x2F;security&#x2F;limits.conf 修改如下内容 * soft nofile 65536* hard nofile 65536* soft nproc 131072* hard nproc 131072* soft core unlimited* - memlock unlimited* - as unlimited* - data unlimited* - fsize unlimited* - rss unlimited* - nproc unlimited 删除 nproc 限制 [root@node1 ~]# rm -rf &#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;* 1.2 配置daedb用户无密码访问在创建gp集群时（集群数大于等于2），需要手动建立信任关系。 1.2.1 创建 daedb 用户【在所有 GP 节点的 root 用户下执行】密码根据需求设置 [root@node1 ~]# useradd -m daedb[root@node1 ~]# passwd daedb 1.2.2 设置 daedb 用户的无密码访问【在所有 GP 节点的 root 用户下执行】本节所有操作都需要顺序在每个 GP 节点上操作一遍 [root@node1 ~]# su - daedb[root@node1 ~]# ssh-keygen -t rsa 将公钥复制到需要无密码登陆的服务器上 [daedb@node1 ~]# ssh-copy-id -i ~&#x2F;.ssh&#x2F;id_rsa.pub daedb@172.168.1.1[daedb@node1 ~]# ssh-copy-id -i ~&#x2F;.ssh&#x2F;id_rsa.pub daedb@172.168.1.2[daedb@node1 ~]# ssh-copy-id -i ~&#x2F;.ssh&#x2F;id_rsa.pub daedb@172.168.1.3 请严格操作，否则后续会报错。机器首次安装需要做，后面重复安装可不需要进行此命令。 验证无密码访问是否成功。 集群内的机器都做好无密码访问后，通过ssh指令来验证，看集群内的机器是否都已经达到了互相之间的无密码访问。首次必须要操作。例如在 node1 节点上可以分别执行： [daedb@node1 ~]# ssh daedb@172.168.1.1[daedb@node1 ~]# ssh daedb@172.168.1.2[daedb@node1 ~]# ssh daedb@172.168.1.3 1.3 Swap分区配置【在所有 GP 节点的 root 用户下执行】 如果swap分区大小已经是64G，本节内容可以忽略不操作。 配置建议： 生成交换分区文件，注意交换分区的大小需要根据实际的内存大小而定，物理内存 256G 为例，生成 SWAP 为 64G 左右。执行以下命令进行修改。 [root@node1 ~]# dd bs&#x3D;1024k if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;home&#x2F;swap conv&#x3D;fdatasync count&#x3D;65526[root@node1 ~]# &#x2F;sbin&#x2F;mkswap &#x2F;home&#x2F;swap[root@node1 ~]# &#x2F;sbin&#x2F;swapon &#x2F;home&#x2F;swap 查看是否swap生效 [root@node1 ~]# free -g total used free shared buff&#x2F;cache availableMem: 376 126 71 4 178 235Swap: 79 0 79 1.4 本地 YUM 源安装 gp 需要的软件执行以下命令，出现报错可忽略。 yum -y install rsyncyum -y install coreutilsyum -y install glib2yum -y install lrzszyum -y install sysstatyum -y install e4fsprogsyum -y install xfsprogsyum -y install ntpyum -y install zlibyum -y install opensslyum -y install smartmontoolsyum -y install flexyum -y install bisonyum -y install perlyum -y install perl-ExtUtils*yum -y install OpenIPMI-toolsyum -y install openldapyum -y install logrotateyum -y install python-pyyum -y installed#nfs功能使用yum -y install nfs-utils#虚拟IP设置使用，需要进入mnt挂载目录Package下执行yum -y install keepalived 2、安装部署 GreenPlum进入ambari界面后点击 … 选择 Add Service，在选项框中勾选上 GreenPlum，点击 NEXT。 选择需要安装的服务器，GPDB Monitor需要与GPDB Master安装在同一台服务器上，GPDB Standby Master与GPDB Master不可选同一台机器。继续点击下一步。 选择需要安装的GPDB Host。 2.1 修改 GreenPlum 配置文件 【Advanced gp-env】配置项： &lt;1&gt; metrics_collector_host_ip： 填写 Ambari Metrics 的地址&lt;2&gt; gp_base_dir： gp安装目录&lt;3&gt; gp_log_dir： gp日志存放目录&lt;4&gt; gp_pid_dir： pid存放目录&lt;5&gt; gp_download： gp版本存放目录，修改IP和需要安装的包名称&lt;6&gt; gp_conf_dir : gp配置文件目录（暂时不用）&lt;7&gt; greenplum_monitor_download_url： greenplum monitor 监控版本存放目录，需修改默认值中的 ip&lt;8&gt; host_disk_num： greenplum 使用的磁盘数，存放 gp 数据，根据实际磁盘数量填写。（单台机器上的数目）&lt;9&gt; seg_hosts: 配置部署集群的服务器 host，如 node1,node2,node3中间用英文逗号隔开&lt;10&gt; standby_hostname： greenplum standby master 节点的主机名 【Advanced gp-mirror】配置项: &lt;1&gt; mirror_host_num： 集群中 mirror 机器数量&lt;2&gt; mirror_host_seq： 安装 mirror 与机器之间的对应，顺序要颠倒 mirror_host_seq配置说明如果gp安装3台机器，seg_hosts填写的是 node1,node2,node3，那么 mirror_host_seq 就需要反过来写 node3,node2,node1 【Advanced greenplum-site】配置项： &lt;1&gt; greenplum_data_dir: Primary segment 数据目录， 如 /data01/gpdata /data02/gpdata 中间使用空格分开，根据实际需要填写。&lt;2&gt; mirror_data_directory: Mirror segment 数据目录，如 /data01/mirror /data02/mirror 中间用空格分开，根据实际需要填写。&lt;3&gt; master_directory: 指定 master 数据目录&lt;4&gt; machine_list_file: seghosts 文件目录 其他采用默认值即可，如下图所示： 2.2 确认安装GreenPlum点击 PROCEED ANYWAY ,点击 DEPLOY 继续下一步，然后查看版本安装进度。 2.3 完成安装 GreenPlum安装启动正常后，可以看到 GPDB Master 和 GPDB Host 的绿色标记。 GreenPlum 安装启动正常后，可在服务器 daedb 用户下输入 gpstate 命令进行查看，如下图所示，能查看到 gp 的信息，说明 gp 已经正常启动了。 gp 初次安装完成后，建议重启一次，因为有可能起不来，原因是需要给 gpmaster 目录和子目录 700 权限，gp 在服务器重启后，需要检查 swap 挂载以及 daedb 用户下的 gpmaster 目录和子目录的权限，需要给 700 权限 3、启停GreenPlum如果要停掉 gp 服务，Services 目录下选中 GREENPLUM，点击 ACTIONS 选择 stop 即可将 gp 数据库停掉。此时在服务器输入 gpstate 查看，如下图所示： 4、安装失败后重装操作如果在 ambari 上安装 greenplum 失败后需要重装，则需要在服务器创建的 daedb 用户下执行以下命令进行重置。 每个节点都需要操作[root@node1 ~]# su - daedb[daedb@node1 ~]$ gpstop -a -M immediate[daedb@node1 ~]$ cd &#x2F;home&#x2F;daedb&#x2F;daedb&#x2F;etc[daedb@node1 ~]$ gpssh -f seghostsrm -fr &#x2F;tmp&#x2F;.s.PGSQL.*rm -fr &#x2F;home&#x2F;daedb&#x2F;*rm -fr &#x2F;data*&#x2F;gpdata&#x2F;*rm -fr &#x2F;data*&#x2F;mirror&#x2F;*最后还需要删除 &#x2F;var&#x2F;run 下所有 daedb 属组的文件及目录 5、Greenplum告警配置若需要对 Greenplum 进行监控，则进行以下操作。 点击页面 metrics 标签下的 + 号： 点击 Create Widget 按如图进行配置：依次选中以下参数，点击 ADD 即可。若要增加多个参数，点击 Add Expression。","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇九：Ambari 安装 Redis","slug":"ambari-install-guides-9-install-redis-md","date":"2020-05-31T09:46:56.000Z","updated":"2020-07-25T11:14:05.803Z","comments":true,"path":"2020/05/31/ambari-install-guides-9-install-redis-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-9-install-redis-md/","excerpt":"1、准备工作 确认 Redis 版本，以及 Redis 部署规划 确认 Redis 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 Redis 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下","text":"1、准备工作 确认 Redis 版本，以及 Redis 部署规划 确认 Redis 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 Redis 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下 如果版本包未上传，则需要上传到指定的目录，并且重启 ambari server 2、安装部署 Redis点击 Add Service 选择 Redis 服务并点击 NEXT 根据部署规划，添加 redis master 节点和 redis monitor 节点并点击 NEXT 根据部署规划，添加redis slave节点 3、修改 Redis 配置文件 在 Advand redis-conf-cluster 配置项 勾选 Cluster Enabled 选项Cluster Master Host List: 添加之前选择节点的 ip 地址Cluster Slave Host List: 添加之前选择节点的 ip 地址具体如下所示： 在 Advanced redis-conf-storage 配置项中，可修改 DB Directory 选项的数据存储路径 数据存放路径最好选择在已经做了 RAID ，且非 RAID0 的磁盘阵列上，这样数据有备份保障，如果不能满足，建议选择一块数据盘存放，比如：/data01/redis/data1 在 Advanced redis-env 配置项中 修改 Metrics Collector Host Ip，对应 metric collector 进程所在的服务器地址；修改 Redis Download Url 配置里 IP 地址：http://72.168.1.1/ambari/extend/redis-5.0.3.tar.gz（redis-5.0.3.tar.gz 这个包名称根据实际的redis的版本名称进行修改）IP 为 ambari severIP修改 Redis Monitor Download Url 配置里的 IP 地址：http://172.168.1.1/ambari/extend/redis-monitor.tar.gz，对应 http 服务所在的 ip 地址（http 安装在 ambari sever 上的）修改 Redis Monitor Config，修改为 /home/redis/redis.properties。稍后安装完成后，再修改这个文件的内容 具体如下： 4、确认安装配置完成后一直点击下一步，执行安装部署 5、完成安装 安装完成后，redis服务进程展示 6、修改 Redis Monitor 配置【在 Redis Monitor 所在节点的 Redis 用户下执行】由上图可以发现，Redis Monitor 没有启动成功，这是由于在配置时，我们制定了一个文件：/home/redis/redis.properties，这个文件，乃至redis这个用户，在安装redis之前，都不存在，因此在安装时，Redis Monitor 读取不到这个文件，于是就在它的目录下新建了一个默认文件，此时，我们需要修改该文件，然后，拷贝到配置指定的目录下。首先修改文件： cd &#x2F;home&#x2F;redis&#x2F;redis&#x2F;monitorvim redis.properties 文件内容如下，按照以下说明，修改每个配置： # redis master 节点列表，逗号分隔masterIpList&#x3D;172.168.1.1,172.168.1.2,172.168.1.3# redis master 端口号，如果没有修改，一般默认为 6379masterPort&#x3D;6379# redis slave 节点列表，逗号分隔slaveIpList&#x3D;172.168.1.1,172.168.1.2,172.168.1.3# redis slave 端口号，如果没有修改，一般默认为 6380slavePort&#x3D;6380# Ambari Metrics 节点 IPmetric_host&#x3D;172.168.1.1# Redis 密码，如果没有特别配置，一般默认为空password&#x3D; 修改完成后，复制该文件到指定目录： cp &#x2F;home&#x2F;redis&#x2F;monitor&#x2F;redis.properties &#x2F;home&#x2F;redis 然后，回到 Ambari 界面，单独启动 Redis Monitor 即可 7、启停点击右上角ACTIONS里的Stop，即可执行停止服务操作 点击右上角ACTIONS里的Start，启动停止后的服务","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇八：Ambari 安装 Hive","slug":"ambari-install-guides-8-install-hive-md","date":"2020-05-31T09:44:35.000Z","updated":"2020-07-25T11:14:09.041Z","comments":true,"path":"2020/05/31/ambari-install-guides-8-install-hive-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-8-install-hive-md/","excerpt":"1、准备工作 确认 Hive 版本，以及 Hive 部署规划 确认 Hive 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 Hive 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下","text":"1、准备工作 确认 Hive 版本，以及 Hive 部署规划 确认 Hive 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 Hive 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下 如果版本包未上传，则需要上传到指定的目录，并且重启 ambari server 2、安装部署 Hive点击 Add Service 选择 Hive 服务并点击 NEXT 选择 Hive Metastore、HiveServer2 和 Hive Monitor 节点： Hive Monitor和Hive Metastore安装在同一台机器上 选择 client 节点 点击 NEXT 按钮，进行配置界面，修改 Hive 配置文件 修改 DATABASE 配置项Hive Database 选择 Existing MySQL/MariaDBDatabase Name 可以自己起名，如果没有特殊要求的话，可以直接填写 hiveDatabase Username &amp; Database Password 填写之前安装的 MySQL 的业务用户名及密码，本文档此前使用的是 rhino 用户，密码为 xxxxxxxxDatabase URL 即 MySQL 数据库的 jdbc 地址： jdbc:mysql://10.0.0.32:3306/hive?createDatabaseIfNotExist=true以上未提到的配置一般不需要修改，默认即可，需要注意一下字符集要为 latin1 修改 ADVANCED 配置项下的 Advanced hive-envMetrics Collector Host Ip 对应 metric collector 进程所在节点的 ip 地址；hive_download 地址 http://172.168.1.1/ambari/extend/apache-hive-2.3.3-bin.tar.gzhive_monitor_download地址 http:// 172.168.1.1/ambari/extend/hive-monitor.tar.gzhiveserver_jmx 地址 http://172.168.1.1:10002/jmx，其中 ip 对应 hiveserver2 所在节点地址，端口取 hive.server2.webui.port 配置的端口。 上传 mysql-connector-java-5.1.40-bin.jar将此前准备的基础版本包中的 MySQL JDBC 驱动包上传到 Ambari Server 节点的 /var/lib/ambari-server/resources 目录下 [root@node1 ~]# cd &#x2F;home&#x2F;package&#x2F;base[root@node1 ~]# cp mysql-connector-java-5.1.40-bin.jar &#x2F;var&#x2F;lib&#x2F;ambari-server&#x2F;resources[root@node1 ~]# chmod 755 &#x2F;var&#x2F;lib&#x2F;ambari-server&#x2F;resources&#x2F;mysql-connector-java-5.1.40-bin.jar 3、确认安装 4、完成安装 安装完成后，hive服务进程展示 5、修改 hdfs 配置文件Hive 服务默认用户是 hive，在 hdfs 的 Custom core-site 配置项下点击 Add Property 添加两个配置：hadoop.proxyuser.hive.groups 和 hadoop.proxyuser.hive.hosts，value都配置 * 。然后重启 hdfs 和 yarn 服务。 6、Hive 启停点击右上角ACTIONS里的Stop，即可执行停止服务操作 Hive2.3.3 如果遇到进程无法启动的问题时。要手动按照此顺序启动进程：先启动 metadata 进程，然后启动 server 进程，最后启动 monitor 进程。Monitor 进程可能启动时会一直不成功，如果实在无法解决，可以忽略，monitor 进程不影响流程的运行，只起监控的作用。 点击右上角ACTIONS里的Start，启动停止后的服务","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇七：Ambari 安装 HBase","slug":"ambari-install-guides-7-install-hbase-md","date":"2020-05-31T09:43:42.000Z","updated":"2020-07-25T11:14:12.268Z","comments":true,"path":"2020/05/31/ambari-install-guides-7-install-hbase-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-7-install-hbase-md/","excerpt":"1、准备工作 确认 HBase 版本，以及 HBase 部署规划 确认 HBase 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 HBase 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下","text":"1、准备工作 确认 HBase 版本，以及 HBase 部署规划 确认 HBase 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 HBase 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下 如果版本包未上传，则需要上传到指定的目录，并且重启 ambari server 2、安装部署 HBase点击 Add Service 选择 HBase 服务并点击 NEXT 根据部署规划，添加 hbase Master 节点并点击 NEXT，可以添加多个，但如果没有特殊需求，一般添加一个即可 根据部署规划，选择 RegionServer 和 client 节点： -- 3、修改 Hbase 配置文件修改 Advanced hbase-env &gt;&gt; hbase_download 地址，http://172.168.1.1/ambari/extend/hbase-1.4.9-bin.tar.gz。（IP地址和包名称根据实际情况修改） 修改 Advanced hbase-site &gt;&gt; zookeeper znode parent 为 /hbase，否则安装完成无法使用 hbase 4、确认安装 Hbase配置完成后一直点击下一步，执行安装部署： 5、完成安装 Hbase 安装完成后，hbase服务进程展示 6、启停点击右上角ACTIONS里的Stop，即可执行停止服务操作 点击右上角ACTIONS里的Start，启动停止后的服务","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇六：Ambari 安装 ElasticSearch","slug":"ambari-install-guides-6-install-elasticsearch-md","date":"2020-05-31T09:42:37.000Z","updated":"2020-07-25T11:14:17.516Z","comments":true,"path":"2020/05/31/ambari-install-guides-6-install-elasticsearch-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-6-install-elasticsearch-md/","excerpt":"1、准备工作 确认 ES 版本，以及 ES 部署规划 确认 ES 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 ES 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下","text":"1、准备工作 确认 ES 版本，以及 ES 部署规划 确认 ES 的安装包是否已经上传到 Ambari Server 节点的 /var/www/html/ambari/extend 路径下 确认 ES 服务包是否已经上传到 Ambari Server 节点的 /var/lib/ambari-server/resources/stacks/HDP/2.6/services 路径下 如果版本包未上传，则需要上传到指定的目录，并且重启 ambari server 2、安装部署 ElasticSearch点击 Add Service 选择 Elasticsearch 服务并点击 NEXT： 点击进入后找到 Elasticsearch Master，点击后面的 + 号按钮可以选择在哪几台服务器上部署 ES，例如选择 node1,node2 两台服务器部署 ES；此时 ES Monitor 只需选择 node1 或 node2 其中一台即可（ES Monitor不要单独安装）。如下图所示： 3、修改ElasticSearch配置文件选择好需要部署的服务器后点击next进入下一个界面，该界面比较重要，主要是配置一些常用的ES参数，以及ES多实例的配置（即在一台服务器启多个ES，提高入库性能）。 安装版本时，默认会创建一个elastic用户和elastic组。这个用户就是 ES 的安装用户，绝大部分 ES 的维护，都需要在这个用户下操作 Advanced elastic-config 配置组配置修改： &lt;1&gt; cluster_name：集群名称，可以修改成自己想要的集群名称，如果没有特别需求，可以直接设置成 elasticsearch，这里需要注意，ES 的集群名称与 Hadoop 的集群名称是不同的两个名称，不能搞混！&lt;2&gt; discovery_zen_minimum_master_nodes：集群中主节点的数量，配置为1。&lt;3&gt; discovery_zen_ping_unicast_hosts: 集群中 master 节点初始列表，如两台服务器安装了 ES Master，则配置为 172.168.1.1:9300,172.168.1.2:9300&lt;4&gt; elastic_download： ES 版本下载地址，只需将默认值中的 ip 修改即可。&lt;5&gt; gateway_recover_after_nodes：设置集群中 N 个节点启动时进行数据恢复，默认为 1。&lt;6&gt; head_download： head-master 界面监控工具下载地址，只需修改 ip 即可。&lt;7&gt; headmaster_ip：将 ip 修改为刚才 ES Monitor 对应的 ip。 &lt;8&gt; http_port: HTTP 端口号，配置和实例个数有关。如果实例个数 为3，则配置9200,9201,9202 注意：中间用英文逗号隔开。 &lt;9&gt; init_heapspace：分配给堆内存的初始值，该值与 max_heapspace 值配置相同。如果该服务器有 30G 分配给 ES，每台服务器启 3 个实例，则该值配为 -Xms26g，该参数需要根据实际集群配置进行灵活调整 注意：该值最大值最好配为 -Xms26g（服务器内存足的情况下），前面的 - 符号不能少。 &lt;10&gt; max_heapspace：和 init_heapspace 值配置相同。&lt;11&gt; nodejs_download: head-master 界面监控工具依赖 nodejs，该项配置 nodejs 下载地址，修改 ip 即可。&lt;12&gt; path_data： ES 数据存储路径。现在默认所有实例中有 1 个实例只做 master，该实例不做 data，不存储数据；其余实例只做 data，不做 master。比如该服务器启了 3 个 ES 实例，该服务器总共有 12 块盘，则第一个实例为 master，不存储数据；实际存储数据的为 2 个实例，则每个实例分配 6 块盘，则配置如下：/home/elastic/esdata;/data01/esdata,/data02/esdata,/data03/esdata,/data04/esdata, /data05/esdata,/data06/esdata;/data07/esdata,/data08/esdata,/data09/esdata,/data10/esdata,/data11/esdata,/data12/esdata 注意：其中/home/elastic/esdata是分配给实例1。data01、data02、data03、data04、data05、data06是分配给实例2，中间用英文逗号分开。实例1和实例2之间的英文用分号分开。同理实例2和实例3之间也是用英文分号分开。 &lt;13&gt; path_logs： ES 启动及一些常用日志存放目录。配置和启动实例个数有关，例如需要启动 3 个实例，则配置为 /home/elastic/elasticsearch/logs0; /home/elastic/elasticsearch/logs1; /home/elastic/elasticsearch/logs2。其中实例1的日志写在 logs0 目录下，实例 2 的日志写在 logs1 目录下，实例3的日志写在 logs2 目录下。 注意：不同目录之间用英文分号分开。 &lt;14&gt; transport_tcp_port： TCP 端口号。配置和实例个数有关，如果实例个数为 3，则配置为 9300,9301,9302 注意：中间用英文逗号隔开 Advanced elastic-env配置项：&lt;1&gt; elastic_base_dir： ES 版本解压后的安装目录&lt;2&gt; elastic_log_dir： ES 安装日志目录（多实例时采用 path_logs 配置项）&lt;3&gt; elastic_pid_dir： ES 启动时 pid 目录，ambari 检测 ES 是否安装成功时会检测该 pid。&lt;4&gt; elastic_conf_dir: ES配置文件目录（暂时用不到）&lt;5&gt; instance_num：多实例个数。一台服务器启动多少个 ES 就配置多少。如果不用多实例，只启动一个 ES 就配置为 1。默认为 3。 Advanced elastic-monitor配置项：&lt;1&gt; es_host： 上报监控信息的ip地址，和ES Monitor选择的服务器ip相同即可。&lt;2&gt; esmonitor_download： es监控包的下载地址，修改ip即可。&lt;3&gt; metric_host： 填写Ambari Metrics的地址。 Metrics地址查看： 4、确认安装 ES修改完成配置项后，点击下一步，进行安装。 完成安装后，如果 ES Monitor 未启动，需要手动启动ES Monitor ES正常启动后，可以看到左侧的Elasticsearch图标显示为绿色，也可在服务器后台输入命令： curl ip:9200/_cat/nodes,能看到启动的所有ES实例。 如果遇到提示 elastic 安装不成功，错误为 mkdir 权限被 denied，则自己手动在 home 下创建一个 elastic 的文件夹，用命令：chown elastic:hadoop elastic/ 来手动更改一下权限，然后点重新安装即可。 5、启停ElasticSearch如果想停止ES服务，则可以先选择左侧的Elasticsearch，然后点击右上角ACTIONS，选择stop，点击CONFIRM STOP停止服务，如下图所示：","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇五：Ambari 安装 YARN","slug":"ambari-install-guides-5-install-yarn-md","date":"2020-05-31T09:40:24.000Z","updated":"2020-07-25T11:13:25.595Z","comments":true,"path":"2020/05/31/ambari-install-guides-5-install-yarn-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-5-install-yarn-md/","excerpt":"1、安装部署","text":"1、安装部署 点击 Add Service 选择 YARN+MapReduce 服务并下拉到页面底部点击 NEXT： 根据组件规划，选择 ResourceManager 节点，App Timeline Server 和 History Server 直接默认即可，不需要修改，点击 NEXT。 选择 NodeManager 节点 注意：NodeManager 和 client 部署方式与 DataNode 一致即可 2、修改 YARN 配置文件 &lt;1&gt; Memory allocated for all YARN containers on a node：该参数值为每个 nodemanager 节点可用内存。大小设置为单台 nodemanager 机器内存大小的80%。&lt;2&gt; Minimum Container Size (Memory)：该参数值为单个任务可申请最小内存。该值需要根据实际业务进行调整，如果不知道实际业务需求的话，建议往小了设置，根据集群总资源，设置一个比较小的参数（建议设置 2048）。该参数后期可以随时调整。&lt;3&gt; Maximum Container Size (Memory)：该参数值为单个任务可申请最大内存。该值可以设置为64G。&lt;4&gt; Number of virtual cores：该参数值设置为单台nodemanager机器cpu逻辑核数的一半。&lt;5&gt; Maximum Container Size (VCores)：该参数值必须小于或等于 Number of virtual cores 的值 以上没有提到的配置，一般默认即可，如果实际有需求，也可以进行修改查看CPU逻辑核数命令：cat /proc/cpuinfo |grep processor |wc -l &lt;6&gt; YARN NodeManager Local directories：该参数值填写 /data01/hadoop/yarn/local,/data02/hadoop/yarn/local,/data03/hadoop/yarn/local,/data04/hadoop/yarn/local,/data05/hadoop/yarn/local,/data06/hadoop/yarn/local,/data07/hadoop/yarn/local,/data08/hadoop/yarn/local,/data09/hadoop/yarn/local,/data10/hadoop/yarn/local &lt;7&gt; YARN NodeManager Log directories：该参数值填写 /data01/hadoop/yarn/log,/data02/hadoop/yarn/log,/data03/hadoop/yarn/log,/data04/hadoop/yarn/log,/data05/hadoop/yarn/log,/data06/hadoop/yarn/log,/data07/hadoop/yarn/log,/data08/hadoop/yarn/log,/data09/hadoop/yarn/log,/data10/hadoop/yarn/log 以上这两个参数根据系统中实际的磁盘数量填写。另外，原则同 HDFS 磁盘选择，遵循木桶效应，即所有节点最多能使用的数据盘数量，取决于所有节点中，数据盘最少的那个节点的数据盘数量 3、确认安装 YARN 完成安装 4、启停 5、YARN 集群高可用 5.1 Get Start在 Ambari 主界面，点击 YARN→ACTIONS→Enable ResourceManager HA 进行高可用设置 按照界面提示，点击Next即可 5.2 指定standby-resourcemanager节点根据部署方案，选择 standby-resourcemanager 管理节点。 5.3 Review默认配置是readonly，不可修改，只可查看，直接点击Next即可 5.4 Configure Components自动完成安装及配置，如下图所示，点击complete即可。 可以使用 jps 查看对应两台机器上的 resourcemanager 和 nodemanager 进程，全部实现了高可用状态。【root用户操作】 5.5 YARN 相关配置 Resource Manager配置 yarn.admin.acl = yarn,rhino resoucemanager 管理员权限用户配置，加上rhino用户，注意，这里安装文档以 rhino 用户为例，实际安装过程中，根据业务需求添加用户，可以直接配置为 *，即意味着所有用户均可以管理 ResourceManager（不建议这样配置） Advanced yarn-site 配置： yarn.client.failover-proxy-provider = org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider NodeManager 配置 该组配置安装 Yarn 时就已经修改过，只需检查一下即可 Setting 配置 Scheduler 配置 修改为 org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler 注意：修改yarn.resourcemanager.scheduler.class后，启动yarn之前要先添加fair-scheduler.xml文件，不然启动yarn会报错找不到fair-scheduler.xml，需要配置fair-scheduler.xml文件只需要拷贝到两台ResourceManager机器上 [root@node1 ~]# cd &#x2F;usr&#x2F;hdp&#x2F;2.6.3.0-235&#x2F;hadoop-yarn&#x2F;etc&#x2F;hadoop&#x2F;[root@node1 hadoop]# chown yarn:hadoop fair-scheduler.xml[root@node1 hadoop]# scp fair-scheduler.xml root@node2:&#x2F;usr&#x2F;hdp&#x2F;2.6.3.0-235&#x2F;hadoop-yarn&#x2F;etc&#x2F;hadoop&#x2F; 如果没有找到该文件，可以直接在目录下新建这个文件 fair-scheduler.xml 需要进行配置，详细配置方法点击这里 Custom yarn-site中增加配置项： yarn.scheduler.fair.allocation.file = /usr/hdp/2.6.3.0-235/hadoop-yarn/etc/hadoop/fair-scheduler.xmlhdp.version = 2.6.3.0-235yarn.nodemanager.localizer.cache.target-size-mb = 1024yarn.nodemanager.localizer.cache.cleanup.interval-ms = 60000 注意：安装ambari 2.7版本的时候，出现因为路径的有空格导致找不到文件，需要注意路径格式。 5.6 YARN 日志老化配置找到yarn-log4j的配置项： 按照以下内容进行修改 # Audit logging for ResourceManagerrm.audit.logger&#x3D;$&#123;hadoop.root.logger&#125;log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger&#x3D;$&#123;rm.audit.logger&#125;log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger&#x3D;falselog4j.appender.RMAUDIT&#x3D;org.apache.log4j.DailyRollingFileAppenderlog4j.appender.RMAUDIT.File&#x3D;$&#123;yarn.log.dir&#125;&#x2F;rm-audit.loglog4j.appender.RMAUDIT.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.RMAUDIT.layout.ConversionPattern&#x3D;%d&#123;ISO8601&#125; %p %c&#123;2&#125;: %m%nlog4j.appender.RMAUDIT.DatePattern&#x3D;.yyyy-MM-dd改为:# Audit logging for ResourceManagerrm.audit.logger&#x3D;$&#123;hadoop.root.logger&#125;log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger&#x3D;$&#123;rm.audit.logger&#125;log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger&#x3D;falselog4j.appender.RMAUDIT&#x3D;org.apache.log4j.RollingFileAppenderlog4j.appender.RMAUDIT.File&#x3D;$&#123;yarn.log.dir&#125;&#x2F;rm-audit.loglog4j.appender.RMAUDIT.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.RMAUDIT.layout.ConversionPattern&#x3D;%d&#123;ISO8601&#125; %p %c&#123;2&#125;: %m%nlog4j.appender.RMAUDIT.MaxFileSize&#x3D;128MBlog4j.appender.RMAUDIT.MaxBackupIndex&#x3D;5------------------------------- 我是分割线 -----------------------------------# Audit logging for NodeManagernm.audit.logger&#x3D;$&#123;hadoop.root.logger&#125;log4j.logger.org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger&#x3D;$&#123;nm.audit.logger&#125;log4j.additivity.org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger&#x3D;falselog4j.appender.NMAUDIT&#x3D;org.apache.log4j.DailyRollingFileAppenderlog4j.appender.NMAUDIT.File&#x3D;$&#123;yarn.log.dir&#125;&#x2F;nm-audit.loglog4j.appender.NMAUDIT.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.NMAUDIT.layout.ConversionPattern&#x3D;%d&#123;ISO8601&#125; %p %c&#123;2&#125;: %m%nlog4j.appender.NMAUDIT.DatePattern&#x3D;.yyyy-MM-dd改为:# Audit logging for NodeManagernm.audit.logger&#x3D;$&#123;hadoop.root.logger&#125;log4j.logger.org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger&#x3D;$&#123;nm.audit.logger&#125;log4j.additivity.org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger&#x3D;falselog4j.appender.NMAUDIT&#x3D;org.apache.log4j.RollingFileAppenderlog4j.appender.NMAUDIT.File&#x3D;$&#123;yarn.log.dir&#125;&#x2F;nm-audit.loglog4j.appender.NMAUDIT.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.NMAUDIT.layout.ConversionPattern&#x3D;%d&#123;ISO8601&#125; %p %c&#123;2&#125;: %m%nlog4j.appender.NMAUDIT.MaxFileSize&#x3D;128MBlog4j.appender.NMAUDIT.MaxBackupIndex&#x3D;5 5.7 MapReduced配置在 MapReduce 的 Custom mapred-site 里增加配置项：hdp.version = 2.6.3.0-235","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇二：环境准备","slug":"ambari-install-guides-2-environment-md","date":"2020-05-31T09:19:20.000Z","updated":"2020-07-25T11:12:56.248Z","comments":true,"path":"2020/05/31/ambari-install-guides-2-environment-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-2-environment-md/","excerpt":"1、基础环境准备","text":"1、基础环境准备 1.1 基础版本包的上传【在每个节点 root 用户下执行】基础版本包即一些基础的，支撑业务的服务包，本文档 ambari 安装包含了以下的必装服务，实际安装时，按照自己的实际需求做增删： apache-tomcat-8.0.35.zip # tomcat 安装包jdk1.8.0_181.tar.gz # jdk 安装包kafka-manager-1.3.3.17.zip # kafka manager 安装包keepalived-1.1.20.tar.gz # mysql 的 keepalived 安装包mysql-connector-java-5.1.40-bin.jar # mysql 的 JDBC 驱动 jar 包parted.sh # 自动挂载脚本mysql-5.6.25-linux-x86_64.tar.gz # mysql 安装包 新建版本包存放目录 [root@node1 ~]# mkdir -p &#x2F;home&#x2F;package&#x2F;base 将这些安装包上传至 /home/package/base 注意：本文档后续安装中，如果使用到这些基础包，将不再赘述将包拷贝至相应目录，在自己实际操作时，记得到该目录下拷贝服务包。 1.2 操作系统安装及数据盘格式化挂载 本节内容主要面向物理机的部署，对于使用了云平台等 PAAS 服务来部署集群的情况，可以不需要进行本节的操作 1.2.1 操作系统安装 CentOS 7x 操作系统安装 1.2.2 数据盘格式化挂载【每个节点root用户下执行】磁盘格式化可以使用脚本进行，下面提供的脚本是基于 xfs 文件格式的，通过 uuid 方式进行挂载的。脚本解析详见文档： 链接 将脚本放入/root目录下，并赋予755权限： [root@node1 ~]# chmod 755 parted.sh 执行脚本： [root@node1 ~]# sh parted.sh根据提示，执行安装：Step 1.No lock file, begin to create lock file and continue.&#x2F;parted.sh: line 201: -----------------------------------------------------MAIN---------------------------------------------: command not foundStep 2.Begin to check free diskYou have a free disk, Now will fdisk it and mount itYou have a free disk, Now will fdisk it and mount itYou have a free disk, Now will fdisk it and mount itYou have a free disk, Now will fdisk it and mount itThis system have free disk :&#x2F;dev&#x2F;sdb&#x2F;dev&#x2F;sdc&#x2F;dev&#x2F;sdd&#x2F;dev&#x2F;sdeconfirm free disk(yes&#x2F;no):yesStep 3.Begin to parted free disk &#x2F;dev&#x2F;sdb Step 3.Begin to parted free disk &#x2F;dev&#x2F;sdc Step 3.Begin to parted free disk &#x2F;dev&#x2F;sddStep 3.Begin to parted free disk &#x2F;dev&#x2F;sde Step 4.Begin to mkfs on disk &#x2F;dev&#x2F;sdb meta-data&#x3D;&#x2F;dev&#x2F;sdb1 isize&#x3D;512 agcount&#x3D;160, agsize&#x3D;163837 blks &#x3D; sectsz&#x3D;512 attr&#x3D;2, projid32bit&#x3D;1 &#x3D; crc&#x3D;1 finobt&#x3D;0, sparse&#x3D;0data &#x3D; bsize&#x3D;4096 blocks&#x3D;26213888, imaxpct&#x3D;25 &#x3D; sunit&#x3D;0 swidth&#x3D;0 blksnaming &#x3D;version 2 bsize&#x3D;4096 ascii-ci&#x3D;0 ftype&#x3D;1log &#x3D;internal log bsize&#x3D;4096 blocks&#x3D;12799, version&#x3D;2 &#x3D; sectsz&#x3D;512 sunit&#x3D;0 blks, lazy-count&#x3D;1realtime &#x3D;none extsz&#x3D;4096 blocks&#x3D;0, rtextents&#x3D;0Step 4.Begin to mkfs on disk &#x2F;dev&#x2F;sdc meta-data&#x3D;&#x2F;dev&#x2F;sdc1 isize&#x3D;512 agcount&#x3D;160, agsize&#x3D;163837 blks &#x3D; sectsz&#x3D;512 attr&#x3D;2, projid32bit&#x3D;1 &#x3D; crc&#x3D;1 finobt&#x3D;0, sparse&#x3D;0data &#x3D; bsize&#x3D;4096 blocks&#x3D;26213888, imaxpct&#x3D;25 &#x3D; sunit&#x3D;0 swidth&#x3D;0 blksnaming &#x3D;version 2 bsize&#x3D;4096 ascii-ci&#x3D;0 ftype&#x3D;1log &#x3D;internal log bsize&#x3D;4096 blocks&#x3D;12799, version&#x3D;2 &#x3D; sectsz&#x3D;512 sunit&#x3D;0 blks, lazy-count&#x3D;1realtime &#x3D;none extsz&#x3D;4096 blocks&#x3D;0, rtextents&#x3D;0Step 4.Begin to mkfs on disk &#x2F;dev&#x2F;sdd meta-data&#x3D;&#x2F;dev&#x2F;sdd1 isize&#x3D;512 agcount&#x3D;160, agsize&#x3D;163837 blks &#x3D; sectsz&#x3D;512 attr&#x3D;2, projid32bit&#x3D;1 &#x3D; crc&#x3D;1 finobt&#x3D;0, sparse&#x3D;0data &#x3D; bsize&#x3D;4096 blocks&#x3D;26213888, imaxpct&#x3D;25 &#x3D; sunit&#x3D;0 swidth&#x3D;0 blksnaming &#x3D;version 2 bsize&#x3D;4096 ascii-ci&#x3D;0 ftype&#x3D;1log &#x3D;internal log bsize&#x3D;4096 blocks&#x3D;12799, version&#x3D;2 &#x3D; sectsz&#x3D;512 sunit&#x3D;0 blks, lazy-count&#x3D;1realtime &#x3D;none extsz&#x3D;4096 blocks&#x3D;0, rtextents&#x3D;0Step 4.Begin to mkfs on disk &#x2F;dev&#x2F;sde meta-data&#x3D;&#x2F;dev&#x2F;sde1 isize&#x3D;512 agcount&#x3D;160, agsize&#x3D;163837 blks &#x3D; sectsz&#x3D;512 attr&#x3D;2, projid32bit&#x3D;1 &#x3D; crc&#x3D;1 finobt&#x3D;0, sparse&#x3D;0data &#x3D; bsize&#x3D;4096 blocks&#x3D;26213888, imaxpct&#x3D;25 &#x3D; sunit&#x3D;0 swidth&#x3D;0 blksnaming &#x3D;version 2 bsize&#x3D;4096 ascii-ci&#x3D;0 ftype&#x3D;1log &#x3D;internal log bsize&#x3D;4096 blocks&#x3D;12799, version&#x3D;2 &#x3D; sectsz&#x3D;512 sunit&#x3D;0 blks, lazy-count&#x3D;1realtime &#x3D;none extsz&#x3D;4096 blocks&#x3D;0, rtextents&#x3D;0Step 5.Begin to mount disk &#x2F;dev&#x2F;sdb &#x2F;data01 not exist! make it &#x2F;&#x2F;不需要自己创建&#x2F;data01,脚本自动创建Step 5.Begin to mount disk &#x2F;dev&#x2F;sdc &#x2F;data02 not exist! make it Step 5.Begin to mount disk &#x2F;dev&#x2F;sdd &#x2F;data03 not exist! make it Step 5.Begin to mount disk &#x2F;dev&#x2F;sde &#x2F;data04 not exist! make it 查看挂载情况： [root@node1 ~]# df -h 获取每块磁盘的uuid [root@node1 ~]# lsblk -f 修改挂载脚本中的uuid [root@node1 ~]# vim &#x2F;root&#x2F;.mount.sh 添加开机自动挂载 [root@node1 ~]# chmod +x &#x2F;etc&#x2F;rc.d&#x2F;rc.local[root@node1 ~]# echo sh &#x2F;root&#x2F;.mount.sh &gt;&gt; &#x2F;etc&#x2F;rc.d&#x2F;rc.local 1.2.3 修改数据目录权限【每个节点 root 用户下执行】需要给所有节点的数据盘目录赋 777 权限，否则在集群安装时，会出现错误 chmod -R 777 &#x2F;data* 1.3 操作系统参数设置【每个节点root用户下执行】1.3.1 修改/etc/security/limits.conf文件[root@node1]# vim &#x2F;etc&#x2F;security&#x2F;limits.conf在文件最下方添加如下内容：* soft nofile 200000* hard nofile 200000* soft nproc 131072* hard nproc 131072* soft core unlimited* - memlock unlimited* - as unlimited* - data unlimited* - fsize unlimited* - rss unlimited 1.3.2 修改/etc/sysctl.conf文件修改以下配置参数 vm.max_map_count&#x3D;262144 该配置修改完需要重启操作系统，但目前不重启，后续基础配置完成后，文档会指导进行重启操作。 可通过 [root@node1]# sysctl -w vm.max_map_count=262144 使得该配置临时生效。 1.3.3 关闭监控程序的服务端和客户端[root@node1]# systemctl start tuned[root@node1]# tuned-adm off[root@node1]# tuned-adm list[root@node1]# systemctl stop tuned[root@node1]# systemctl disable tuned 1.3.4系统自启动文件修改[root@node1]# echo &quot;echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled&quot; &gt;&gt; &#x2F;etc&#x2F;rc.local 1.4 禁用 SELinux 和 Firewall【每个节点 root 用户下执行】查看 SELinux 状态 getenforce 如果状态为Disabled，说明 selinux 已经被禁用；状态为permissive，说明 selinux 为临时关闭；状态为enforcing，说明 selinux 处于启动状态。 永久修改 SELinux 状态 [root@node1]# vim &#x2F;etc&#x2F;selinux&#x2F;config修改如下配置：SELINUX&#x3D;disabled 本步骤需要重启操作系统才能生效，但此时不需要重启，等其他基础参数修改完，文档会指导进行重启操作。可通过 [root@node1]# setenforce 0 使得该配置临时生效。 禁用 Firewall 和网络管理工具 [root@node1]# systemctl disable firewalld[root@node1]# systemctl stop firewalld[root@node1]# systemctl statue firewalld返回结果如下即表示执行成功：● firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;firewalld.service; disabled; vendor preset: enabled) Active: inactive (dead) Docs: man:firewalld(1)[root@node1]# systemctl disable NetworkManager.service[root@node1]# systemctl stop NetworkManager.service 1.5 配置主机名及 hosts【每个节点 root 用户下执行】配置主机名（以下命令只适用于 CentOS7 及以上版本） [root@node1]# hostnamectl set-hostname node1 选择一台节点，修改 /etc/hosts 文件，先清空文件内容，然后添加： 添加所有节点的IP和主机名对应关系示例：127.0.0.1 localhost172.168.1.1 node1172.168.1.2 node2172.168.1.3 node3 注意：127.0.0.1 localhost 必须放在第一行 复制该文件到其他所有节点 示例：[root@node1]# scp &#x2F;etc&#x2F;hosts root@172.168.1.2:&#x2F;etc[root@node1]# scp &#x2F;etc&#x2F;hosts root@172.168.1.3:&#x2F;etc 1.6 修改ssh参数配置【每个节点 root 用户下执行】[root@node1]# vim &#x2F;etc&#x2F;ssh&#x2F;ssh_config修改配置参数：StrickHostKeyChecking no 1.7 重启操作系统【每个节点 root 用户下执行】[root@node1]# reboot 1.8 配置本地源【在 Ambari Server 所在节点的 root 用户下执行】检查操作系统版本，务必和即将挂载的镜像版本一致，否则会报错： [root@node1]# lsb_release -aDescription: CentOS Linux release 7.5.1804 (Core) Release: 7.5.1804Codename: Core 检查 8080 端口是否被占用，Ambari需要使用 8080 端口： [root@node1]# netstat -an |grep 8080如果没有任何返回，即表示端口没有被占用 新建目录，用于存放系统镜像： [root@node1]# mkdir &#x2F;centos7.5 上传系统镜像至新建目录 新建挂载目录： [root@node1]# mkdir -p &#x2F;var&#x2F;www&#x2F;html&#x2F;mnt 挂载镜像： [root@node1]# mount -o loop &#x2F;centos7.5&#x2F;CentOS-7-x86_64-Everything-1804.iso &#x2F;var&#x2F;www&#x2F;html&#x2F;mnt 添加开机自动挂载： [root@node1]# vim &#x2F;etc&#x2F;rc.d&#x2F;rc.local添加如下内容，保存：mount -o loop &#x2F;7.5&#x2F;CentOS-7-x86_64-Everything-1804.iso &#x2F;var&#x2F;www&#x2F;html&#x2F;mnt[root@node1]# chmod +x &#x2F;etc&#x2F;rc.d&#x2F;rc.local 保留cdrom.repo，删除其它源文件 [root@node1]# cd &#x2F;etc&#x2F;yum.repos.d&#x2F;[root@node1]# tar zcvf repo.tar.gz .&#x2F;*[root@node1]# rm -rf CentOS* 修改yum源（如果没有直接新建） [root@node1]# vim &#x2F;etc&#x2F;yum.repos.d&#x2F;cdrom.repo添加&#x2F;修改如下内容：[cdrom]name&#x3D;cdrombaseurl&#x3D;file:&#x2F;&#x2F;&#x2F;var&#x2F;www&#x2F;html&#x2F;mntenabled&#x3D;1gpgcheck&#x3D;0 清空原先的 yum 缓存 [root@node1]# yum clean all出现以下内容：Loaded plugins: fastestmirror, langpacksCleaning repos: cdromCleaning up everythingMaybe you want: rm -rf &#x2F;var&#x2F;cache&#x2F;yum, to also free up space taken by orphaned data from disabled or removed reposCleaning up list of fastest mirrors[root@node1]# yum repolist出现以下内容：Loaded plugins: fastestmirror, langpacksDetermining fastest mirrorscdrom | 3.6 kB 00:00:00 (1&#x2F;2): cdrom&#x2F;group_gz | 166 kB 00:00:00 (2&#x2F;2): cdrom&#x2F;primary_db | 5.9 MB 00:00:00 repo id repo name statuscdrom cdrom 9,911repolist: 9,911 1.9 安装 httpd 服务【在 Ambari Server 所在节点的 root 用户下执行】检查 httpd 是否已安装 yum info httpd显示以下内容：Installed PackagesName : httpdArch : x86_64Version : 2.4.6Release : 31.el7.centosSize : 9.4 MRepo : installedFrom repo : anacondaSummary : Apache HTTP ServerURL : http:&#x2F;&#x2F;httpd.apache.org&#x2F;License : ASL 2.0Description : The Apache HTTP Server is a powerful, efficient, and extensible: web server. 如果显示：No matching Packages to list 等信息，则表示没有安装 安装服务（如果已经安装，则跳过该步） [root@node1]# yum install -y httpd 启动 httpd 服务，被设置开机自启动 [root@node1]# systemctl start httpd[root@node1]# systemctl enable httpd[root@node1]# systemctl status httpd 1.10 安装 wget 服务【在所有部署 Ambari Agent 节点的 root 用户下执行】保留cdrom.repo，删除其它源文件 [root@node2]# cd &#x2F;etc&#x2F;yum.repos.d&#x2F;[root@node2]# tar zcvf repo.tar.gz .&#x2F;*[root@node2]# rm -rf CentOS* 修改 yum 源 [root@node2]# vim cdrom.repo[cdrom]name&#x3D;cdrombaseurl&#x3D;http:&#x2F;&#x2F;10.0.0.1&#x2F;mnt&#x2F; &#x2F;&#x2F; 10.0.0.1 为 ambari server 节点的 IPenabled&#x3D;1gpgcheck&#x3D;0 清空原先的yum缓存 [root@node2]# yum clean allLoaded plugins: fastestmirror, langpacksCleaning repos: cdromCleaning up everythingMaybe you want: rm -rf &#x2F;var&#x2F;cache&#x2F;yum, to also free up space taken by orphaned data from disabled or removed reposCleaning up list of fastest mirrors[root@node2]# yum repolistLoaded plugins: fastestmirror, langpacksDetermining fastest mirrorscdrom | 3.6 kB 00:00:00 (1&#x2F;2): cdrom&#x2F;group_gz | 166 kB 00:00:00 (2&#x2F;2): cdrom&#x2F;primary_db | 5.9 MB 00:00:00 repo id repo name statuscdrom cdrom 9,911repolist: 9,911 在ambari agent的所有服务器上安装wget服务 [root@node2]# yum install -y wget 1.11 统一服务器时区【所有节点的 root 用户下执行】注意点：此项为配置时间同步使用，确保每台机器在同一个时区，本手册统一时区在Asia/Shanghai，通过date命令查看。 [root@rhino141 ~]# dateFri Mar 1 09:03:55 CST 2019 出现&quot;CST&quot;字样说明时区是在“Asia/Shanghai”，不需要执行修改时区的操作。如果显示不是的话，则按下面步骤设置“Asia/Shanghai”时区。 方法一：通过底层文件修改时区（推荐） cp &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai &#x2F;etc&#x2F;localtime 方法二：通过timedatectl修改时区（不推荐）列出所有时区 timedatectl list-timezones 将硬件时钟调整为与本地时钟一致, 0 为设置为 UTC 时间 timedatectl set-local-rtc 1 设置系统时区为上海 timedatectl set-timezone Asia&#x2F;Shanghai 1.12 安装 NTP 服务端【在 ambari server 节点的 root 用户下执行】检查 ntp 服务是否已经安装（一般是已经安装好的） [root@node1]# rpm -q ntpntp-4.2.6p5-29.el7.centos.x86_64 如果没有安装则执行安装 [root@node1]# yum install ntp ntpdate –y 修改配置文件 [root@node1]# vim &#x2F;etc&#x2F;ntp.conf在 #Hosts on local network are less restricted. 下面添加如下语句：restrict 172.168.1.0 mask 255.255.255.0 nomodifynotrap&gt; 172.168.1.0 表示：只允许 172.168.1.x 网段内的机器使用本机的服务端同步时间在 #manycastclient 239.255.254.254 autokey # manycast client 下面添加如下语句：server 127.127.1.0fudge 127.127.1.0 stratum 10&gt; 本段配置表示：外界同步源无法联系时，使用本地时间为同步服务。注释如下语句，并新增本机为同步源：#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver 172.168.1.1 启动服务，配置开机自启动 [root@node2]# systemctl start ntpd [root@node2]# systemctl enable ntpd[root@node2]# systemctl status ntpd 1.13 安装 NTP 客户端【在除 ambari server 节点以外的其他所有节点的 root 用户下执行】检查 ntp 服务是否已经安装（一般是已经安装好的） [root@node1]# rpm -q ntpntp-4.2.6p5-29.el7.centos.x86_64 如果没有安装则执行安装 [root@node1]# yum install ntp ntpdate –y 修改配置文件： [root@node2]# vim &#x2F;etc&#x2F;ntp.conf注释如下语句，并新增 ntp 服务端为同步源：#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver 172.168.1.1&gt; 172.168.1.1 即上一步配置的 ntp 服务端的节点 IP 启动服务，配置开机自启动 [root@node2]# systemctl start ntpd [root@node2]# systemctl enable ntpd[root@node2]# systemctl status ntpd 在客户端查看同步状态 [root@node2]# ntpq -p remote refid st t when poll reach delay offset jitter&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;*node1 LOCAL(0) 11 u 29 128 377 0.187 -0.009 0.009 LOCAL(0) .LOCL. 10 l 112m 64 0 0.000 0.000 0.000 1.14 配置root用户无密码访问【所有节点 root 用户执行】以root用户登录所有服务器，修改配置文件ssh_config，关闭首次无密码访问询问： [root@node1]# vim &#x2F;etc&#x2F;ssh&#x2F;ssh_configStrictHostKeyChecking no 设置各台服务器的root用户无密码访问，执行如下命令，出现提示后按回车: [root@node1]# ssh-keygen -t rsa 执行如下命令，设置各台机器的无密码访问，注意每台机器都要执行 [root@node1]# ssh-copy-id root@172.168.1.1[root@node1]# ssh-copy-id root@172.168.1.2[root@node1]# ssh-copy-id root@172.168.1.3 使用ssh命令，测试无密码访问是否成功 示例：[root@node1]# ssh root@172.168.1.2如果没有任何提示，直接登录到该节点上，即表示无密码访问成功 1.15 配置rhino用户无密码访问【所有节点 rhino 用户执行】新建rhino用户并为它设定密码，root用户操作（如果已经新建了rhino，则可以跳过本步骤） [root@node1]# useradd -g root -md &#x2F;home&#x2F;rhino rhino[root@node1]# passwd rhinoChanging password for user rhino.New password: xxxxxxxxBAD PASSWORD: The password is shorter than 8 charactersRetype new password: xxxxxxxxpasswd: all authentication tokens updated successfully. 以rhino用户登录所有服务器，输入命令：ssh-keygen -t rsa生成public key(id_rsa.pub)和private key(id_rsa)文件。当出现”Enter Passphrase”的提示时直接按回车 [rhino@node1 ~]# ssh-keygen -t rsa 执行如下命令，设置各台机器的无密码访问，注意每台机器都要执行 [rhino@node1 ~]# ssh-copy-id root@172.168.1.1[rhino@node1 ~]# ssh-copy-id root@172.168.1.2[rhino@node1 ~]# ssh-copy-id root@172.168.1.3 使用ssh命令，测试无密码访问是否成功 示例：[rhino@node1 ~]# ssh rhino@172.168.1.2如果没有任何提示，直接登录到该节点上，即表示无密码访问成功 1.2 安装 JDK 1.2.1 JDK安装及环境配置【选择任一节点的 root 用户下执行】将安装包 jdk1.8.0_181.tar.gz 上传至 /home/rhino 目录下 解压 jdk1.8.0_181.tar.gz [rhino@node1 ~]# cd &#x2F;home&#x2F;rhino[rhino@node1 ~]# tar -zxvf jdk1.8.0_181.tar.gz 配置 JAVA_HOME 环境变量 [rhino@node1 ~]# vim ~&#x2F;.bashrc 添加如下内容：export JAVA_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181 export PATH&#x3D;$JAVA_HOME&#x2F;bin:$PATH 拷贝 JDK 及环境变量到其他节点 [rhino@node1 ~]# scp -r ~&#x2F;jdk1.8.0_181 rhino@node2:~[rhino@node1 ~]# scp -r ~&#x2F;jdk1.8.0_181 rhino@node3:~[rhino@node1 ~]# scp ~&#x2F;.bashrc rhino@node2:~[rhino@node1 ~]# scp ~&#x2F;.bashrc rhino@node3:~ 使每台机器环境变量生效（每台节点都需要执行） [rhino@node1 ~]# source ~&#x2F;.bashrc 1.2.2 环境配置及修改权限【选择任一节点的 root 用户下执行】root用户下的 ~/.bashrc 文件需要配置JDK相关的环境变量 [root@node1 ~]# vim ~&#x2F;.bashrc 添加如下内容：export JAVA_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;binexport LD_LIBRARY_PATH&#x3D;$LD_LIBRARY_PATH:$JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;amd64&#x2F;server&#x2F; root用户下的 /etc/profile 文件添加 [root@node1 ~]# vim &#x2F;etc&#x2F;profile 添加如下内容：export JAVA_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181export JRE_HOME&#x3D;&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181&#x2F;jreexport CLASSPATH&#x3D;.:&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181&#x2F;lib:&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181&#x2F;jre&#x2F;libexport PATH&#x3D;&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181&#x2F;bin:&#x2F;bin:&#x2F;usr&#x2F;lib64&#x2F;qt-3.3&#x2F;bin:&#x2F;root&#x2F;perl5&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;bin&#x2F;:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;root&#x2F;bin 拷贝JDK及环境变量到其他节点 [root@node1 ~]# scp &#x2F;etc&#x2F;profile root@node2:&#x2F;etc[root@node1 ~]# scp &#x2F;etc&#x2F;profile root@node3:&#x2F;etc[root@node1 ~]# scp ~&#x2F;.bashrc root@node2:~[root@node1 ~]# scp ~&#x2F;.bashrc root@node3:~ 使每台节点环境变量生效（每台节点都需要执行） [root@node1 ~]$ source ~&#x2F;.bashrc[root@node1 ~]$ source &#x2F;etc&#x2F;profile 为 /home/ 目录下的 rhino 目录加 755 权限, 而不是 777，否则无密码访问不正常（每台节点都需要执行） [root@node1 ~]# cd &#x2F;home&#x2F;[root@node1 home]# chmod 755 rhino 为 rhino/jdk1.8.0_181/bin/java 加 777 权限（每台节点都需要执行） [root@node1 ~]# cd &#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181&#x2F;bin[root@node1 bin]# chmod 777 java 1.3 MySQL数据库安装 为了方便部署，这里将 Hive元数据库/ambari/业务 所使用的 MySQL 集中在一个 MySQL 上，使用 keepalive 和 mysql 主备保证 MySQL 数据库的高可用和数据安全。 1.3.1 节点规划及端口检查 节点规划：建议在 Ambari Server 机器上安装 MySQL 主节点，选择另一台机器安装 MySQL 备节点；端口默认 3306。本文档假设在：node1和node2两台节点上部署 检查 3306 端口是否被占用： [root@node1 ~]# netstat -anp|grep 3306 若3306端口被占用，则使用3307端口，以此类推；服务端口被占用的情况下，端口循序向上叠加+1。另外还需要规划 MySQL 的密码，主要配置 root 和 rhino 两个数据库用户的密码，这里需要注意，数据库用户：root和rhino，与操作系统用户：root和rhino是不一样的概念，不要搞混。 1.3.2 安装部署Mysql双主备【在规划安装 MySQL 的两台节点的 rhino 用户下操作】将mysql-5.6.25-linux-x86_64.tar.gz安装包上传到 node1 机器的/home/rhino目录下解压，拷贝至 node2 节点 [rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino[rhino@node1 ~]$ tar -zxvf mysql-5.6.25-linux-x86_64.tar.gz[rhino@node1 ~]$ scp -r ~&#x2F;mysql-5.6.25-linux-x86_64 rhino@node2:~ 新建数据目录 [rhino@node1 ~]$ mkdir -p &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;lock&#x2F;subsys[rhino@node1 ~]$ mkdir -p &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp 修改配置文件 [rhino@node1 ~]$ vim &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;my.cnf 配置文件如下，按照说明，进行修改： [client]port&#x3D;3306 # mysql端口号，填入规划的端口号即可default-character-set&#x3D;utf8 # 字符集，默认为utf8即可[mysqld]join_buffer_size&#x3D;16Msort_buffer_size&#x3D;16Mread_rnd_buffer_size&#x3D;16Mmax_allowed_packet&#x3D;200M # 该配置按照此处的修改max_connections&#x3D;1000 # 该配置按照此处的修改port&#x3D;3306 # mysql端口号，填入规划的端口号即可socket&#x3D; &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sockcharacter-set-server&#x3D;utf8 # 字符集，默认为utf8即可basedir&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64 # 基础目录，为mysql安装目录，需要确认实际目录路径datadir&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata # 数据目录，一般在mysql安装目录下的mydata目录，需要确认实际目录路径explicit_defaults_for_timestamp&#x3D;truebinlog_format&#x3D;MIXEDlog-bin&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;master-bin # 需要确认目录路径server_id&#x3D;131 #两台主备mysql id 需要不一样,在一个网段内如果有其他mysql做主备，那么它们的id值都不能相同。auto_increment_increment&#x3D;1auto_increment_offset&#x3D;1#binlog-do-db&#x3D;test#binlog-ignore-db&#x3D;mysql#replicate-do-db&#x3D;test#replicate-ignore-db&#x3D;mysqllog-slave-updatesslave-skip-errors&#x3D;allsql_mode&#x3D;NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES 修改mysql.server配置文件，在配置项basedir 、datadir、lockdir第一次出现的位置修改 [rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;support-files[rhino@node1 ~]$ vim mysql.server参照如下内容修改，实际路径需要根据实际情况确认：basedir&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;datadir&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydatalockdir&#x3D;&#39;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;lock&#x2F;subsys&#39; 1.3.3 初始化及启动Mysql数据库【两台 MySQL 节点的 rhino 用户下执行】为两台机器分别用mysql用户初始化数据库 [rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64[rhino@node1 ~]$ scripts&#x2F;mysql_install_db --defaults-file&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;my.cnf --datadir&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata --basedir&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64; 为两台机器上启动mysql [rhino@node1 ~]$ nohup bin&#x2F;mysqld_safe --defaults-file&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;my.cnf --user&#x3D;rhino &amp; 为两台机器上设置root密码 [rhino@node1 ~]$ bin&#x2F;mysqladmin --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock -P3306 -uroot password输入root用户密码：xxxxxxxx 进入mysql shell，验证mysql是否启动成功。 [rhino@node1 ~]$ cd mysql-5.6.25-linux-x86_64&#x2F;[rhino@node1 ~]$ bin&#x2F;mysql --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock -P3306 -uroot -p输入上一步修改完成后的root用户的密码。 停止mysql命令(如果需要停止mysql时执行，不需要停止可以跳过本步骤) [rhino@node1 ~]$ bin&#x2F;mysqladmin --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock -P3306 -uroot -p shutdown 1.3.4 登录Mysql设置双主备模式【两台 MySQL 节点的 rhino 用户下执行】登录 node1 节点的 MySQL [rhino@node1 ~] cd mysql-5.6.25-linux-x86_64[rhino@node1 ~]$ bin&#x2F;mysql --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock -P3306 -uroot -pxxxxxxxx 执行如下语句 mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &#39;backup&#39;@&#39;172.168.1.2&#39; IDENTIFIED BY &#39;backup&#39;;mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &#39;root&#39;@&#39;*&#39; IDENTIFIED BY &#39;xxxxxxxx&#39;;mysql&gt; FLUSH PRIVILEGES;mysql&gt; show master status;&gt; 172.168.1.2 即另外一台 mysql 节点的 IP 执行完毕后，会有以下的回显，后面的步骤需要使用到这个回显里的内容： 登录 node2 的 MySQL [rhino@node2 ~] cd mysql-5.6.25-linux-x86_64[rhino@node2 ~]$ bin&#x2F;mysql --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock -P3306 -uroot -pxxxxxxxx 执行如下语句 mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &#39;backup&#39;@&#39;172.168.1.1&#39; IDENTIFIED BY &#39;backup&#39;;mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &#39;root&#39;@&#39;*&#39; IDENTIFIED BY &#39;xxxxxxxx&#39;;mysql&gt; FLUSH PRIVILEGES;mysql&gt; show master status;&gt; 172.168.1.1 即另外一台 mysql 节点的 IP 执行完毕后，会有以下的回显，后面的步骤需要使用到这个回显里的内容： 为 node1 设置主备模式，在 node1 节点的 mysql 内执行： mysql&gt;CHANGE MASTER TO MASTER_HOST&#x3D;&#39;172.168.1.2&#39;, MASTER_USER&#x3D;&#39;backup&#39;,MASTER_PASSWORD&#x3D;&#39;backup&#39;, MASTER_LOG_FILE&#x3D;&#39;master-bin.000005&#39;,MASTER_LOG_POS&#x3D;772;&gt; 此处 MASTER_LOG_FILE 和 MASTER_LOG_POS 的值都是从上面得到的 node2 的状态表格中获取。mysql&gt; start slave; 为 node2 设置主备模式，在 node2 节点的 mysql 内执行： mysql&gt;CHANGE MASTER TO MASTER_HOST&#x3D;&#39;172.168.1.1&#39;, MASTER_USER&#x3D;&#39;backup&#39;,MASTER_PASSWORD&#x3D;&#39;backup&#39;, MASTER_LOG_FILE&#x3D;&#39;master-bin.000004&#39;,MASTER_LOG_POS&#x3D;625;&gt; 此处 MASTER_LOG_FILE 和 MASTER_LOG_POS 的值都是从上面得到的 node1 的状态表格中获取。mysql&gt; start slave; 1.3.5 验证Mysql双主备模式【两台节点 MySQL 上执行】在 node1 上创建数据库及表 mysql&gt; create database mysqltest;use mysqltest;mysql&gt; create table testtab(id int);insert into testtab values (1), (2); 在 node2 上查看数据库及表是否已经同步 mysql&gt; show databases;use mysqltest;mysql&gt; show tables;select * from testtab; 在 node2 上删除mysqltest数据库 mysql&gt; drop database mysqltest; 查看 node1 上mysqltest数据库是否也已经删除成功 mysql&gt; show databases; 1.3.6 为Mysql添加rhino用户【主 MySQL 节点 rhino 用户下执行】登陆rhino213的mysql并为其创建rhino用户、登录密码及设置权限 mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &#39;rhino&#39;@&#39;%&#39; IDENTIFIED BY &#39;xxxxxxxx&#39; WITH GRANT OPTION;mysql&gt;GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;xxxxxxxx&#39; WITH GRANT OPTION;mysql&gt; delete from mysql.user where user&#x3D;&#39;&#39;;mysql&gt; flush privileges; 验证能否用新建的rhino用户登陆 [rhino@node1 ~]$&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;bin&#x2F;mysql --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock -urhino -pxxxxxxxx 1.3.7 创建hive_rhino数据库【主 MySQL 节点 rhino 用户下执行】使用rhino用户登录mysql [rhino@node1 ~]$&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;bin&#x2F;mysql --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock -urhino -pxxxxxxxx 创建hive_rhino数据库，字符集为latin1 mysql&gt; create database hive_rhino CHARACTER SET latin1; 1.3.8 添加Mysql开机自启【在 MySQL 主节点 root 用户下执行】在rc.local配置文件中增加配置 [root@node1 ~]# vim &#x2F;etc&#x2F;rc.d&#x2F;rc.local 在文件中添加以下内容 su - rhino -c &quot;cd &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64; bin&#x2F;mysqld_safe --defaults-file&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;my.cnf --user&#x3D;rhino &amp;&quot; 赋予可执行权限 [root@node1 ~]# chmod +x &#x2F;etc&#x2F;rc.d&#x2F;rc.local 1.4 部署 Keepalived（MySQL双机切换）元数据mysql需要做双机切换，从而当主 mysql机器宕机后可以立即切换到备份的 mysql 上。 该章节以元数据 mysql 为例介绍了如何通过 keepalived 进程进行双机切换。最终 mysql 客户端连接的是虚拟出来的 ip 地址。但是需要注意，这个虚拟出来的 IP 地址也必须在整个网络环境中，未被使用。 节点规划：在 ambari server 机器上安装 keepalived (master)，选择另一台机器安装备 keepalived (slave)。虚拟后的ip地址 172.168.1.4。 keepalived的安装机器和mysql的安装机器保持一致。 1.4.1 安装部署 MySQL 双机切换【在 MySQL 所在两个节点的 root 用户下执行】 根据上面 MySQL 的部署情况，MySQL 分别部署在 node1 和 node2 两个节点 将 keepalived-1.1.20.tar.gz 安装包上传到 node1 和 node2 的 /home/rhino 目录下 分别在 node1 和 node2 上执行如下操作 [root@node1 ~]# cd &#x2F;root[root@node1 ~]# tar -zxvf keepalived-1.1.20.tar.gz[root@node1 ~]# cd keepalived-1.1.20[root@node1 ~]# chmod 700 sbin&#x2F;keepalived[root@node1 ~]# chmod 755 etc&#x2F;rc.d&#x2F;init.d&#x2F;keepalived[root@node1 ~]# chmod 755 etc&#x2F;sysconfig&#x2F;keepalived[root@node1 ~]# chmod 644 etc&#x2F;keepalived&#x2F;keepalived.conf[root@node1 ~]# chmod 644 etc&#x2F;keepalived&#x2F;samples&#x2F;*[root@node1 ~]# chmod 644 share&#x2F;man&#x2F;man5[root@node1 ~]# chmod 644 share&#x2F;man&#x2F;man8[root@node1 ~]# chmod 755 bin&#x2F;genhash[root@node1 ~]# chmod 644 share&#x2F;man&#x2F;man1[root@node1 ~]# cp etc&#x2F;rc.d&#x2F;init.d&#x2F;keepalived &#x2F;etc&#x2F;init.d&#x2F;keepalived[root@node1 ~]# cp sbin&#x2F;keepalived &#x2F;usr&#x2F;sbin&#x2F;[root@node1 ~]# cp etc&#x2F;sysconfig&#x2F;keepalived &#x2F;etc&#x2F;sysconfig&#x2F;[root@node1 ~]# mkdir -p &#x2F;etc&#x2F;keepalived&#x2F;[root@node1 ~]# cp &#x2F;root&#x2F;keepalived-1.1.20&#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf &#x2F;etc&#x2F;keepalived 为两台节点的 keepalived 进程添加开机自启动 [root@node1 ~]# systemctl enable keepalived.service[root@node1 ~]# systemctl start keepalived.service 1.4.2 配置Mysql双机切换【在 MySQL 所在两个节点的 root 用户下执行】 配置主节点 node1 [root@node1 ~]# vim &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf 参照以下内容配置 #! Configuration File for keepalived#check_all puts all check in one process: including check jboss, vip, mysql, timeout, statusvrrp_script chk_all &#123;script &quot;&#x2F;etc&#x2F;keepalived&#x2F;check_mysql.sh&quot; # connects and exitsinterval 2 # check every secondweight -2 # default prio: -2 if connect fails&#125;global_defs &#123;router_id CM-HA&#125;vrrp_instance VI_1 &#123;# state MASTERstate BACKUPnopreempt# 通过ifconfig查看可以绑定的网卡名，是eth0还是eth1要根据实际来选择相应的网卡名称，下面同理。interface eth1# 同一网段内如果有另外一组keepalived进程，那么virtual_router_id值不能相同；同一组keepalived进程virtual_router_id相同。virtual_router_id 13priority 100advert_int 1authentication &#123;auth_type PASSauth_pass 8888&#125;virtual_ipaddress &#123;172.168.1.4&#x2F;24 dev eth1 brd 172.168.1.255 label eth1:1&#125;track_interface &#123;eth1&#125;# notify_master &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh master&quot;# notify_backup &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh slave&quot;# notify_fault &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh fault&quot;# notify_stop &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh stop&quot;track_script &#123;chk_all&#125;&#125; 此处的 172.168.1.4 为虚拟的 IP 地址。172.168.1.255 为网关地址，按照实际配置。 配置备节点 node2 [root@rhino214 ~]# vim &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf 参照以下内容配置 #! Configuration File for keepalived#check_all puts all check in one process: including check jboss, vip, mysql, timeout, statusvrrp_script chk_all &#123;script &quot;&#x2F;etc&#x2F;keepalived&#x2F;check_mysql.sh&quot; # connects and exitsinterval 2 # check every secondweight -2 # default prio: -2 if connect fails&#125;global_defs &#123;router_id CM-HA&#125;vrrp_instance VI_1 &#123;# state MASTERstate BACKUPnopreempt# 通过ifconfig查看绑定的网卡名interface eth1# 同一网段内如果有另外一组keepalived进程，那么virtual_router_id值不能相同；同一组keepalived进程virtual_router_id相同。virtual_router_id 13priority 90advert_int 1authentication &#123;auth_type PASSauth_pass 8888&#125;virtual_ipaddress &#123;172.168.1.4&#x2F;24 dev eth1 brd 172.168.1.255 label eth1:1&#125;track_interface &#123;eth1&#125;# notify_master &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh master&quot;# notify_backup &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh slave&quot;# notify_fault &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh fault&quot;# notify_stop &quot;&#x2F;etc&#x2F;keepalived&#x2F;notify.sh stop&quot;track_script &#123;chk_all&#125;&#125; 此处的 172.168.1.4 为虚拟的 IP 地址。172.168.1.255 为网关地址，按照实际配置。 配置检测脚本 分别为主备服务器配置检测脚本。 （两台机器都要操作）若需要检测haproxy，需要添加其中注释的部分，并按照注释修改。 下面以 node1 节点配置作为示例： [root@node1 ~]#vim &#x2F;etc&#x2F;keepalived&#x2F;check_mysql.sh 参照以下内容配置 #!&#x2F;bin&#x2F;bashMYSQL&#x3D;&quot;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;bin&#x2F;mysql --socket&#x3D;&#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64&#x2F;mydata&#x2F;tmp&#x2F;mysql.sock&quot;MYSQL_HOST&#x3D;localhostMYSQL_USER&#x3D;root # mysql用户名MYSQL_PASSWORD&#x3D;root@34#$ # mysql密码CHECK_TIME&#x3D;2#mysql is working MYSQL_OK is 1 , mysql down MYSQL_OK is 0MYSQL_OK&#x3D;1#检测haproxy时定义haproxy状态# HAPROXY_OK&#x3D;1function check_mysql_helth ()&#123;$MYSQL -h $MYSQL_HOST -u $MYSQL_USER -p$&#123;MYSQL_PASSWORD&#125; -e &quot;show status;&quot;&gt;&#x2F;dev&#x2F;null 2&gt;&amp;1if [ $? -eq 0 ] ;thenMYSQL_OK&#x3D;1elseMYSQL_OK&#x3D;0fireturn$MYSQL_OK&#125;#检测haproxy状态的函数# function check_haproxy_helth ()&#123;# haproxyprocessnum&#x3D;ps -ef | grep haproxy | grep -v grep | awk &#39;&#123;print $2&#125;&#39; | wc -l# if [ $haproxyprocessnum -eq 0 ] ;then# HAPROXY_OK&#x3D;0# else# HAPROXY_OK&#x3D;1# fi# return $HAPROXY_OK#&#125;while [ $CHECK_TIME -ne 0 ]dolet &quot;CHECK_TIME -&#x3D; 1&quot;check_mysql_helthif [ $MYSQL_OK -eq 1 ] ; thenCHECK_TIME&#x3D;0exit 0fiif [ $MYSQL_OK -eq 0 ] &amp;&amp; [ $CHECK_TIME -eq 0 ]then&#x2F;etc&#x2F;init.d&#x2F;keepalived stopsleep 1#此处可以添加需要拉起的进程，根据实际需求来定&#x2F;etc&#x2F;init.d&#x2F;keepalived startexit 1fisleep 1done# 检测haproxy时上面的while…done部分替换为以下注释内# while [ $CHECK_TIME -ne 0 ]# do# let &quot;CHECK_TIME -&#x3D; 1&quot;# check_mysql_helth# check_haproxy_helth# if [ $MYSQL_OK -eq 1 ] &amp;&amp; [ $HAPROXY_OK -eq 1 ]; then# CHECK_TIME&#x3D;0# exit 0# fi # if [ $MYSQL_OK -eq 0 ] || [ $HAPROXY_OK -eq 0 ] &amp;&amp; [ $CHECK_TIME -eq 0 ] # then# &#x2F;etc&#x2F;init.d&#x2F;keepalived stop# sleep 1# &#x2F;etc&#x2F;init.d&#x2F;keepalived start# exit 1# fi# sleep 1# done 为主备服务器检测脚本添加执行权限 （两台机器都要操作）[root@node1 ~]# chmod +x &#x2F;etc&#x2F;keepalived&#x2F;check_mysql.sh [root@rhino214 ~]# chmod +x &#x2F;etc&#x2F;keepalived&#x2F;check_mysql.sh 1.4.3 启停keepalived进程【在 MySQL 所在两个节点的 root 用户下执行】主备服务器启动keepalived [root@node1 ~]# systemctl start keepalived.service[root@node1 ~]# systemctl status keepalived.service keepalived 停止命令（需要停止该进程时执行，正常安装启动后不需要执行） [root@node1 ~]# systemctl stop keepalived.service 1.4.4 验证 MySQL 双机切换主备服务器启动后，可以通过ifconfig命令来查看网卡的状态，其中一台服务器包含一个虚拟网卡，另外一台机器没有虚拟网卡，如下图所示。 当将 node1 上的 mysql 停止时，虚拟网卡应该切换到 node2 上。","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇一：基本介绍","slug":"ambari-install-guides-1-introduction-md","date":"2020-05-31T09:15:27.000Z","updated":"2020-07-25T11:06:54.065Z","comments":true,"path":"2020/05/31/ambari-install-guides-1-introduction-md/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/31/ambari-install-guides-1-introduction-md/","excerpt":"基于 Ambari 2.7.3、CentOS 7.x Ambari官网地址：点击这里","text":"基于 Ambari 2.7.3、CentOS 7.x Ambari官网地址：点击这里 1、大数据组件介绍 Hadoop快速入门系列——篇一：HBase入门 Hadoop快速入门系列——篇二：ElasticSearch Hadoop快速入门系列——篇三：Hive Hadoop快速入门系列——篇四：HDFS Hadoop快速入门系列——篇五：Kafka Hadoop快速入门系列——篇六：其他组件简介 2、Ambari介绍Apache Ambari 是一种基于 Web 的工具，支持 Apache Hadoop 集群的供应、管理和监控。Ambari 已支持大多数 Hadoop 组件，包括 HDFS、MapReduce、Hive、Pig、 Hbase、Zookeeper、Sqoop和Hcatalog 等。 Apache Ambari 支持 HDFS、MapReduce、Hive、Pig、Hbase、Zookeepr、Sqoop 和 Hcatalog 等的集中管理。也是 5 个顶级 hadoop 管理工具之一。 3、集群规划指导 大数据集群需要有合理的规划，以满足业务对于高可用、高性能等的要求，如果规划不当，会导致集群性能折损，甚至导致集群出现故障。因此，一个好的部署规划至关重要，在部署之前，需要根据实际的服务器数量和资源，合理进行规划，规划完成后，需要严格遵循规划的内容，进行部署。由于服务器数量、性能千变万化，下面只对各个组件的规划要求进行列举，并给出本次使用的 3 台服务器的部署规划（三台服务器可以认为是集群部署的最低服务器数量要求） 首先必须确定集群组件的版本需求，不同版本的组件对部署的要求是不一样的 Zookeeper：zookeeper 需要部署奇数台节点，所以最少需要部署三台，在机器数量允许的情况下，可以适当增加，为后面的高可用集群环境做准备。本文以 3 台为例。机器数量允许的情况下，zookeeper 和 journalnode 要求部署5台。 Ambari：ambari server 节点部署 http 服务，ambari agent 节点部署 wget 服务 Ambari：为了方便部署组件客户端，如果业务服务器上也需要使用 Ambari 部署客户端的话，也需要在这些服务器上部署 ambari agent，其实比较建议对所有服务器都部署上 ambari agent，ambari agent 不会消耗很多资源，且这样操作方便后期部署工作的灵活调整，当然，后期再扩展 ambari agent 也是很简单的。 HDFS：当服务器数量充足的情况下，为考虑管理节点的安全性，部署方案中的两台管理节点上不安装datanode和client。 HDFS &amp; YARN：HDFS和yarn数据盘规划遵循木桶效应，即实际数据盘个数以所有节点中数据盘最少的节点的数据盘数量为准 YARN：NodeManager 和 client 部署方式与 DataNode 一致即可Standby-namenode 选择一台机器作为另一个管理节点。具体情况以部署方案为准。 HDFS：为保证 journalnode 的高可用，建议 journalnode 安装5台。本文以 3 台为例。 ElasticSearch：ES Monitor 不要单独安装，与主或备 Master 安装在一台节点上，建议与主 ES Master 安装在一个节点上Hive Monitor 和 Hive Metastore 安装在同一台机器上 Redis：为了保证业务的正常运行，Redis 集群建议部署三台节点以上，如果节点不够，可以采用单节点多实例的方式进行部署。原则上，Redis 主从服务不建议部署在同一节点上，但如果实际节点数不够的情况下，可以采用单节点一主一从的部署方式 本文档以三台服务器为例，主机名/IP分别为：node1/172.168.1.1，node2/172.168.1.2，node3/172.168.1.3","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"YARN fair-scheduler 资源调度介绍及配置","slug":"yarn-fair-scheduler-guide","date":"2020-05-23T01:59:14.000Z","updated":"2020-07-25T11:17:42.852Z","comments":true,"path":"2020/05/23/yarn-fair-scheduler-guide/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/23/yarn-fair-scheduler-guide/","excerpt":"本文部分内容来源网络：CSDN用户@冰上浮云另附官方文档中这部分内容的解析：点击这里","text":"本文部分内容来源网络：CSDN用户@冰上浮云另附官方文档中这部分内容的解析：点击这里 简介公平调度器（FairScheduler）它是hadoop的一个可插拔的调度器，目的是让应用程序在YARN上能够公平的共享巨大的集群资源。公平调度是一种将资源分配给应用程序的方法，这样所有应用程序平均都能在一段时间内获得相同的资源份额。Hadoop NextGen能够调度多个资源类型。默认情况下，Fair调度器仅基于内存来进行公平性决策。利用Ghodsi等人提出的占主导地位的资源公平性概念，可以将其配置为同时使用内存和CPU进行调度。当只有一个应用程序运行时，该应用程序将使用整个群集资源。当其他应用被提交时，释放的资源被分配给新的应用，这样每个应用最终获得的资源量大致相同。与默认的Hadoop调度程序（它构造一个应用程序队列）不同，它允许小的应用程序在合理的时间内完成，而不会使耗时长的应用挨饿。这也是在多个用户之间共享集群的合理方法。最后，公平分享也可以与应用程序优先级一起工作-优先级被用作权重，以确定每个应用程序应获得的总资源的百分比。 调度程序将应用程序进一步组织到“队列”中，并在这些队列之间公平地共享资源。默认情况下，所有用户共享一个名为“default”的队列。如果应用程序在容器资源请求中明确列出队列，则请求将提交到该队列。还可以通过配置根据请求中包含的用户名分配队列。在每个队列中，调度策略用于在运行的应用程序之间共享资源。默认情况下是基于内存的公平共享，但也可以配置具有主要资源公平性的FIFO和多资源共享策略。队列可以按层次结构排列以划分资源，并配置权重以按特定比例共享集群。 除了提供公平共享外，公平调度程序还允许将保证的最小资源分配给队列，这对于确保某些用户、组或生产应用程序始终获得足够的资源非常有用。当队列包含应用程序时，它至少获得其最小共享，但当队列不需要其完全保证的共享时，多余的部分将在其他正在运行的应用程序之间分配。这样，当队列不包含应用程序时，调度器可以保证队列的容量，同时有效地利用资源。 Fair调度程序允许默认情况下运行所有应用程序，但也可以通过配置文件限制每个用户和每个队列运行的应用程序数。当用户必须一次提交数百个应用程序时，这可能很有用；如果同时运行太多应用程序会导致创建太多中间数据或切换太多上下文，这通常可以提高性能。限制应用不会导致任何随后提交的应用失败，只会在计划程序队列中等待，直到用户的某些早期应用完成。 队列简析fair调度程序支持分层队列。所有队列的根是“root”。可用资源以典型的公平调度方式分布在根队列的子队列中。然后，子队列以同样的方式将分配给他们的资源分配给他们的下级队列。应用程序只能在叶队列上调度。通过将队列作为其父队列的子元素放置在公平调度程序分配文件中，可以将队列指定为其他队列的子队列。 队列的名称以其父队列的名称开头，句点作为分隔符。因此，根队列下名为“queue1”的队列称为“root.queue1”，名为“parent1”的队列下名为“queue2”的队列称为“root.parent1.queue2”。当引用队列时，名称的根部分是可选的，因此queue1可以被称为“queue1”，queue2可以被称为“parent1.queue2”。 此外，fair调度器允许为每个队列设置不同的自定义策略，以允许以用户希望的任何方式共享队列的资源。可以通过扩展org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy来构建自定义策略。FifoPolicy、FairSharePolicy（默认）和DominantResourceFairnessPolicy都是内置的，可以随时使用。 在原始（MR1）公平调度程序中还没有支持某些附加项。其中之一，是使用一种自定义策略来管理优先“提升”某些应用程序。 自动将应用程序放入队列Fair调度程序允许管理员配置策略，以便自动将提交的应用程序放置到适当的队列中。程序具体放入哪个队列取决于提交者的用户（user）和组(group)以及应用程序传递的请求队列名。策略由一组有顺序的规则组成，这些规则用于对传入的应用程序进行分类。每个规则要么将应用放入队列，要么拒绝它，要么继续下一个规则。有关如何配置这些策略，请参阅下面的分配文件格式。 前提务必确认 YARN 的以下配置，否则 yarn 不会读取这个文件yarn.resourcemanager.scheduler.class = org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler两周方法进行配置： 使用 Ambari 配置在 YARN 的 Config 内，找到这个配置项，将内容修改为上面的内容即可 使用 yarn-site.xml 文件配置打开文件，查找该配置项，并修改内容，如果没有该配置项，则需要新建：&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;&lt;/property&gt; 配置详解本节的参数需要在yarn-site.xml中，将配置参数yarn.resourcemanager.scheduler.class设置为org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler 才会生效。Fair Scheduler的配置选项包括两部分，其中一部分在yarn-site.xml中，主要用于配置调度器级别的参数，另外一部分在一个自定义配置文件（默认是fair-scheduler.xml）中，主要用于配置各个队列的资源量、权重等信息。 yarn-site.xml 参数名称 说明 缺省值 yarn.scheduler.fair.allocation.file 自定义XML配置文件所在位置，该文件主要用于描述各个队列的属性，比如资源量、权重等，具体配置格式将在后面介绍。 yarn.scheduler.fair.user-as-default-queue 当应用程序未指定队列名时，是否指定用户名作为应用程序所在的队列名。如果设置为false或者未设置，所有未知队列的应用程序将被提交到default队列中 true yarn.scheduler.fair.preemption 是否启用抢占机制 false yarn.scheduler.fair.sizebasedweight 在一个队列内部分配资源时，默认情况下，采用公平轮询的方法将资源分配各各个应用程序，而该参数则提供了另外一种资源分配方式：按照应用程序资源需求数目分配资源，即需求资源数量越多，分配的资源越多。 false yarn.scheduler.assignmultiple 是否启动批量分配功能。当一个节点出现大量资源时，可以一次分配完成，也可以多次分配完成。 false yarn.scheduler.fair.max.assign 如果开启批量分配功能，可指定一次分配的container数目。 -1，表示不限制。 yarn.scheduler.fair.locality.threshold.node 当应用程序请求某个节点上资源时，它可以接受的可跳过的最大资源调度机会。当按照分配策略，可将一个节点上的资源分配给某个应用程序时，如果该节点不是应用程序期望的节点，可选择跳过该分配机会暂时将资源分配给其他应用程序，直到出现满足该应用程序需的节点资源出现。通常而言，一次心跳代表一次调度机会，而该参数则表示跳过调度机会占节点总数的比例 -1.0，表示不跳过任何调度机会 yarn.scheduler.fair.locality.threshold.rack 当应用程序请求某个机架上资源时，它可以接受的可跳过的最大资源调度机会。 yarn.scheduler.increment-allocation-mb 内存规整化单位 默认是1024，这意味着，如果一个Container请求资源是1.5GB，则将被调度器规整化为ceiling(1.5 GB / 1GB) * 1G=2GB。 yarn.scheduler.increment-allocation-vcores 虚拟CPU规整化单位 默认是1，含义与内存规整化单位类似。 自定义配置文件 fair-schedulerFair Scheduler允许用户将队列信息专门放到一个配置文件（默认是fair-scheduler.xml），对于每个队列，管理员可配置以下几个选项： 参数名称 说明 minResources 最少资源保证量，设置格式为“X mb, Y vcores”，当一个队列的最少资源保证量未满足时，它将优先于其他同级队列获得资源，对于不同的调度策略（后面会详细介绍），最少资源保证量的含义不同，对于fair策略，则只考虑内存资源，即如果一个队列使用的内存资源超过了它的最少资源量，则认为它已得到了满足；对于drf策略，则考虑主资源使用的资源量，即如果一个队列的主资源量超过它的最少资源量，则认为它已得到了满足。 maxResources 最多可以使用的资源量，fair scheduler会保证每个队列使用的资源量不会超过该队列的最多可使用资源量。 maxRunningApps 最多同时运行的应用程序数目。通过限制该数目，可防止超量Map Task同时运行时产生的中间输出结果撑爆磁盘。 minSharePreemptionTimeout 最小共享量抢占时间。如果一个资源池在该时间内使用的资源量一直低于最小资源量，则开始抢占资源。 schedulingMode/schedulingPolicy 队列采用的调度模式，可以是fifo、fair或者drf。 aclSubmitApps 可向队列中提交应用程序的Linux用户或用户组列表，默认情况下为“*”，表示任何用户均可以向该队列提交应用程序。需要注意的是，该属性具有继承性，即子队列的列表会继承父队列的列表。配置该属性时，用户之间或用户组之间用“，”分割，用户和用户组之间用空格分割，比如“user1, user2 group1,group2”。 aclAdministerApps 该队列的管理员列表。一个队列的管理员可管理该队列中的资源和应用程序，比如可杀死任意应用程序。 自定义配置文件配置方法 登录YARN ResourceManager（active或standby均可）所在服务器 进入yarn安装目录：/usr/hdp/2.6.3.0-235/hadoop-yarn/etc/hadoop/ 修改：fair-schedule.xml文件下面给出该文件的配置项说明和配置参考：配置说明：&lt;?xml version&#x3D;&quot;1.0&quot;?&gt;&lt;allocations&gt; &lt;queue name&#x3D;&quot;sample_queue&quot;&gt; &#x2F;&#x2F;队列名 &lt;minResources&gt;10000 mb,0vcores&lt;&#x2F;minResources&gt; &#x2F;&#x2F;最小资源 &lt;maxResources&gt;90000 mb,0vcores&lt;&#x2F;maxResources&gt; &#x2F;&#x2F;最大资源 &lt;maxRunningApps&gt;50&lt;&#x2F;maxRunningApps&gt; &#x2F;&#x2F;可以同时运行的作业数 &lt;weight&gt;2.0&lt;&#x2F;weight&gt; &#x2F;&#x2F;权值 &lt;schedulingPolicy&gt;fair&lt;&#x2F;schedulingPolicy&gt; &#x2F;&#x2F;队列内部调度策略，可选的有：fair、fifo、drf 或者 继承该类的子类（org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy） &lt;queue name&#x3D;&quot;sample_sub_queue&quot;&gt; &#x2F;&#x2F;队列的子目录 &lt;minResources&gt;5000 mb,0vcores&lt;&#x2F;minResources&gt; &lt;&#x2F;queue&gt; &lt;&#x2F;queue&gt; &lt;user name&#x3D;&quot;sample_user&quot;&gt; &#x2F;&#x2F;对于特定用户的配置 &lt;maxRunningApps&gt;30&lt;&#x2F;maxRunningApps&gt; &lt;&#x2F;user&gt; &lt;userMaxAppsDefault&gt;5&lt;&#x2F;userMaxAppsDefault&gt; &#x2F;&#x2F;默认的用户最多可以同时运行的任务&lt;&#x2F;allocations&gt; 这里给出一个模板作为参考，实际部署时，可以按照这个模板，修改一些配置： &lt;?xml version=\"1.0\" ?&gt;&lt;allocations&gt;&lt;queue name=\"root\"&gt;&lt;minResources&gt;2048 mb,4 vcores&lt;/minResources&gt;&lt;maxResources&gt;262144 mb,112 vcores&lt;/maxResources&gt; &lt;!-- 现场修改 --&gt;&lt;maxRunningApps&gt;10&lt;/maxRunningApps&gt;&lt;schedulingMode&gt;fair&lt;/schedulingMode&gt;&lt;aclSubmitApps&gt; &lt;/aclSubmitApps&gt;&lt;aclAdministerApps&gt; &lt;/aclAdministerApps&gt;&lt;queue name=\"spark-max\"&gt;&lt;minResources&gt;2048 mb,4 vcores&lt;/minResources&gt;&lt;maxResources&gt;125829 mb,53 vcores&lt;/maxResources&gt; &lt;!-- 现场修改 --&gt;&lt;maxRunningApps&gt;10&lt;/maxRunningApps&gt;&lt;aclSubmitApps&gt;rhino,admin,flink&lt;/aclSubmitApps&gt;&lt;weight&gt;2.0&lt;/weight&gt;&lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt;&lt;/queue&gt;&lt;queue name=\"spark-min\"&gt;&lt;minResources&gt;2048 mb,4 vcores&lt;/minResources&gt;&lt;maxResources&gt;41943 mb,17 vcores&lt;/maxResources&gt; &lt;!-- 现场修改 --&gt;&lt;maxRunningApps&gt;10&lt;/maxRunningApps&gt;&lt;aclSubmitApps&gt;rhino,admin,flink&lt;/aclSubmitApps&gt;&lt;weight&gt;1.0&lt;/weight&gt;&lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt;&lt;/queue&gt;&lt;queue name=\"spark-mid\"&gt;&lt;minResources&gt;2048 mb,4 vcores&lt;/minResources&gt;&lt;maxResources&gt;41943 mb,17 vcores&lt;/maxResources&gt; &lt;!-- 现场修改 --&gt;&lt;maxRunningApps&gt;10&lt;/maxRunningApps&gt;&lt;aclSubmitApps&gt;rhino,admin,flink&lt;/aclSubmitApps&gt;&lt;weight&gt;1.0&lt;/weight&gt;&lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt;&lt;/queue&gt;&lt;queue name=\"root.hive\"&gt;&lt;minResources&gt;2048 mb,4 vcores&lt;/minResources&gt;&lt;maxResources&gt;41943 mb,17 vcores&lt;/maxResources&gt; &lt;!-- 现场修改 --&gt;&lt;maxRunningApps&gt;10&lt;/maxRunningApps&gt;&lt;aclSubmitApps&gt;rhino,admin,flink&lt;/aclSubmitApps&gt;&lt;weight&gt;1.0&lt;/weight&gt;&lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt;&lt;/queue&gt;&lt;user name=\"rhino\"&gt;&lt;!-- 对于特定用户的配置 --&gt;&lt;maxRunningApps&gt;10&lt;/maxRunningApps&gt;&lt;/user&gt;&lt;/queue&gt;&lt;userMaxAppsDefault&gt;10&lt;/userMaxAppsDefault&gt;&lt;!-- 默认的用户最多可以同时运行的任务 --&gt;&lt;/allocations&gt; 模板中标注了“现场修改”字样的部分，前面的 “41943 mb” 为内存，后面的 “17 vcores” 为逻辑核数注意：模板复制后，建议将里面的中文全部删除，否则可能会因为字符的原因，导致文件读取失败 资源的配置上，根据实际业务，进行分配即可，下面给出模板里的分配细节：&lt;queue name=&quot;root&quot;&gt; 配置总资源① 的 80%&lt;queue name=&quot;spark-max&quot;&gt; 配置&lt;queue name=&quot;root&quot;&gt;资源的 60%&lt;queue name=&quot;spark-min&quot;&gt; 配置&lt;queue name=&quot;root&quot;&gt;资源的 20%&lt;queue name=&quot;spark-mid&quot;&gt; 配置&lt;queue name=&quot;root&quot;&gt;资源的 20% ①总资源的计算方法：在 ambari 界面，查看 YARN NodeManagers 所在机器，将这些机器上的内存和cpu逻辑核② 加上即可②CPU逻辑核数查看方法：cat /proc/cpuinfo | grep &quot;processor&quot; | wc -l 文件修改完成后，保存，将文件复制同步到另外一个 ResourcesManager 服务器的相同路径下 登录 ambari 界面，重启 YARN 组件，然后根据 ambari 界面指示，重启其他受影响的组件","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Hadoop","slug":"CS-Soft/Server/Hadoop","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Hadoop/"}],"tags":[{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://winyter.github.io/MyBlog/tags/Hadoop/"},{"name":"Yarn","slug":"Yarn","permalink":"https://winyter.github.io/MyBlog/tags/Yarn/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇三：Ambari 及基础组件部署","slug":"ambari-install-guides-3-install-ambari-and-base-components","date":"2020-05-21T12:00:17.000Z","updated":"2020-07-25T11:13:09.062Z","comments":true,"path":"2020/05/21/ambari-install-guides-3-install-ambari-and-base-components/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/21/ambari-install-guides-3-install-ambari-and-base-components/","excerpt":"1、准备工作 1.1 Ambari 版本包解析Ambari版本包较多，下面会对所有版本包进行简单的说明，本文档以 Ambari 2.7.3 版本为例：","text":"1、准备工作 1.1 Ambari 版本包解析Ambari版本包较多，下面会对所有版本包进行简单的说明，本文档以 Ambari 2.7.3 版本为例： Ambari 安装包即 Ambari 自身的安装包 ambari-2.7.3.0-centos7.tar.gz HDP 集群安装包即大数据平台的安装包，包含了各类组件 HDP-2.6.3.0-centos7-rpm.tar.gzHDP-UTILS-1.1.0.21-centos7.tar.gz 组件包Ambari 支持针对单个组件进行不同的组件版本定制，并非只能使用 HDP 版本包中的组件版本，你只需要上传你想要的组件版本包，并进行一些简单的操作，即可自使用自己定制的版本进行安装，本文档后面会指导如何定制组件版本进行安装，下面列出几个包名称，作为示例： elasticsearch-6.8.1.tar.gzkafka-1.1.0.tar.gz 组件服务包在定制组件版本时，我们不仅需要上传组件版本包，部分组件还需要上传组件服务包，用来帮助 Ambari 指定针对该版本的一些配置，本文档后面会指导这些服务包如何使用，下面列出几个服务包名称，作为示例： ELASTICSEARCH_6.8.1.tar.gzHBASE_1.4.9.tar.gz 1.2 准备工作【在 Ambari Server 节点的 root 用户下执行】将上一节所说的所有安装包上传到 /home/package/ambari 目录下，并建议根据不同种类的安装包分文件夹存放 |_ &#x2F;home&#x2F;package&#x2F;ambari |_ versionPackage # 存放 Ambari 和 HDP 版本包，共 3 个包 |_ dependentPackage # 存放独立组件版本包，按需上传即可 |_ servicePackage # 存放组件服务包，按需上传即可 1.3 配置 Ambari Server 本地源【在 Ambari Server 节点的 root 用户下操作】创建本地目录 [root@node1 ~]# mkdir -p &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari 拷贝 ambari-2.7.3.0-centos7.tar.gz 到 /var/www/html/ambari 目录下，并解压。 [root@node1 ~]# cp &#x2F;home&#x2F;package&#x2F;ambari&#x2F;versionPackage&#x2F;ambari-2.7.3.0-centos7.tar.gz &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari[root@node1 ~]# cd &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari[root@node1 ~]# tar -zxvf ambari-2.7.3.0-centos7.tar.gz 修改ambari.repo文件，将该文件拷贝到 /etc/yum.repos.d目录下。 [root@node1 ~]# cp &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari&#x2F;ambari&#x2F;centos7&#x2F;2.7.3.0-139&#x2F;ambari.repo &#x2F;etc&#x2F;yum.repos.d&#x2F; 编辑ambari.repo [root@node1 ~]# cd &#x2F;etc&#x2F;yum.repos.d&#x2F;[root@node1 ~]# vim ambari.repo 参照以下内容修改： # VERSION_NUMBER&#x3D;2.6.0.0-267[ambari-2.7.3.0]name&#x3D;ambari Version - ambari-2.7.3.0baseurl&#x3D;http:&#x2F;&#x2F;172.168.1.1&#x2F;ambari&#x2F;ambari&#x2F;centos7&#x2F;2.7.3.0-139gpgcheck&#x3D;1gpgkey&#x3D;http:&#x2F;&#x2F;172.168.1.1&#x2F;ambari&#x2F;ambari&#x2F;centos7&#x2F;2.7.3.0-139&#x2F;RPM-GPG-KEY&#x2F;RPM-GPG-KEY-Jenkinsenabled&#x3D;1priority&#x3D;1 ambari.repo 中的 baseurl 与 gpgkey 都需根据实际的路径填写。正常情况下只需修改IP地址即可。 1.4 准备 HDP 集群本地源文件【在 Ambari Server 节点的 root 用户下操作】创建目录 [root@node1 ~]# mkdir -p &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari&#x2F;hdp 拷贝 HDP-2.6.3.0-centos7-rpm.tar.gz、HDP-UTILS-1.1.0.21-centos7.tar.gz 到 /var/www/html/ambari/hdp 目录下，并解压。 [root@node1 ~]# cp &#x2F;home&#x2F;package&#x2F;ambari&#x2F;versionPackage&#x2F;HDP* &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari&#x2F;hdp[root@node1 ~]# cd &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari&#x2F;hdp[root@node1 ~]# tar -zxvf HDP-2.6.3.0-centos7-rpm.tar.gz[root@node1 ~]# tar -zxvf HDP-UTILS-1.1.0.21-centos7.tar.gz 1.5 准备组件本地源文件【在 Ambari Server 节点的 root 用户下操作】创建目录 [root@node1 ~]# mkdir -p &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari&#x2F;extend 根据组件版本清单，拷贝相应版本包，下面举两个例子，作为示例 [root@node1 ~]# cp &#x2F;home&#x2F;package&#x2F;ambari&#x2F;dependentPackage&#x2F;elasticsearch-6.3.0.tar.gz &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari&#x2F;extend[root@node1 ~]# cp &#x2F;home&#x2F;package&#x2F;ambari&#x2F;dependentPackage&#x2F;kafka-1.1.0.tar.gz &#x2F;var&#x2F;www&#x2F;html&#x2F;ambari&#x2F;extend 一般情况下，不建议单个组件上传多个版本，这通常会导致版本冲突，可能会使你想要的那个组件版本不会在 Ambari 中出现，导致无法安装 2、安装&amp;配置 Ambari Server 2.1 安装 Ambari Server【在 Ambari Server 节点的 root 用户下执行】输入以下命令安装Ambari Server [root@node1 ~]# yum install ambari-server 每次出现以下交互信息时，输入y继续 [root@node1 ~]# Is this ok [y&#x2F;d&#x2F;N]: y 2.2 在 MySQL 中新建 Ambari 用户【在 MySQL 主节点的 rhino 用户下执行】 Ambari 默认安装 PostgreSQL 数据库，使用的数据库为 ambari，默认使用的用户名和密码为 ambari/bigdata。本次安装使用之前安装的 MySQL 数据库，数据库所在的节点为 172.168.1.1，端口号 3306。若不想使用默认的 PostgreSQL 数据库，需要在安装 Ambari Server 前安装数据库，并为 Ambari 创建用户和数据库。下面在 rhino 用户的 MySQL 中为 Ambari 创建使用的用户及数据库，Ambari 使用 MySQL 的用户名为 ambari，密码为 xxxxxxxx，数据库名称为 ambari： 登录 MySQL [rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64[rhino@node1 ~]$ bin&#x2F;mysql -h0 -urhino -pxxxxxxxx 执行如下语句：172.168.1.1 表示 Ambari Server 所在节点的 IP，按照实际情况修改 CREATE USER &#39;ambari&#39;@&#39;%&#39; IDENTIFIED BY &#39;xxxxxxxx&#39;;GRANT ALL PRIVILEGES ON . TO &#39;ambari&#39;@&#39;%&#39;;CREATE USER &#39;ambari&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;xxxxxxxx&#39;;GRANT ALL PRIVILEGES ON . TO &#39;ambari&#39;@&#39;localhost&#39;;CREATE USER &#39;ambari&#39;@&#39;172.168.1.1&#39; IDENTIFIED BY &#39;xxxxxxxx&#39;;GRANT ALL PRIVILEGES ON . TO &#39;ambari&#39;@&#39;172.168.1.1&#39;;FLUSH PRIVILEGES; 2.3 拷贝建库脚本到 MySQL 所在节点【在 Ambari Server 节点的 root 用户下执行】[root@node1]# cd &#x2F;var&#x2F;lib&#x2F;ambari-server&#x2F;resources[root@node1]# scp Ambari-DDL-MySQL-CREATE.sql rhino@192.168.50.213:~ 2.4 新建 Ambari 数据库【在 MySQL 主节点的 rhino 用户下执行】修改刚才拷贝的文件的用户属组（在 MySQL 主节点的 root 用户下执行） [root@node1]# cd &#x2F;home&#x2F;rhino[root@node1]# chown -R rhino:root Ambari-DDL-MySQL-CREATE.sql 使用 ambari 用户登录 MySQL [rhino@node1 ~]$ cd &#x2F;home&#x2F;rhino&#x2F;mysql-5.6.25-linux-x86_64[rhino@node1 mysql-5.6.25-linux-x86_64]$ bin&#x2F;mysql -h0 -uambari -pxxxxxxxx 执行以下命令 mysql&gt; CREATE DATABASE ambari;mysql&gt; USE ambari;mysql&gt; source &#x2F;home&#x2F;rhino&#x2F;Ambari-DDL-MySQL-CREATE.sql; 安装完成后可以在metainfo表中查看版本号（版本号2.7.3） mysql&gt; select * from metainfo;+--------------------+-----------------------+| metainfo_key | metainfo_value |+--------------------+-----------------------+| version | 2.7.3 |+--------------------+-----------------------+1 row in set (0.00 sec)mysql&gt; exit; 2.5 拷贝 MySQL JDBC 驱动包【在 MySQL 主节点的 root 用户下执行】将连接mySQL的JDBC驱动jar包拷贝到/usr/share/java目录下 [root@node1 ~]# cd &#x2F;home&#x2F;rhino&#x2F;version&#x2F;dependentPackage[root@node1 ~]# cp mysql-connector-java-5.1.40-bin.jar &#x2F;usr&#x2F;share&#x2F;java 2.6 配置 Ambari Server【在 Ambari Server 节点的 root 用户下执行】使用以下命令配置Ambari Server ambari-server setupUsing python &#x2F;usr&#x2F;bin&#x2F;pythonSetup ambari-serverChecking SELinux…​SELinux status is &#39;enabled&#39;SELinux mode is &#39;permissive&#39;WARNING: SELinux is set to &#39;permissive&#39; mode and temporarily disabled.OK to continue [y&#x2F;n] (y)?y 配置过程中，会出现各种提示，最后()中给出的值为默认配置，如果所有配置都按默认配置，只需要按提示进行即可，如果要指定配置，需要按提示给出相应的配置信息。 Customize user account for ambari-server daemon [y&#x2F;n] (n)? n 是否使用默认的用户root，选择n使用root用户。 继续，提示： Checking JDK…​[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8[2] Custom JDK&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;Enter choice (1): 2 默认选择为1，如果选择2，则需要手动安装JDK（包括Hadoop集群中所有机器）。这里选择2，出现提示： WARNING:JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.Path to JA home: &#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_181Validating JDK on Ambari Server…​done.Check JDK version for Ambari Server…​JDK version found: 8Minimum JDK version is 8 for Ambari. Skipping to setup different JDK for Ambari Server.Checking GPL software agreement…​GPL License for LZO: https:&#x2F;&#x2F;www.gnu.org&#x2F;licenses&#x2F;old-licenses&#x2F;gpl-2.0.en.htmlEnable Ambari Server to download and install GPL Licensed LZO packages [y&#x2F;n] (n)?n 输入有效的 Java Home 路径 /home/rhino/jdk1.8.0_181。继续，出现提示： Enter advanced database configuration [y&#x2F;n] (n)?y 选择y，出现提示： Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle[3] - MySQL &#x2F; MariaDB[4] - PostgreSQL[5] - Microsoft SQL Server (Tech Preview)[6] - SQL Anywhere[7] - BDB&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;Enter choice (1): 3 选择3，依次输入mysql所在的节点名称，IP地址，ip是mysql安装机器真实的ip，不要配置虚拟ip、mysql端口号、数据库名称、用户名、用户密码： Hostname (localhost): 172.168.1.1Port (3306): 3306Database name (ambari): ambariUsername (ambari): ambariEnter Database Password (bigdata): xxxxxxxxEnter full path to custom jdbc driver: &#x2F;usr&#x2F;share&#x2F;java&#x2F;mysql-connector-java-5.1.40-bin.jar 继续，提示： WARNING:Before starting Ambari Server, you must run the following DDL against the database to create the schema: &#x2F;var&#x2F;lib&#x2F;ambari-server&#x2F;resources&#x2F;Ambari-DDL-MySQL-CREATE.sql Proceed with configuring remote database connection properties [y&#x2F;n] (y)? y 按提示输入y执行，继续，最后出现： Ambari Server &#39;setup&#39; completed successfully. 表示配置成功。| ||---|| 如果在配置的过程中被中断，可以再输入ambari-server setup命令重新配置，此时已经完成的配置会以默认值出现，如： || Configuring database…============================================================= Choose one of the following options: [1] - PostgreSQL (Embedded) [2] - Oracle [3] - MySQL / MariaDB [4] - PostgreSQL [5] - Microsoft SQL Server (Tech Preview) [6] - SQL Anywhere [7] - BDB ============================================================= Enter choice (3): 3 Hostname (172.168.1.1): ||这里配置数据库的hostname，默认值显示的是上一次配置的172.168.1.1。| 使用以下命令启动Ambari Server [root@node1 ~]# ambari-server start 最后出现： Ambari Server &#39;start&#39; completed successfully. 说明启动成功。 查看Ambari Server的运行状态： [root@node1 ~]# ambari-server statusUsing python &#x2F;usr&#x2F;bin&#x2F;pythonAmbari-server statusAmbari Server runningFound Ambari Server PID: 26512 at: &#x2F;var&#x2F;run&#x2F;ambari-server&#x2F;ambari-server.pid 3、部署 HDP 集群 3.1 登录Ambari界面输入172.168.1.1:8080登录 ambari 界面，默认用户/密码为：admin/admin。点击Launch Install Wizard按钮，进入安装界面 3.2 集群命名第一步为要创建的集群设置名称，每个集群的名称应该是唯一的。这里设置为 $rhino$，点击下一步。 3.3 版本与安装方式在联网的情况下，Ambari会列出可使用的版本信息，如果有版本定义文件（Version Definition File），也可以点击Add Version添加自定义版本信息。 选择Add Version添加自定义版本信息。 输入http://172.168.1.1/ambari/hdp/HDP/centos7/2.6.3.0-235/HDP-2.6.3.0-235.xml 点击ok，选择stack为2.6版本。 根据实际使用的操作系统填写URL信息。如果可以联网，可以选择公共源安装，如果不能联网选择本地源安装，需要手动配置本地源的URL。不需要的操作系统信息可以点击Remove删除。点击下一步时会检测填写的URL是否有效。本次安装选择本地源通过浏览器填写hdp的URL，结果如下： 其中，hdp.repo 的 URL 为 http://172.168.1.1/ambari/hdp/HDP/centos7/2.6.3.0-235/，hdp-util.repo 的 URL 为 http://172.168.1.1/ambari/hdp/HDP-UTILS-1.1.0.21/repos/centos7/。 URL 中 172.168.1.1 为安装 ambari-server 的机器 IP，版本号和操作系统可能会有所不同，请根据实际情况填写。 3.4 安装节点设置填写集群的所有节点，提示要输入的是各节点的完全限定域名，没有设置的话填写主机名称： 填写 ambari server 机器私钥 确保 ambari-server 能自动安装 ambari-agent 到各目标机器上，将 ambari-server 机器上生成的私钥在 /root/.ssh/id_rsa 文件中，将 id_rsa 文件的内容全部复制。 [root@node1 ~]# cat &#x2F;root&#x2F;.ssh&#x2F;id_rsa-----BEGIN RSA PRIVATE KEY-----MIIEpQIBAAKCAQEApAuiPxOkng83NO2d16feoP21XT8HsGhD+KuS050sDE1OqLlMDnst8jxv+Ptm+olxasBlowU6TBoPwAoA+EFaLuva53Tvg1C8emrUSlNZgIsLmC+m33H2g1z9JkznYRQ6i9ib3RhNj30EHap7nAhNnJT3tFp73ePILGHVbpqMzAka5tFUxsYzr&#x2F;IJjWEh+0kha+6D9bjqIIzlE3qWMo8Xwshe1CF5Yq0yRzp1gZOjN8XOc27hP5NJMbaTPUdW+pn5WlnpPZhStQ7aE+zFR6exqZOPGWEBr4YfXaj1dgghFJOB1GYjmIQAaVbWFksCZuu0FB6d1ulwuR3EdtwVGhuxGQIDAQABAoIBAFjAlpIrzXdaYhL3r9saTn+pY&#x2F;NEA6P0dTnXkcN9mHQ7ayAryDNZf308J5R8Z7WKoNsRpqtxS54ax4StpOKrcOBL6I4rHN5d0uskWyCvQZAjKi23MkBXxvhBWhDbsJ88M4Svt3wCBwxnpc9rCh9b+qmJiw5&#x2F;md5tu0IP2EpTwXMtZ+lf91g&#x2F;XuDrB6nlvv8a&#x2F;x&#x2F;eC3xj+LWBd3MODaMsXOwQhsCzepH81oO4a1djzqP5oHe36syCKS2DTxQcQJb+3WMR4QlGD99dF2jlSsWMPY8Z+8RRo4mLtR5nkk9TZsbNPyArXYzlfKhQJ8P8oaS3yWdHJkDy61RoT0FLLjrkK7UCgYEA1wHxGXGPWuYar3xXkVnqx+47mRwk7yeeaeyG9n5SqtRodvo0gUzjOXuaYLMtVc47xzXw3QRGvZgmEvCvGfnIaCuEqEi6Z9f+pewqHcUUDlptzp5So&#x2F;84r+KpLAGKgZlXAuG4t4B1KnHvbXbyVk0lz6&#x2F;5caoAOE8g2LjDzggMNXcCgYEAw1JW4zJWH33wGQsr+BZEkLQULPXetsh76DE3AqyiI+8JH1e34Ny4Ed+rJiv4TE1Mvdp+8bWQIHxxUEwqk1&#x2F;aNOs66Scf6h3mNrnJi2zlGb5pQowKuI0cQVJ2BiAkMfWk7TsMG4YiTd7hABcO&#x2F;MvWS463tpx7CnwzDxg4F6nWMe8CgYEArCsK5GPx9kjyL3mzKVpGTeiv4rANx1ADYzCa9kE7cz35lORQLQXjokEe4rY35FDrv16rTGBDQUjXnC0NGhDRcNEAPj1WvxbP&#x2F;A97vjD1GXVCHsTayiXyP62R6AIn5hVi&#x2F;pS&#x2F;dHmx2NY5cn9gGMlYMNHqPiFyYaTDWafCa3Y4SIsCgYEAjpKl9cWic+5bugwblkW62t139LGsVkPVnlF+VCdrW0t6nzRKdormmbVomr5xylCKefLpwsnDYNM1a2WNlnHbN9GU+OekNiKJDt1irrFYMgh5kSfkE359Z3knyaTghs9GChyV4+lvVOQh3Qz31bN8wz5z&#x2F;4oNjkPGiXgmSa+hOGUCgYEAykaL1zW3BmbbkNtFx1+rtoq67FKXzNLYo5NBWwDI14zpZnzNXwhiB7IPJOvV&#x2F;LvlmpVRdjTUv4VJiYjhqjNVUPeQxt50A4GgEKYgHKHYIf5QdhbwA4VEQWwuUhvLptJ7yW81piz8V6CUCxNzg&#x2F;M1YEqJWgU&#x2F;rV2LCD50btwK82s&#x3D;-----END RSA PRIVATE KEY----- 点击继续后，由于填写的不是 FQDN，所以出现以下提示，点击 CONTINUE 继续下一步 3.5 ambari-agent 注册根据上一步填写的信息安装 ambari-agent，之后 ambari-agent 向 ambari-server 注册，界面显示每个节点的执行结果，当执行失败时点击 Failed 可以查看详细日志。 报错处理 按照日志修改后，点击retry failed按钮重试。所有机器注册成功后，会检测warning；当检测到warning时，点击查看warning，并需要按照提示手动解决。 这个warning提示The following processes should not be running，&#x2F;home&#x2F;rhino&#x2F;jdk1.8.0_65&#x2F;bin&#x2F;java -D… 解决方法可以通过 pgrep java 查看进程占用情况，并通过 kill -9 进程号（注意不要删了 ambari-server 的 8080 进程）其他的进程都杀掉。 3.6 查看ambari进程状态【在所有 Ambari Agent 节点的 root 用户下执行】[root@node2 ~]# ambari-agent statusFound ambari-agent PID: 8935ambari-agent running.Agent PID at: &#x2F;run&#x2F;ambari-agent&#x2F;ambari-agent.pidAgent out at: &#x2F;var&#x2F;log&#x2F;ambari-agent&#x2F;ambari-agent.outAgent log at: &#x2F;var&#x2F;log&#x2F;ambari-agent&#x2F;ambari-agent.logambari-server statusUsing python &#x2F;usr&#x2F;bin&#x2F;pythonAmbari-server statusAmbari Server runningFound Ambari Server PID: 32729 at: &#x2F;var&#x2F;run&#x2F;ambari-server&#x2F;ambari-server.pid 3.7 安装zookeeper、Ambari Metrics服务安装 zookeeper，Ambari Metrics。SmartSense 是默认安装的，所以必选。在 Ambari 页面选择 Add Service 选择 zookeeper，Ambari Metrics 点击下一步，会提示没有安装一些必要的服务——Ambari Metrics 和 SmartSence，点击 PROCEED ANYWAY Metrics 在第一次安装完后，需要修改 HBase root directory 配置项为：file:///data01/var/lib/ambari-metrics-collection/hbase，（实际目录并不一定是上面这个，主要检查目录中，不能出现类似：file:///data01/data01/var/lib/ambari-metrics-collection/hbase 这样的目录存在），否则 Metrics 会一直报错无法启动 3.8 zookeeper服务选择需要安装zookeeper服务的主机，其中Metrics Collector所在服务器需要测试一下到所有服务器（包括自己）的无密码访问是否可以，由于存在部分服务器进行首次无密码访问时需要手动输入yes，才能进去，不验证的话，后续Metrics Collector服务会有问题。 1.选择需要安装Zookeeper Server的主机。这里选择了三台机器。 2.选择没有安装zookeeper的服务器安装client。 这里需要注意，由于部分组件需要使用 zookeeper 客户端来进行业务调度，已经部署了 zookeeper 的节点自带 zookeeper 客户端，所以可以不部署 zookeeper 客户端，但其他节点必须都安装上 zookeeper 客户端。 3.点击下一步，此处配置默认不修改。 3.9 输入Grafana密码用户名和密码都输入admin 默认即可，一路NEXT 3.10 确认安装确认安装信息，安装的信息可以直接打印出来： 如果进程启动失败或测试没有通过，则显示 WARNING。 注意：安装完成之后是不能返回到之前的步骤检查配置的。 完成 Hadoop 安装 3.11 删除SmartSense服务 SmartSense 为 HortonWorks 提供的一个增值服务，用于收集集群诊断数据，协助支持案例故障排除，除非确实有必有，否则建议卸载该服务。 由于 SmartSense 为默认安装的服务，所以，需要我们手动删除该服务 首先停止服务 删除此服务","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"基于 Ambari 的大数据集群安装指导——篇四：Ambari 安装 HDFS","slug":"ambari-install-guides-4-install-hdfs","date":"2020-05-21T12:00:17.000Z","updated":"2020-07-25T11:13:19.865Z","comments":true,"path":"2020/05/21/ambari-install-guides-4-install-hdfs/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/05/21/ambari-install-guides-4-install-hdfs/","excerpt":"1、安装部署 HDFS","text":"1、安装部署 HDFS 点击services菜单栏右侧的 ... 符号，弹出services拓展选项框。 点击Add Service选项，出现下述服务选择菜单，选择 HDFS。点击 NEXT 根据部署规划，分别选择 NameNode 节点和 SNameNode 节点，点击 NEXT 根据部署规划，选择 datanode 节点和 client 节点，点击 NEXT 1.1 修改 HDFS 配置文件 修改 NameNode 和 DataNode 目录NameNode directories 填写 /hadoop/hdfs/namenodeDataNode directories 填写 /data01/hadoop/hdfs/data,/data02/hadoop/hdfs/data,/data03/hadoop/hdfs/data… 注意：① NameNode directories不要存放在数据盘，防止数据盘损坏导致namenode挂掉② DataNode 磁盘根据有多少块数据盘来配置目录，比如，10块数据盘，那么配置就写为：/data01/hadoop/hdfs/data,...,/data10/hadoop/hdfs/data 修改副本数Block replication 配置为 2 Advanced core-site配置hadoop proxyuser.* 配置为 * Advanced hdfs-site配置dfs.permissions.enabled 配置为 false该配置为关闭 HDFS 的权限控制，该配置根据实际业务来配置，为了防止业务出现问题，如果对 HDFS 的安全要求不高的话，建议直接配置为 false 即可 Custom core-site中新增配置项添加如下两个配置，为 Hadoop 添加业务用户 rhino，实际根据自己的业务需求配置用户名：hadoop.proxyuser.rhino.groups=*hadoop.proxyuser.rhino.hosts=* 修改以下配置：ha.zookeeper.quorum=node1:2181,node2:2181,node3:2181该配置配置为 zookeeper 节点的 主机名:端口号hadoop.proxyuser.root.hosts=node1该配置配置为 HDFS 主节点 主机名hadoop.proxyuser.yarn.hosts=node1,node2该配置配置为 YARN 节点 主机名 1.2 HDFS日志老化配置 修改日志保存路径：Hadoop Log Dir Prefix 配置为 /data01/log/hadoop hadoop-env.sh关闭gc日志打印修改 Advanced Hadoop-env &gt;&gt; hadoop-env template 配置项 该配置项为一个脚本的内容，长度很长，建议将内容复制到 notepad++ 等文本编辑器，根据下面的步骤，修改完成后，再复制到配置内即可。 ① 搜索 HADOOP_JOBTRACKER_OPTS、SHARED_HADOOP_NAMENODE_OPTS、HADOOP_DATANODE_OPTS 三个变量② 将这三个变量中的以下内容删除 -Xloggc:&#123;&#123;hdfs_log_dir_prefix&#125;&#125;&#x2F;$USER&#x2F;gc.log- &#96;date +&#39;%Y%m%d%H%M&#39;&#96; -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps Advanced hdfs-log4j 更改日志为按大小分割并配置日志文件个数把所有按天分文件日志改为按大小，找到下面这一段：# # hdfs audit logging# hdfs.audit.logger&#x3D;INFO,consolelog4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit&#x3D;$&#123;hdfs.audit.logger&#125;log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit&#x3D;falselog4j.appender.DRFAAUDIT&#x3D;org.apache. log4j.DailyRollingFileAppenderlog4j.appender.DRFAAUDIT.File&#x3D;$&#123;hadoop.log.dir&#125;&#x2F;hdfs-audit.loglog4j.appender.DRFAAUDIT.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.DRFAAUDIT.layout.ConversionPattern&#x3D;%d&#123;ISO8601&#125; %p %c&#123;2&#125;: %m%nlog4j.appender.DRFAAUDIT.DatePattern&#x3D;.yyyy-MM-dd改为：# # hdfs audit logging# hdfs.audit.logger&#x3D;INFO,consolelog4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit&#x3D;$&#123;hdfs.audit.logger&#125;log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit&#x3D;falselog4j.appender.DRFAAUDIT&#x3D;org.apache.log4j.RollingFileAppenderlog4j.appender.DRFAAUDIT.File&#x3D;$&#123;hadoop.log.dir&#125;&#x2F;hdfs-audit.loglog4j.appender.DRFAAUDIT.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.DRFAAUDIT.layout.ConversionPattern&#x3D;%d&#123;ISO8601&#125; %p %c&#123;2&#125;: %m%nlog4j.appender.DRFAAUDIT.MaxFileSize&#x3D;128MBlog4j.appender.DRFAAUDIT.MaxBackupIndex&#x3D;5--------------------------------------------------------- 我是分割线 ----------------------------------------------------# # mapred audit logging # mapred.audit.logger&#x3D;INFO,consolelog4j.logger.org.apache.hadoop.mapred.AuditLogger&#x3D;$&#123;mapred.audit.logger&#125;log4j.additivity.org.apache.hadoop.mapred.AuditLogger&#x3D;falselog4j.appender.MRAUDIT&#x3D;org.apache.log4j.DailyRollingFileAppenderlog4j.appender.MRAUDIT.File&#x3D;$&#123;hadoop.log.dir&#125;&#x2F;mapred-audit.loglog4j.appender.MRAUDIT.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.MRAUDIT.layout.ConversionPattern&#x3D;%d&#123;ISO8601&#125; %p %c&#123;2&#125;: %m%nlog4j.appender.MRAUDIT.DatePattern&#x3D;.yyyy-MM-dd改为：# # mapred audit logging# mapred.audit.logger&#x3D;INFO,consolelog4j.logger.org.apache.hadoop.mapred.AuditLogger&#x3D;$&#123;mapred.audit.logger&#125;log4j.additivity.org.apache.hadoop.mapred.AuditLogger&#x3D;falselog4j.appender.MRAUDIT&#x3D;org.apache.log4j.RollingFileAppenderlog4j.appender.MRAUDIT.File&#x3D;$&#123;hadoop.log.dir&#125;&#x2F;mapred-audit.loglog4j.appender.MRAUDIT.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.MRAUDIT.layout.ConversionPattern&#x3D;%d&#123;ISO8601&#125; %p %c&#123;2&#125;: %m%nlog4j.appender.MRAUDIT.MaxFileSize&#x3D;128MBlog4j.appender.MRAUDIT.MaxBackupIndex&#x3D;5 修改后配置项如下图： 1.3 确认安装 HDFS点击next，直到deploy，安装部署hdfs。 2、完成安装 HDFS 2.1 启停 HDFS 3、Hadoop 集群高可用 3.1 指定 Standby-NameNode 和 JournalNode 节点 设置集群名称点击 HDFS，设置 hadoop 集群升级高可用的界面；设置集群名称为 mycluster(名称自定义) 指定Standby-namenode和journalnode节点 Standby-namenode 选择一台机器作为另一个管理节点。具体情况以部署方案为准。为保证 journalnode 的高可用，建议 journalnode 安装5台。这里以3台为例。下图中的 Additional NameNode 即为 Standby-NameNode review配置此步骤是readonly，不必修改，可以根据实际情况将JouralNode的存储路径进行修改。 3.2 手动执行提示命令【在页面提示的节点上的 root 用户下执行】 手动执行命令 注意看页面提示在哪台机器上执行 按照提示去 node1 主机操作 [root@node1 ~]# sudo su hdfs -l -c &#39;hdfs dfsadmin -safemode enter&#39;Safe mode is ON[root@node1 ~]# sudo su hdfs -l -c &#39;hdfs dfsadmin -saveNamespace&#39;Save namespace successful 根据页面提示：前两步执行完成即可点击Next，页面会提示以下操作在哪一台机器上执行，必须严格按照页面提示，在对应的机器上执行 Configure Components点击Next 3.3 手动命令初始化JournalNode 必须严格按照页面提示，在对应的机器上执行。 [root@node1 ~]# sudo su hdfs -l -c &#39;hdfs namenode -initializeSharedEdits&#39; 根据界面提示执行命令。出现如下提示：18/02/03 10:06:10 INFO util.ExitUtil: Exiting with status 0 意味成功。此步骤执行完成后，点击 Next 即可。 3.4 启动Zookeeper和Namnode 3.5 手动执行Initialize Metadata 必须严格按照页面提示，在对应的机器上执行。 # Initialize the metadata for NameNode automatic failover by running:[root@node1 ~]# sudo su hdfs -l -c &#39;hdfs zkfc -formatZK&#39;# Initialize the metadata for the Additional NameNode by running:[root@node1 ~]# sudo su hdfs -l -c &#39;hdfs namenode -bootstrapStandby&#39; 前两步手动执行成功后，按照界面提示点击Next即可。 3.6 等待完成安装自动完成安装及配置，如下图所示，点击done即可 切换到hdfs用户下jps即查看对应的namenode和datanode进程，全部实现了高可用状态。","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Ambari","slug":"CS-Soft/Server/Ambari","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://winyter.github.io/MyBlog/tags/Ambari/"},{"name":"Bigdata","slug":"Bigdata","permalink":"https://winyter.github.io/MyBlog/tags/Bigdata/"}]},{"title":"Linux命令集锦【持续更新】","slug":"UNIX-commands-collection","date":"2020-04-19T06:49:43.000Z","updated":"2020-07-25T11:06:14.312Z","comments":true,"path":"2020/04/19/UNIX-commands-collection/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/04/19/UNIX-commands-collection/","excerpt":"查看CPU信息查看CPU型号： cat &#x2F;proc&#x2F;cpuinfo | grep name | cut -f2 -d: | uniq -c","text":"查看CPU信息查看CPU型号： cat &#x2F;proc&#x2F;cpuinfo | grep name | cut -f2 -d: | uniq -c 查看CPU颗数及CPU物理核心数(个数)： cat &#x2F;proc&#x2F;cpuinfo查看最后一个CPU的physical id(从0开始，数出的个数即为CPU颗数),cpu cores显示的即为单颗核心数，总个数为颗数*核心数 或者 # 查看CPU颗数cat &#x2F;proc&#x2F;cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l# 查看每个CPU内的核数cat &#x2F;proc&#x2F;cpuinfo| grep &quot;cpu cores&quot;| uniq 查看逻辑核数 cat &#x2F;proc&#x2F;cpuinfo | grep &quot;processor&quot; | wc -l CentOS 7.5修改hostname hostnamectl sethostname &lt;hostname&gt; 释放内存cache syncecho 3 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"OS","slug":"CS-Soft/OS","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/"},{"name":"UNIX","slug":"CS-Soft/OS/UNIX","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/UNIX/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://winyter.github.io/MyBlog/tags/Linux/"},{"name":"Collection","slug":"Collection","permalink":"https://winyter.github.io/MyBlog/tags/Collection/"},{"name":"UNIX","slug":"UNIX","permalink":"https://winyter.github.io/MyBlog/tags/UNIX/"},{"name":"Commands","slug":"Commands","permalink":"https://winyter.github.io/MyBlog/tags/Commands/"}]},{"title":"MacOS 快捷键记录【持续更新】","slug":"MacOS-hotkey-collection","date":"2020-04-19T06:45:04.000Z","updated":"2020-07-25T11:05:29.282Z","comments":true,"path":"2020/04/19/MacOS-hotkey-collection/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/04/19/MacOS-hotkey-collection/","excerpt":"不同 App 窗口切换：command+tab App 内窗口切换：command+~ App 内标签页切换：control+tab","text":"不同 App 窗口切换：command+tab App 内窗口切换：command+~ App 内标签页切换：control+tab 输入法快速切换：control+space 快速显示桌面(之一)：F11 打开关闭 Dock 的自动隐藏：option+command+D 拷贝 &amp; 剪切：command+c &amp; command+v / command+option+v 翻页：fn+▲ / fn+▼","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"OS","slug":"CS-Soft/OS","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/"},{"name":"UNIX","slug":"CS-Soft/OS/UNIX","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/UNIX/"},{"name":"MacOS","slug":"CS-Soft/OS/UNIX/MacOS","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/UNIX/MacOS/"}],"tags":[{"name":"MacOS","slug":"MacOS","permalink":"https://winyter.github.io/MyBlog/tags/MacOS/"},{"name":"Hotkey","slug":"Hotkey","permalink":"https://winyter.github.io/MyBlog/tags/Hotkey/"},{"name":"Collection","slug":"Collection","permalink":"https://winyter.github.io/MyBlog/tags/Collection/"}]},{"title":"网站配置 DNS 时，各类符号的含义","slug":"dns-config-parameters-introduction","date":"2020-04-19T05:47:41.000Z","updated":"2020-07-25T11:15:50.394Z","comments":true,"path":"2020/04/19/dns-config-parameters-introduction/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/04/19/dns-config-parameters-introduction/","excerpt":"from 阿里云 主机记录就是域名前缀，常见用法有：","text":"from 阿里云 主机记录就是域名前缀，常见用法有： www：解析后的域名为www.aliyun.com。@：直接解析主域名 aliyun.com。*：泛解析，匹配其他所有域名 *.aliyun.com。mail：将域名解析为mail.aliyun.com，通常用于解析邮箱服务器。二级域名：如：abc.aliyun.com，填写abc。手机网站：如：m.aliyun.com，填写m。显性URL：不支持泛解析（泛解析：将所有子域名解析到同一地址）","categories":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/"},{"name":"Basic","slug":"CS-Concept/Basic","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Basic/"},{"name":"Network","slug":"CS-Concept/Basic/Network","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Basic/Network/"}],"tags":[{"name":"Website","slug":"Website","permalink":"https://winyter.github.io/MyBlog/tags/Website/"},{"name":"DNS","slug":"DNS","permalink":"https://winyter.github.io/MyBlog/tags/DNS/"}]},{"title":"靠谱的数据来源和数据统计平台","slug":"data-resource-platform","date":"2020-04-19T05:43:59.000Z","updated":"2020-07-25T11:15:30.273Z","comments":true,"path":"2020/04/19/data-resource-platform/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/04/19/data-resource-platform/","excerpt":"via:【知乎问题：有哪些好的数据来源或者大数据平台？】 国内官方数据 国家统计局：宏观经济、国计民生 中国互联网信息中心 中国信通院","text":"via:【知乎问题：有哪些好的数据来源或者大数据平台？】 国内官方数据 国家统计局：宏观经济、国计民生 中国互联网信息中心 中国信通院 指数 百度指数：搜索关键词风向 艾瑞指数：反映中国互联网整体和移动互联网市场客观情况 阿里指数：淘宝平台风向标 艺恩：院线电影数据统计 易车指数：国内汽车销售市场 高德交通指数：城市实时交通详情 房天下房地产指数：房地产交易 研究报告 数据分析网：数据分析行业的媒体网站 投中研究院：投资数据 360安全中心研究报告：互联网安全领域 腾讯大数据：基于腾讯产品的数据研究报告 阿里研究院：基于阿里巴巴产品的数据研究报告 第一财经商业数据中心 国外Gold Top50美国管理协会（ AMA）旗下杂志《Marketing News》每年会发布一份Gold Top 50（原为Honomichl Top 50）榜单，列举过去一年美国营收排名前50的市场研究公司。上榜的公司就是非常好的数据来源。 AMA 官网 2018 年 Gold Top50 榜单 科技 IT IDC Gartner 两家公司在IT、电信、消费电子、应用软件领域有很深积累，每年都会发布全球市场智能手机、平板电脑、PC出货量。其实除了科技产业，IDC和Gartner还会定期公开能源、健康、制造等的调研数据。进行相关领域研究时，可以将它们的数据作为一项参考。 股市研究上市公司的人员结构、业务构成，财报是常用手段。以一定时间跨度分析一家公司的财报，比单纯看某个季度更有价值。美股财报可以访问纳斯达克、纽交所或SEC的网站获取，港股财报可以访问香港联交所网站获取，A股财报可以访问上交所、深交所或证监会网站获取。 美股 纳斯达克 纽交所 SEC港股 香港联交所A股 上海证券交易所 深圳证券交易所 证监会 媒体与营销数据来源： 皮尤：独立民调机构，调查范围覆盖政治、社会趋势、宗教，媒体新闻、科技互联网，调查报告和数据可以免费查看。皮尤具有非常现代化的网页设计，体验好过大多数调研机构网站。 VidStatsX：第三方YouTube统计平台，可以提供不同频道的订阅数、排名、视频观看量等数据。VidStatsX数据的时间跨度很大，时效性也很强，可以观察一些爆款视频的数据变化。 移动应用 App Annie：App Annie可以提供一款应用在不同应用商店中的日排名，历史排名以及在不同国家的评级数据。用户也可以查看更详细的下载、收入预估等数据，但这些都需要付费订阅。 数据统计汇总平台导航网站 简道云 199IT","categories":[{"name":"Source","slug":"Source","permalink":"https://winyter.github.io/MyBlog/categories/Source/"}],"tags":[{"name":"Data_Resource","slug":"Data-Resource","permalink":"https://winyter.github.io/MyBlog/tags/Data-Resource/"}]},{"title":"ES新建索引分片失败\"shards_acknowledged\"：false的解决","slug":"ERRORS-ES-shards-acknowledged-false","date":"2020-04-19T05:41:56.000Z","updated":"2020-07-25T11:03:07.882Z","comments":true,"path":"2020/04/19/ERRORS-ES-shards-acknowledged-false/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/04/19/ERRORS-ES-shards-acknowledged-false/","excerpt":"起初在新建索引模板时，始终会报&quot;shards_acknowledged&quot; : false，分片失败。查询分片状态全部是Unassigned。使用curl 11.11.11.11:9200/_cat/nodes查询节点状态只显示一条数据（现场为单机三实例的部署规划），一开始我以为因为是单机版的，所以节点只有一个，于是也没有认为这有异常。包括Head插件的页面也只显示一个节点，一开始我都没觉得这是异常 直到查看两个数据节点的日志，一直有报错：","text":"起初在新建索引模板时，始终会报&quot;shards_acknowledged&quot; : false，分片失败。查询分片状态全部是Unassigned。使用curl 11.11.11.11:9200/_cat/nodes查询节点状态只显示一条数据（现场为单机三实例的部署规划），一开始我以为因为是单机版的，所以节点只有一个，于是也没有认为这有异常。包括Head插件的页面也只显示一个节点，一开始我都没觉得这是异常 直到查看两个数据节点的日志，一直有报错： [2020-03-23T09:48:14,487][INFO ][o.e.d.z.ZenDiscovery ] [ziqKzd9] failed to send join request to master [&#123;ziqKzd9&#125;&#123;ziqKzd9NTt63gLyFN0XxeA&#125;&#123;dc7R_7KzTHq795myQzhrNg&#125;&#123;baiyun-yewu&#125;&#123;68.4.173.174:9300&#125;&#123;ml.machine_memory&#x3D;134765240320, ml.max_open_jobs&#x3D;20, xpack.installed&#x3D;true, ml.enabled&#x3D;true&#125;], reason [RemoteTransportException[[ziqKzd9][68.4.173.174:9300][internal:discovery&#x2F;zen&#x2F;join]]; nested: IllegalArgumentException[can&#39;t add node &#123;ziqKzd9&#125;&#123;ziqKzd9NTt63gLyFN0XxeA&#125;&#123;g4dOXxBGQ3KRV1Xc1TznsA&#125;&#123;baiyun-yewu&#125;&#123;68.4.173.174:9301&#125;&#123;ml.machine_memory&#x3D;134765240320, ml.max_open_jobs&#x3D;20, xpack.installed&#x3D;true, ml.enabled&#x3D;true&#125;, found existing node &#123;ziqKzd9&#125;&#123;ziqKzd9NTt63gLyFN0XxeA&#125;&#123;dc7R_7KzTHq795myQzhrNg&#125;&#123;baiyun-yewu&#125;&#123;68.4.173.174:9300&#125;&#123;ml.machine_memory&#x3D;134765240320, xpack.installed&#x3D;true, ml.max_open_jobs&#x3D;20, ml.enabled&#x3D;true&#125; with the same id but is a different node instance]; ] 查询资料得知是因为数据节点连接不上master节点，原因是因为数据节点的数据存储目录下有数据，顿时恍然大悟，此前es曾删掉重装过，且重新分配过实例和磁盘，所以很可能之前的安装内容没有清空。于是进入数据目录，将目录下的内容清空，重启es即可 （寻找数据目录的方法：查看es配置项：path_data，将配置值目录下的内容清空即可）","categories":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/"},{"name":"Big Data","slug":"CS-Concept/Big-Data","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Big-Data/"}],"tags":[{"name":"ERROR","slug":"ERROR","permalink":"https://winyter.github.io/MyBlog/tags/ERROR/"},{"name":"ES","slug":"ES","permalink":"https://winyter.github.io/MyBlog/tags/ES/"}]},{"title":"Linux NFS 挂载共享文件","slug":"linux-NFS-mount-guide","date":"2020-04-19T05:35:35.000Z","updated":"2020-07-25T11:17:12.422Z","comments":true,"path":"2020/04/19/linux-NFS-mount-guide/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/04/19/linux-NFS-mount-guide/","excerpt":"环境介绍：现有两台服务器：均为 CentOS 7.5 环境，挂载端称为主机，被挂载端称为从机","text":"环境介绍：现有两台服务器：均为 CentOS 7.5 环境，挂载端称为主机，被挂载端称为从机 1、安装 NFS 服务所需的软件包【主从机均需执行】(如果已经安装nfs服务，可以不执行此步骤) yum install -y nfs-utils 2、编辑 exports 文件，添加从机【主机执行】为挂载目录添加777权限： chmod -R 777 &#x2F;home&#x2F;xxx&#x2F;xxx 修改exports文件 vim &#x2F;etc&#x2F;exports添加如下内容：&#x2F;home&#x2F;xxx&#x2F;xxx 111.111.111.0&#x2F;24(rw,sync,root_squash,no_subtree_check) rw 表示可读写，sync 表示同步写，/home/xxx/xxx 为挂载目录，111.111.111.0表示IP在111.111.111.x域内的IP可以挂载这个目录 3、启动 NFS 服务【主机执行】设置开机自启动 systemctl enable rpcbind.servicesystemctl enable nfs.service 启动 rpcbind 和 nfs 服务： systemctl start rpcbind.servicesystemctl start nfs.service 确认NFS服务器启动成功： rpcinfo -p 生效配置： exportfs 可以查看到以下内容，即为成功： &#x2F;home&#x2F;xxx&#x2F;xxx 111.111.111.0&#x2F;24 4、从机上启动NFS客户端【从机执行】为rpcbind做开机启动： systemctl enable rpcbind.service 然后启动rpcbind服务： systemctl start rpcbind.service 注意：客户端不需要启动 服务 5、执行挂载【从机执行】检查是否能读取到NFS服务端的共享目录： showmount -e 111.111.111.111 111.111.111.111为主机IP如有以下内容显示，即为成功： Export list for 111.111.111.111:&#x2F;home&#x2F;xxx&#x2F;xxx 111.111.111.0&#x2F;24 执行挂载： mount -t nfs 111.111.111.111:&#x2F;home&#x2F;xxx&#x2F;xxx &#x2F;home&#x2F;xxxx&#x2F; /home/xxxx为被挂载目录查看挂载结果： df -h","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"OS","slug":"CS-Soft/OS","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/"},{"name":"UNIX","slug":"CS-Soft/OS/UNIX","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/UNIX/"},{"name":"Linux","slug":"CS-Soft/OS/UNIX/Linux","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/UNIX/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://winyter.github.io/MyBlog/tags/Linux/"},{"name":"Tool","slug":"Tool","permalink":"https://winyter.github.io/MyBlog/tags/Tool/"}]},{"title":"VIM 编辑器用法大全【持续更新】","slug":"vim-editor-guide","date":"2020-04-19T05:34:14.000Z","updated":"2020-07-25T11:17:30.846Z","comments":true,"path":"2020/04/19/vim-editor-guide/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/04/19/vim-editor-guide/","excerpt":"设置鼠标模式set mouse&#x3D;a","text":"设置鼠标模式set mouse&#x3D;a 替换文本将 foo 替换为 bar，全局替换：:%s&#x2F;foo&#x2F;bar&#x2F;g 当前行替换 foo 为 bar：:s&#x2F;foo&#x2F;bar&#x2F;g 替换每一个 foo 为 bar，但需要确认：:%s&#x2F;foo&#x2F;bar&#x2F;gc 单词匹配替换，需要确认：:%s&#x2F;\\&lt;foo\\&gt;&#x2F;bar&#x2F;gc 忽略 foo 大小写，替换为 bar，需确认：:%s&#x2F;foo&#x2F;bar&#x2F;gci","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"OS","slug":"CS-Soft/OS","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/"},{"name":"UNIX","slug":"CS-Soft/OS/UNIX","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/OS/UNIX/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://winyter.github.io/MyBlog/tags/Linux/"},{"name":"VIM","slug":"VIM","permalink":"https://winyter.github.io/MyBlog/tags/VIM/"}]},{"title":"Hive操作集锦【持续更新】","slug":"hive-commands-collection","date":"2020-04-19T05:30:29.000Z","updated":"2020-07-25T11:16:51.205Z","comments":true,"path":"2020/04/19/hive-commands-collection/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/04/19/hive-commands-collection/","excerpt":"Hive操作集锦【持续更新】","text":"Hive操作集锦【持续更新】 Hive删库操作# 强制删库# test_db为库名drop database if exists test_db cascade; Hive删表(完全删除,包括表数据和表结构)# test_1 替换成实际的表名drop table if exists test_1; Hive删表数据操作(保留表结构)分为分区表和非分区表 1.分区表删法分区表无法在beeline客户端内使用sql删除，需要进入hdfs，直接删除分区下的数据文件 hdfs dfs -rmr &#x2F;hive&#x2F;test_1&#x2F;date_partition&#x3D;1585065600&#x2F;* 2.非分区表删法即普通表删除方法： # test 替换成实际的表名，前后两个都要替换insert overwrite table test select * from test where 1&#x3D;2; Hive导出表数据到本地insert overwrite local directory &#39;&#x2F;home&#x2F;hive&#x2F;data&#39; row format delimited fields terminated by &#39;,&#39; select * from test_1","categories":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/"},{"name":"Bigdata","slug":"CS-Concept/Bigdata","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://winyter.github.io/MyBlog/tags/Hive/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://winyter.github.io/MyBlog/tags/Hadoop/"}]},{"title":"华为FusionInsight大数据集群关系型数据库——Libra入门","slug":"FusionInsight-Libra-introduction","date":"2020-01-07T16:46:51.000Z","updated":"2020-07-25T11:04:40.515Z","comments":true,"path":"2020/01/08/FusionInsight-Libra-introduction/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/01/08/FusionInsight-Libra-introduction/","excerpt":"Libra简介LibrA是一个基于开源数据库Postgres-XC开发的分布式并行关系型数据库系统。由于是关系型数据库，它拥有所有关系型数据库的特性及模型，在使用上，我们甚至可以简单的把它看成是一个和Oracle、MySQL一样的数据库。当然，在实际的架构上和业务方式上还是有所区别的。","text":"Libra简介LibrA是一个基于开源数据库Postgres-XC开发的分布式并行关系型数据库系统。由于是关系型数据库，它拥有所有关系型数据库的特性及模型，在使用上，我们甚至可以简单的把它看成是一个和Oracle、MySQL一样的数据库。当然，在实际的架构上和业务方式上还是有所区别的。 Libra架构LibrA由多个MPPDBServer组成，LibrA结构具体如图1所示 模块说明MPPDBServer(CM)LibrA集群管理模块CM（Cluster Manager），即含CMServer进程的特殊MPPDBServer，负责管理和监控分布式系统中各个功能单元和物理资源的运行情况，确保整个系统的稳定运行。CM分为主CM和备CM。正常情况下，只由主CM提供LibrA集群管理服务。当主CM发生故障的情况下，备CM会主动升为主CM提供LibrA集群管理服务。 CM通过CM服务完成对各个MPPDBServer管理。CM服务由CMServer、CMAgent、Monitor组成。 CMServer是整个LibrA集群的大脑，它会根据CMAgent上报上来的各MPPDBServer状态信息来决定是否需要状态变更。CMServer只部署在主备CM上。 CMAgent是MPPDBServer上面部署的实例代理线程，负责接收CMServer下发的命令和上报MPPDBServer的Coordinator、Datanode、GTM的状态给CMServer。每个MPPDBServer均会部署一个CMAgent，也包括主备CM和主备GTM。 Monitor是watchdog定时任务，其唯一的任务是在CMAgent停止的情况下将CMAgent重启。每个MPPDBServer均会部署一个Monitor，也包括主备CM和主备GTM。 MPPDBServer(GTM)全局事务管理模块GTM（Global Transaction Manager），即含GTM进程的特殊MPPDBServer，负责生成和维护全局事务ID、事务快照、时间戳等需要全局唯一的信息。GTM分为主GTM和备GTM。正常情况下，只由主GTM提供全局事务管理服务。当主GTM发生故障的情况下，备GTM会主动升为主GTM提供服务。 MPPDBServer业务模块，即除MPPDBServer(CM)、MPPDBServer(GTM)以外的MPPDBServer，由Coordinator及多个Datanode组成，负责执行CM、GTM下发的任务。业务模块与MPPDBServer(CM)、MPPDBServer(GTM)主要区别是不包含CMServer与GTM进程。 Coordinator负责提供外部应用接口、优化全局执行计划、向Datanode分发执行计划，以及汇总、处理执行结果。 Datanode负责存储业务数据、执行数据查询任务以及向Coordinator返回执行结果。 Datanode实例分为主Datanode实例、备Datanode实例和从备Datanode实例，它们之间的工作原理如下： 主、备实例间可以正常同步数据时，主实例不会同步数据到从备实例。主、备实例间无法正常同步数据时，主实例会将数据同步到从备实例。主、备实例间数据同步恢复正常后，主实例会将异常期间的数据同步到备实例上，并在完成后知会从备实例清空之前同步的这部分数据。备实例同步主实例数据期间，如果主实例发生故障不可用，备实例将升为主实例，并在升为主实例成功后从备实例上同步之前异常期间的数据。 Storage服务器的本地存储资源，持久化存储数据（支持行存、列存、混合存储）。 Libra数据查询流程作为关系型数据库系统，LibrA主要业务为数据的查询与存储。LibrA进行数据查询的流程如图2所示。 具体查询流程如下：1、用户通过应用程序发出查询本地数据的SQL请求到Coordinator。2、Coordinator接收用户的SQL请求，分配服务进程，向GTM请求分配全局事务信息。3、GTM接收到Coordinator的请求，返回全局事务信息给Coordinator。4、Coordinator根据数据分布信息以及系统元信息，解析SQL为查询计划树，从查询计划树中提取可以发送到Datanode的执行步骤，封装成SQL语句或者子执行计划树，发送到Datanode执行。5、Datanode接收到读取任务后，查询具体Storage上的本地数据块。6、Datanode任务执行后，将执行结果返回给Coordinator。7、Coordinator将查询结果通过应用程序返回给用户。LibrA数据存储流程与数据查询流程相近，请参考数据查询流程，此处不再介绍。 Libra客户端的使用命令行客户端的使用操作步骤：1、登录安装了客户端的节点及用户2、执行环境变量语句 source &lt;环境变量文件&gt; 3、执行安全认证（如果使用的是非安全认证的集群，本步骤可以跳过）进入 认证文件所在目录 eg.&#x2F;home&#x2F;websrv&#x2F;keytab&#x2F;hbase 查看认证用户名： klist -kt user.keytab 执行认证： kinit -kt user.keytab &lt;组件业务用户名&gt; --&lt;组件业务用户名&gt;即上一步看到的 认证用户名 eg. kinit -kt user.keytab admin@HBASE.COM 4、登录Libra集群命令行交互 gsql –U &lt;用户名&gt; -W &lt;密码&gt; -d &lt;数据库名&gt; -p &lt;数据库端口号&gt; -h &lt;集群主Master所在节点IP&gt;eg.gsql -U winter -W winter -d myblog -p 25308 -h 111.111.111.111 5、Libra个性化操作命令Libra有着完备的SQL支持，只需要使用与其他关系型数据库相同的SQL语句即可对Libra数据库做增删改查操作，但Libra客户端有一些基础的操作命令，可以帮助用户简单快捷的对数据库做操作： Informational (options: S &#x3D; show system objects, + &#x3D; additional detail) \\d[S+] list tables, views, and sequences \\d[S+] NAME describe table, view, sequence, or index \\da[S] [PATTERN] list aggregates \\db[+] [PATTERN] list tablespaces \\dc[S+] [PATTERN] list conversions \\dC[+] [PATTERN] list casts \\dd[S] [PATTERN] show object descriptions not displayed elsewhere \\ddp [PATTERN] list default privileges \\dD[S+] [PATTERN] list domains \\ded[+] [PATTERN] list data sources \\det[+] [PATTERN] list foreign tables \\des[+] [PATTERN] list foreign servers \\deu[+] [PATTERN] list user mappings \\dew[+] [PATTERN] list foreign-data wrappers \\df[antw][S+] [PATRN] list [only agg&#x2F;normal&#x2F;trigger&#x2F;window] functions \\dF[+] [PATTERN] list text search configurations \\dFd[+] [PATTERN] list text search dictionaries \\dFp[+] [PATTERN] list text search parsers \\dFt[+] [PATTERN] list text search templates \\dg[+] [PATTERN] list roles \\di[S+] [PATTERN] list indexes \\dl list large objects, same as \\lo_list \\dL[S+] [PATTERN] list procedural languages \\dn[S+] [PATTERN] list schemas \\do[S] [PATTERN] list operators \\dO[S+] [PATTERN] list collations \\dp [PATTERN] list table, view, and sequence access privileges \\drds [PATRN1 [PATRN2]] list per-database role settings \\ds[S+] [PATTERN] list sequences \\dt[S+] [PATTERN] list tables \\dT[S+] [PATTERN] list data types \\du[+] [PATTERN] list roles \\dv[S+] [PATTERN] list views \\dE[S+] [PATTERN] list foreign tables \\dx[+] [PATTERN] list extensions \\l[+] list all databases \\sf[+] FUNCNAME show a function&#39;s definition \\z [PATTERN] same as \\dp 图形化客户端的使用由于Libra是基于Postgres进行开发的，所以所有支持Postgres的图形化客户端均能对Libra进行操作(例如：Navicat)，这里推荐一个客户端——Data Studio，供参考使用下面简单讲一讲该软件的使用方法： ①此处为连接按钮，点击后，会有新建连接信息的弹窗，按照要求，填写相关连接信息即可登录；②Data Studio支持对表做一些操作，右击相应的表，即可根据菜单，对表做操作；③此处为SQL命令输入框；④此处为结果反馈显示区域；⑤该按钮是执行语句的按钮，Data Studio对语句执行的操作与PL/SQL对语句的操作基本一样：对于鼠标选中的预警进行执行，如果没有选中，则执行整个命令框内的所有命令；⑥Data Studio支持表修改操作自动提交，该功能默认为开启，如果想关闭，可以点击⑥处的按钮进行关闭，在对生产环境进行操作时，为了保证数据的安全，建议将该功能关闭，手动对表修改进行提交。","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"HW-FusionInsight","slug":"CS-Soft/Server/HW-FusionInsight","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/HW-FusionInsight/"}],"tags":[{"name":"Libra","slug":"Libra","permalink":"https://winyter.github.io/MyBlog/tags/Libra/"},{"name":"FusionInsight","slug":"FusionInsight","permalink":"https://winyter.github.io/MyBlog/tags/FusionInsight/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://winyter.github.io/MyBlog/tags/PostgreSQL/"},{"name":"MPP","slug":"MPP","permalink":"https://winyter.github.io/MyBlog/tags/MPP/"},{"name":"BigData","slug":"BigData","permalink":"https://winyter.github.io/MyBlog/tags/BigData/"}]},{"title":"Hadoop快速入门系列——篇五：Kafka","slug":"hadoop-kafka","date":"2020-01-07T16:46:17.000Z","updated":"2020-07-25T11:16:30.640Z","comments":true,"path":"2020/01/08/hadoop-kafka/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/01/08/hadoop-kafka/","excerpt":"Kafka简介Kafka是用于构建实时数据管道和数据流的应用程序。具有实时横向扩展、高吞吐量、支持大量堆积具有容错性和速度快等特点。它是一个高性能分布式消息系统。","text":"Kafka简介Kafka是用于构建实时数据管道和数据流的应用程序。具有实时横向扩展、高吞吐量、支持大量堆积具有容错性和速度快等特点。它是一个高性能分布式消息系统。 Kafka架构及业务实现原理 生产者（Producer）将消息发布到Kafka主题（Topic），消费者（Consumer）订阅这些主题并消费这些消息。在Kafka集群上一个服务器称为一个Broker。对于每一个主题，Kafka集群保留一个用于缩放、并行化和容错性的分区（Partition）。每个分区是一个有序、不可变的消息序列，并不断追加到提交日志文件。分区的消息每个也被赋值一个称为偏移顺序（Offset）的序列化编号。 消费者使用一个消费者组名称来标记自己，主题的每个消息被传递给每个订阅消费者组中的一个消费者。如果所有的消费者实例都属于同样的消费组，它们就像传统队列负载均衡方式工作。如上图中，Consumer1与Consumer2之间为负载均衡方式；Consumer3、Consumer4、Consumer5与Consumer6之间为负载均衡方式。如果消费者实例都属于不同的消费组，则消息会被广播给所有消费者。如上图中，Topic1中的消息，同时会广播到Consumer Group1与Consumer Group2中。 结构图说明Broker在Kafka集群上一个服务器称为一个Broker。Topic/主题一个Topic就是一个类别或者一个可订阅的条目名称，也即一类消息。一个主题可以有多个分区，这些分区可以作为并行的一个单元。Partition/分区是一个有序的、不可变的消息序列，这个序列可以被连续地追加—个提交日志。在分区内的每条消息都有一个有序的ID号，这个ID号被称为偏移（Offset），这个偏移量可以唯一确定每条消息在分区内的位置。Producer/生产者向Kafka的主题发布消息。Consumer/消费者向Topic订阅，并且接收发布到这些Topic的消息。 Kafka特性消息可靠性Kafka Broker收到消息后，会持久化到磁盘，同时，Topic的每个Partition有自己的Replica（备份），每个Replica分布在不同的Broker节点上，以保证当某一节点失效时，可以自动故障转移到可用消息节点。 高吞吐量Kafka通过以下方式提供系统高吞吐量：数据磁盘持久化：消息不在内存中cache，直接写入到磁盘，充分利用磁盘的顺序读写性能。Zero-copy：减少IO操作步骤。数据批量发送：提高网络利用率。Topic划分为多个Partition，提高并发度，可以由多个Producer、Consumer数目之间的关系并发来读、写消息。Producer根据用户指定的算法，将消息发送到指定的Partition。 消息订阅-通知机制消费者对感兴趣的主题进行订阅，并采取pull的方式消费数据，使得消费者可以根据其消费能力自主地控制消息拉取速度，同时，可以根据自身情况自主选择消费模式，例如批量、重复消费，从尾端开始消费等；另外，需要消费者自己负责维护其自身消息的消费记录。 可扩展性当在Kafka集群中可通过增加Broker节点以提供更大容量时。新增的Broker会向Zookeeper注册，而Producer及Consumer会及时从Zookeeper感知到这些变化，并及时作出调整。 Kafka操作指南Kafka命令精要首先需要声明，由于Kafka很多业务需要有Zookeeper去完成，所以Kafka的操作命令中，有一部分需要用到Zookeeper组件的节点IP，在实际使用中，需要注意哪些命令使用Kafka节点IP，哪些使用Zookeeper节点IP，如果使用混了，很容易会导致查询出错。另外，还需要关注Kafka的不同端口，比如一些带有安全认证的集群，有安全端口和非安全端口的区别。 查看当前集群Topic列表。 .&#x2F;kafka-topics.sh --list --zookeeper &lt;ZooKeeper集群IP:24002&#x2F;kafka&gt; 查看单个Topic详细信息。 .&#x2F;kafka-topics.sh --describe --zookeeper &lt;ZooKeeper集群IP:24002&#x2F;kafka&gt; --topic &lt;Topic名称&gt; 删除Topic，由管理员用户操作。 .&#x2F;kafka-topics.sh --delete --zookeeper &lt;ZooKeeper集群IP:24002&#x2F;kafka&gt; --topic &lt;Topic名称&gt; 创建Topic，由管理员用户操作。 .&#x2F;kafka-topics.sh --create --zookeeper &lt;ZooKeeper集群IP:24002&#x2F;kafka&gt; --partitions 6 --replication-factor 2 --topic &lt;Topic名称&gt; Old Producer API生产数据，服务端“allow.everyone.if.no.acl.found”配置为“True”。 .&#x2F;kafka-console-producer.sh --broker-list &lt;Kafka集群IP:21005&gt; --topic &lt;Topic名称&gt; --old-producer -sync Old Consumer API消费数据，服务端“allow.everyone.if.no.acl.found”配置为“True”。该命令从topic头部开始消费数据，也即从最老的数据向新数据开始消费 .&#x2F;kafka-console-consumer.sh --zookeeper &lt;ZooKeeper集群IP:24002&#x2F;kafka&gt; --topic &lt;Topic名称&gt; --from-beginning 赋Consumer权限命令，由管理员用户操作。 .&#x2F;kafka-acls.sh --authorizer-properties zookeeper.connect&#x3D;&lt;ZooKeeper集群IP:24002&#x2F;kafka &gt; --add --allow-principal User:&lt;用户名&gt; --consumer --topic &lt;Topic名称&gt; --group &lt;消费者组名称&gt; 赋Producer权限命令，由管理员用户操作。 .&#x2F;kafka-acls.sh --authorizer-properties zookeeper.connect&#x3D;&lt;ZooKeeper集群IP:24002&#x2F;kafka &gt; --add --allow-principal User:&lt;用户名&gt; --producer --topic &lt;Topic名称&gt; New Producer API生产消息，需要拥有该Topic生产者权限。 .&#x2F;kafka-console-producer.sh --broker-list &lt;Kafka集群IP:21007&gt; --topic &lt;Topic名称&gt; --producer.config config&#x2F;producer.properties New Consumer API消费数据，需要拥有该Topic的消费者权限，该命令是从topic尾部开始消费数据，也即从最新的数据向老数据开始消费 .&#x2F;kafka-console-consumer.sh --topic &lt;Topic名称&gt; --bootstrap-server &lt;Kafka集群IP:21007&gt; --new-consumer --consumer.config config&#x2F;consumer.properties 消费kafka数据并输出（通常用来查看topic中是否有数据，以及监控topic数据）： .&#x2F;kafka-console-consumer.sh --topic business_imperson --bootstrap-server 17.22.148.17:21005,17.22.148.18:21005,17.22.148.19:21005,17.22.148.20:21005 Tips：在已经运行了一段时间的现场环境中，如果有些命令如果不知道怎么使用，可以在命令行下，Ctrl+R，然后输入相应命令的关键词，以查找该命令的历史使用记录（该技巧不仅可以用于Kafka命令的查找，也可以用于其他命令）更多Kafka知识和命令介绍参考hadoop官网说明文档：http://kafka.apache.org/","categories":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/"},{"name":"Bigdata","slug":"CS-Concept/Bigdata","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://winyter.github.io/MyBlog/tags/Hadoop/"},{"name":"Kafka","slug":"Kafka","permalink":"https://winyter.github.io/MyBlog/tags/Kafka/"}]},{"title":"Hadoop快速入门系列——篇六：其他组件简介","slug":"hadoop-other","date":"2020-01-07T16:46:01.000Z","updated":"2020-07-25T11:16:34.189Z","comments":true,"path":"2020/01/08/hadoop-other/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/01/08/hadoop-other/","excerpt":"FlinkFlink是一个批处理和流处理结合的统一计算框架，其核心是一个提供了数据分发以及并行化计算的流数据处理引擎。它的最大亮点是流处理，是业界最顶级的开源流处理引擎。Flink最适合的应用场景是低时延的数据处理（Data Processing）场景：高并发pipeline处理数据，时延毫秒级，且兼具可靠性。","text":"FlinkFlink是一个批处理和流处理结合的统一计算框架，其核心是一个提供了数据分发以及并行化计算的流数据处理引擎。它的最大亮点是流处理，是业界最顶级的开源流处理引擎。Flink最适合的应用场景是低时延的数据处理（Data Processing）场景：高并发pipeline处理数据，时延毫秒级，且兼具可靠性。 Flink架构 Flink整个系统包含三个部分：ClientFlink Client主要给用户提供向Flink系统提交用户任务（流式作业）的能力。TaskManagerFlink系统的业务执行节点，执行具体的用户任务。TaskManager可以有多个，各个TaskManager都平等。JobManagerFlink系统的管理节点，管理所有的TaskManager，并决策用户任务在哪些Taskmanager执行。JobManager在HA模式下可以有多个，但只有一个主JobManager。 Flink系统特性：低时延提供ms级时延的处理能力。Exactly Once提供异步快照机制，保证所有数据真正只处理一次。HAJobManager支持主备模式，保证无单点故障。水平扩展能力TaskManager支持手动水平扩展。 Zookeeper在分布式框架中，分布式应用面临的最大的问题就是数据一致性。那么Zookeeper就是一个比较好的解决方案。在分布式框架中起到协调作用。 Zookeeper是什么zookeeper是高性能的分布式协作服务和分布式数据一致性解决方案，由雅虎创建，是Goole Chubby的开源实现。 zookeeper可以保证分布式一致性特性，包括顺序一致性、原子性、单一视图（无论客户端连接哪一个ZK服务器看到的都是一样的数据模型）、可靠性、实时性（在一定时间内客户端可以读取到最新数据状态而不是提交后所有服务器马上就全部更新。） zookeeper的数据模型是一个树形节点，服务启动后，所有数据加载到内存中这样来提高服务器吞吐并减少延迟。 Zookeeper结构 ZooKeeper集群中的节点分为三种角色：Leader、Follower和Observer，其结构和相互关系如图2-21所示。通常来说，需要在集群中配置奇数个（2N+1）ZooKeeper服务，至少（N+1）个投票才能成功的执行写操作。Leader：ZK集群的核心角色，通过选举产生，为客户端提供读写服务，也就是处理事务请求Follower：集群状态的跟随者，参加选举，没有被选上就是这个角色，提供读取服务，也就是处理非事务请求，对于收到的事务请求会转发给Leader服务器Observer：观察者角色，不参加选举，但是提供数据读取服务，提供读取服务，也就是处理非事务请求，对于收到的事务请求会转发给Leader服务器。 YarnYARN是Hadoop2.0中的资源管理系统，是一个通用的资源管理模块，可以为各类应用程序进行资源管理和调度。YARN起源于Hadoop1.0中的MRv1，起初这个模块仅用来作为MapReduce的资源管理工具，后来逐步发展，在Hadoop2.0中，该模块也成为了其他计算框架的同意资源调度工具。YARN主要包括ResourceManager、ApplicationMaster与NodeManager三个部分。 结构图说明：ClientYARN Application客户端，用户可以通过客户端向ResourceManager提交任务，查询Application运行状态等。ResourceManager(RM)负责集群中所有资源的统一管理和分配。接收来自各个节点（NodeManager）的资源汇报信息，并根据收集的资源按照一定的策略分配给各个应用程序。 RM是一个全局的资源管理器，负责整个系统的资源管理和分配。主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager）。 调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念Container表示。Container是一个动态资源分配单位，将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需要设计新的调度器，YARN提供了多种直接可用的调度器，比如Fair Scheduler和Capacity Scheduler等。 应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动等。 NodeManager(NM)NodeManager（NM）是YARN中每个节点上的代理，管理Hadoop集群中单个计算节点，包括与ResourceManger保持通信，监督Container的生命周期管理，监控每个Container的资源使用（内存、CPU等）情况，追踪节点健康状况，管理日志和不同应用程序用到的附属服务（auxiliary service）。 NM是每个节点上的资源和任务管理器，一方面，会定时向RM汇报本节点上的资源使用情况和各个Container的运行状态；另一方面，接收并处理来自AM的Container启动/停止等请求。 ApplicationMaster(AM)即图中的App Mstr，负责一个Application生命周期内的所有工作。包括：与RM调度器协商以获取资源；将得到的资源进一步分配给内部任务（资源的二次分配）；与NM通信以启动/停止任务；监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。 AM负责一个Application生命周期内的所有工作。包括： 与RM调度器协商以获取资源。 将得到的资源进一步分配给内部的任务(资源的二次分配)。 与NM通信以启动/停止任务。 监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。 ContainerContainer是YARN中的资源抽象，封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等（目前仅封装内存和CPU），当AM向RM申请资源时，RM为AM返回的资源便是用Container表示。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。 RedisRedis，本质上上一个KEY-VALUE类型的内存数据库，整个数据库都加载在内存当中进行操作，定期通过异步操作把数据库数据flush到硬盘上进行保存。因此它是纯内存操作，Redis的性能非常出色，每秒可以处理超过10万次读写操作。虽然是内存数据库，但是其数据可以持久化，而且支持丰富的数据类型。 Redis支持保存LIST列表和SET集合的数据结构，而且还支持对LIST进行各种操作，例如从LIST两端进行PUSH和POP数据，取LIST区间，排序等等。对SET支持各种集合的并集交集操作，单个value的最大限制是1GB。 Redis主要的缺点是受到物理内存限制，不能用作海量数据的高性能读写，而且它没有原生的可扩展机制，不具有扩展能力，要依赖客户端来实现分布式读写，因此其适合的应用场景主要局限在较小数据量的高性能操作和运算上。 键值类型的数据库主要使用哈希表，这个表中有一个特定的键和一个指针指向特定数据。KEY/VALUE模型对于IT系统来说的优势在于简单、容易部署。主要特点是具有极高的并发读写性能。 Redis逻辑架构 Redis Server：Redis组件的核心模块，负责Redis协议的数据读写、数据持久化、主从复制、集群功能。Redis-WS：Redis WebService管理模块，主要负责Redis集群的创建、扩容、减容、查询、删除等操作，集群管理信息存入DB数据库。 Redis持久化Redis的所有数据都保存在内存中，然后不定期的通过异步方式保存到磁盘上（这称为半持久化）；也可以把每一次数据变化都写入到磁盘（这称为全持久化）。所谓持久化就是将内存数据转换为硬盘数据，内存模型到存储模型的转换，或者说是瞬时状态与持久状态的相互转换。Redis有两种持久化方式，默认是snapshot方式（RDB方式），实现方法是定时将内存的快照持久化到硬盘，这种方式的缺点是持久化之后如果出现crash则会丢失一段数据。另外一种是AOF方式，在写入内存数据的同时将操作命令保存到日志文件中。1、RDB方式：这种快照方式和虚拟机的快照一样，保存某一时刻的完整数据。Redis在使用这种方式做持久化的时候，定期（默认5分钟）会先写入到一个临时文件，写入完成后，会用这个文件去替换上次的旧的文件。这种方式的好处是，任何一次的快照文件都是完整可用的。但是缺点是，它每隔一段时间（默认最快1分钟，最慢15分钟）做一次，所以会存在一段时间的数据丢失。2、AOF方式：这种方式就是把对Redis内存数据的的写指令记录下来，这些指令会被记录在AOF文件的末尾，然后每秒做一次fsync操作（默认每秒一次），把指令在后台在执行一次执行过程其实就是修改磁盘上的数据库内容。所以如果出现故障也只丢失1秒的数据。 上面这种方式就很类似于传统数据库服务器的事务日志。 如果遇到在追加日志的时候遇到意外，可以使用redis-check-aof工具进行日志修复。 因为采用了追加方式，所以AOF会越来越大（这一点又和传统数据库不一样，传统数据库事务日志文件都比较小），因此redis有另外一个机制就是AOF文件重写，当AOF文件达到一个设定的阈值后，会自动启动AOF文件压缩，只保留可以恢复数据的最小指令集。 通过上面的对AOF的描述，可以看到AOF是一个面向过程的，而RDB是面向对象的。 AOF方式的优点： 丢失数据最小 AOF方式的缺点： 同等数据量，AOF文件比RDB文件体积大 AOF恢复速度比RDB方式慢 MapReduceMapReduce是一种简化并行计算的编程模型，名字源于该模型中的两项核心操作：Map和Reduce。Map将一个作业分解成为多个任务，Reduce将分解后多个任务处理的结果汇总起来，得出最终的分析结果。MapReduce的推出给大数据并行处理带来了巨大的革命性影响，使其已经成为事实上的大数据处理的工业标准,是到目前为止最为成功、最广为接受和最易于使用的大数据并行处理技术（批处理技术）。当然，随着技术的进步，以MapReduce为代表的传统批处理计算框架也逐渐被流处理、批流合一的计算框架取代。 MapReduce的原理Hadoop MapReduce的思想来源于Google在03、04年发表的两篇关于GFS和MapReduce的论文，在那篇MapReduce的论文中，Google表示，MapReduce的灵感来源于函数式语言（比如Lisp）中的内置函数map和reduce。在函数式语言里，map表示对一个列表（List）中的每个元素做计算，reduce表示对一个列表中的每个元素做迭代计算。它们具体的计算是通过传入的函数来实现的，map和reduce提供的是计算的框架。不过从这样的解释到现实中的MapReduce还太远，仍然需要一个跳跃。再仔细看，reduce既然能做迭代计算，那就表示列表中的元素是相关的，比如我想对列表中的所有元素做相加求和，那么列表中至少都应该是数值吧。而map是对列表中每个元素做单独处理的，这表示列表中可以是杂乱无章的数据。这样看来，就有点联系了。在MapReduce里，Map处理的是原始数据，自然是杂乱无章的，每条数据之间互相没有关系；到了Reduce阶段，数据是以key后面跟着若干个value来组织的，这些value有相关性，至少它们都在一个key下面，于是就符合函数式语言里map和reduce的基本思想了。 这样我们就可以把MapReduce理解为，把一堆杂乱无章的数据按照某种特征归纳起来，然后处理并得到最后的结果。Map面对的是杂乱无章的互不相关的数据，它解析每个数据，从中提取出key和value，也就是提取了数据的特征。经过MapReduce的Shuffle阶段之后，在Reduce阶段看到的都是已经归纳好的数据了，在此基础上我们可以做进一步的处理以便得到结果。这就回到了最初，终于知道MapReduce为何要这样设计。","categories":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/"},{"name":"Bigdata","slug":"CS-Concept/Bigdata","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://winyter.github.io/MyBlog/tags/Hadoop/"},{"name":"Redis","slug":"Redis","permalink":"https://winyter.github.io/MyBlog/tags/Redis/"},{"name":"Flink","slug":"Flink","permalink":"https://winyter.github.io/MyBlog/tags/Flink/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://winyter.github.io/MyBlog/tags/Zookeeper/"},{"name":"Yarn","slug":"Yarn","permalink":"https://winyter.github.io/MyBlog/tags/Yarn/"},{"name":"Mapreduce","slug":"Mapreduce","permalink":"https://winyter.github.io/MyBlog/tags/Mapreduce/"}]},{"title":"MacOS 安装 Hexo 时提示 EACCES 时的解决方法","slug":"ERRORS-hexo-MacOS-EACCES","date":"2020-01-05T16:19:44.000Z","updated":"2020-07-25T11:04:25.288Z","comments":true,"path":"2020/01/06/ERRORS-hexo-MacOS-EACCES/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/01/06/ERRORS-hexo-MacOS-EACCES/","excerpt":"该错误的解决方法在官方文档中有提到，但写的不甚详细，所以我在亲自测试后，写下详细步骤。","text":"该错误的解决方法在官方文档中有提到，但写的不甚详细，所以我在亲自测试后，写下详细步骤。 官方文档中提到的解决方法的链接 该文档提供了两个方法 使用 node 版本管理工具； 修改 npm 的默认路径 由于建议使用第一种方法，因此，下面的步骤就选择了第一种方法进行讲解。而每个平台有不同的 node 版本管理工具，这里建议使用 nvm 下载并安装 nvmcurl -o- https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;nvm-sh&#x2F;nvm&#x2F;v0.35.2&#x2F;install.sh | bash 或者使用： wget -qO- https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;nvm-sh&#x2F;nvm&#x2F;v0.35.2&#x2F;install.sh | bash 提供 NVM 的 github 地址，方便获取最新的 nvm 版本：点击这里 获取 nodejs 版本信息：nvm ls-remote 该命令会把 nodejs 所有版本打印出来，你可以按照需求选择你想要的版本 安装 nodejs你不需要担心你之前已经安装的 nodejs，nvm 会直接覆盖安装 nvm install v12.14.0 至此，EACCES 的问题就已经解决。","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Client","slug":"CS-Soft/Client","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Client/"},{"name":"Hexo","slug":"CS-Soft/Client/Hexo","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Client/Hexo/"}],"tags":[{"name":"ERROR","slug":"ERROR","permalink":"https://winyter.github.io/MyBlog/tags/ERROR/"},{"name":"Hexo","slug":"Hexo","permalink":"https://winyter.github.io/MyBlog/tags/Hexo/"},{"name":"Tool","slug":"Tool","permalink":"https://winyter.github.io/MyBlog/tags/Tool/"},{"name":"MacOS","slug":"MacOS","permalink":"https://winyter.github.io/MyBlog/tags/MacOS/"}]},{"title":"Hadoop快速入门系列——篇四：HDFS","slug":"hadoop-HDFS","date":"2020-01-05T16:17:10.000Z","updated":"2020-07-25T11:16:18.974Z","comments":true,"path":"2020/01/06/hadoop-HDFS/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/01/06/hadoop-HDFS/","excerpt":"HDFS简介Hadoop分布式文件系统（Hadoop Distributed File System）能提供高吞吐量的数据访问，适合大规模数据集方面的应用，为海量数据提供存储。","text":"HDFS简介Hadoop分布式文件系统（Hadoop Distributed File System）能提供高吞吐量的数据访问，适合大规模数据集方面的应用，为海量数据提供存储。 HDFS架构HDFS包含主、备NameNode和多个DataNode，如图1所示。 HDFS是一个Master/Slave的架构，在Master上运行NameNode，而在每一个Slave上运行DataNode，ZKFC需要和NameNode一起运行。 NameNode和DataNode之间的通信都是建立在TCP/IP的基础之上的。NameNode、DataNode、ZKFC和JournalNode能部署在运行Linux的服务器上。 模块说明NameNode用于管理文件系统的命名空间、目录结构、元数据信息以及提供备份机制等，分为： Active NameNode：管理文件系统的命名空间、维护文件系统的目录结构树以及元数据信息；记录写入的每个“数据块”与其归属文件的对应关系。 Standby NameNode：与Active NameNode中的数据保持同步；随时准备在Active NameNode出现异常时接管其服务。DataNode用于存储每个文件的“数据块”数据，并且会周期性地向NameNode报告该DataNode的数据存放情况。JournalNodeHA集群下，用于同步主备NameNode之间的元数据信息。ZKFCZKFC是需要和NameNode一一对应的服务，即每个NameNode都需要部署ZKFC。它负责监控NameNode的状态，并及时把状态写入Zookeeper。ZKFC也有选择谁作为Active NameNode的权利。ZK ClusterZooKeeper是一个协调服务，帮助ZKFC执行主NameNode的选举。HttpFS gatewayHttpFS是个单独无状态的gateway进程，对外提供webHDFS接口，对HDFS使用FileSystem接口对接。可用于不同Hadoop版本间的数据传输，及用于访问在防火墙后的HDFS(HttpFS用作gateway)。 HDFS原理 在HDFS内部，一个文件分成一个或多个“数据块”，这些“数据块”存储在DataNode集合里，NameNode负责保存和管理所有的HDFS元数据。客户端连接到NameNode，执行文件系统的“命名空间”操作，例如打开、关闭、重命名文件和目录，同时决定“数据块”到具体DataNode节点的映射。DataNode在NameNode的指挥下进行“数据块”的创建、删除和复制。客户端连接到DataNode，执行读写数据块操作。 HDFS与其他组件之间的关系HDFS是整个Hadoop系统的底层存储实现，与其他很多组件都有着交互，了解HDFS与他们之间的交互将有助于理解Hadoop整套系统的交互运作方式。 HDFS和HBase的配合关系HDFS是Apache的Hadoop项目的子项目，HBase利用Hadoop HDFS作为其文件存储系统。HBase位于结构化存储层，Hadoop HDFS为HBase提供了高可靠性的底层存储支持。除了HBase产生的一些日志文件，HBase中的所有数据文件都可以存储在Hadoop HDFS文件系统上。 MapReduce和HDFS的配合关系HDFS是Hadoop分布式文件系统，具有高容错和高吞吐量的特性，可以部署在价格低廉的硬件上，存储应用程序的数据，适合有超大数据集的应用程序。而MapReduce是一种编程模型，用于大数据集（大于1TB）的并行运算。在MapReduce程序中计算的数据可以来自多个数据源，如Local FileSystem、HDFS、数据库等。最常用的是HDFS，可以利用HDFS的高吞吐性能读取大规模的数据进行计算。同时在计算完成后，也可以将数据存储到HDFS。 Spark和HDFS的配合关系通常，Spark中计算的数据可以来自多个数据源，如Local File、HDFS等。最常用的是HDFS，用户可以一次读取大规模的数据进行并行计算。在计算完成后，也可以将数据存储到HDFS。分解来看，Spark分成控制端(Driver)和执行端（Executor）。控制端负责任务调度，执行端负责任务执行。 ZooKeeper和HDFS的配合关系 ZKFC（ZKFailoverController）作为一个ZooKeeper集群的客 户端，用来监控NameNode的状态信息。ZKFC进程仅在部署了NameNode的节点中存在。HDFS NameNode的Active和Standby节点均部署有zkfc进程。1、HDFS NameNode的ZKFC连接到ZooKeeper，把主机名等信息保存到ZooKeeper中，即“/hadoop-ha”下的znode目录里。先创建znode目录的NameNode节点为主节点，另一个为备节点。HDFS NameNode Standby通过ZooKeeper定时读取NameNode信息。2、当主节点进程异常结束时，HDFS NameNode Standby通过ZooKeeper感知“/hadoop-ha”目录下发生了变化，NameNode会进行主备切换。 HDFS操作指南HDFS命令需要在安装了HDFS命令环境的操作系统上才能使用，由于各家大数据集群产品客户端使用方法不尽相同，因此，这里不再介绍环境加载的方法，仅提供HDFS操作命令。 操作HDFS需要使用hadoop fs 或者 hdfs dfs命令，这两个命令没有实质上的区别，实际使用中可以随意选择，下面会简单介绍一些常用的命令使用方法，如果想要了解这两条命令的详细使用，可以在服务器上执行帮助命令： hadoop fs –help 或者 hdfs dfs –help命令举例：eg. hdfs dfs -ls &#x2F;tenant&#x2F; 常用的hadoop fs(或 hdfs dfs)命令： 命令 使用方法 作用 -ls -ls &lt;路径&gt; 查看指定路径的当前目录结构 -lsr -lsr &lt;路径&gt; 递归查看指定路径的目录结构 -du -du &lt;路径&gt; 统计目录下各文件的大小 -dus -dus &lt;路径&gt; 汇总统计目录下文件(夹)大小 -count -count [-q] &lt;路径&gt; 统计文件(夹)数量 -mv -mv &lt;源路径&gt; &lt;目的路径&gt; 移动 -cp -cp &lt;源路径&gt; &lt;目的路径&gt; 复制 -rm -rm [-skipTrash] &lt;路径&gt; 删除文件/空白文件夹 &lt;慎用，数据无价&gt; -rmr -rmr [-skipTrash] &lt;路径&gt; 递归删除 &lt;慎用，数据无价&gt; -put -put &lt;多个Linux上的文件&gt; &lt;hdfs路径&gt; 上传文件 -copyFromLocal -copyFromLocal &lt;多个Linux上的文件&gt; &lt;hdfs路径&gt; 从本地复制 -moveFromLocal -moveFromLocal &lt;多个Linux上的文件&gt; &lt;hdfs路径&gt; 从本地移动 -getmerge -getmerge &lt;源路径&gt; &lt;Linux路径&gt; 合并到本地 -cat -cat &lt;hdfs路径&gt; 查看文件内容 -text -text &lt;hdfs路径&gt; 查看文件内容 -copyToLocal -copyToLocal [-ignoreCrc] [-crc] [hdfs源路径] [Linux目的路径] 从本地复制 -moveToLocal -moveToLocal [-crc] &lt;hdfs源路径&gt; &lt;Linux目的路径&gt; 从本地移动 -mkdir -mkdir &lt;hdfs路径&gt; 创建空白文件夹 -setrep -setrep [-R] [-w] &lt;副本数&gt; &lt;路径&gt; 修改副本数量 -touchz -touchz &lt;文件路径&gt; 创建空白文件 -stat -stat [format] &lt;路径&gt; 显示文件统计信息 -tail -tail [-f] &lt;文件&gt; 查看文件尾部信息 -chmod -chmod [-R] &lt;权限模式&gt; [路径] 修改权限 -chown -chown [-R] [属主][:[属组]] &lt;路径&gt; 修改属主 -chgrp -chgrp [-R] &lt;属组名称&gt; &lt;路径&gt; 修改属组 -help -help [命令选项] 帮助","categories":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/"},{"name":"Bigdata","slug":"CS-Concept/Bigdata","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://winyter.github.io/MyBlog/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"https://winyter.github.io/MyBlog/tags/HDFS/"}]},{"title":"Hadoop快速入门系列——篇三：Hive","slug":"hadoop-hive","date":"2020-01-05T16:14:47.000Z","updated":"2020-07-25T11:16:25.216Z","comments":true,"path":"2020/01/06/hadoop-hive/","link":"","permalink":"https://winyter.github.io/MyBlog/2020/01/06/hadoop-hive/","excerpt":"Hive简介Hive是建立在Hadoop上的数据仓库框架，提供大数据平台批处理计算能力，能够对结构化/半结构化数据进行批量分析汇总完成数据计算。提供类似SQL的Hive Query Language语言操作结构化数据，其基本原理是将HQL语言自动转换成MapReduce任务，从而完成对Hadoop集群中存储的海量数据进行查询和分析。 Hive构建在关系型数据库结构之上，有着与Oracle、MySQL相似的数据结构，同时Hive还提供了类SQL语句进行数据库的增删改查，因此，一个习惯使用传统关系型数据库的操作者，也能无缝对接上Hive的使用。","text":"Hive简介Hive是建立在Hadoop上的数据仓库框架，提供大数据平台批处理计算能力，能够对结构化/半结构化数据进行批量分析汇总完成数据计算。提供类似SQL的Hive Query Language语言操作结构化数据，其基本原理是将HQL语言自动转换成MapReduce任务，从而完成对Hadoop集群中存储的海量数据进行查询和分析。 Hive构建在关系型数据库结构之上，有着与Oracle、MySQL相似的数据结构，同时Hive还提供了类SQL语句进行数据库的增删改查，因此，一个习惯使用传统关系型数据库的操作者，也能无缝对接上Hive的使用。 Hive架构HiveServer：一个集群内可部署多个HiveServer，负荷分担。对外提供Hive数据库服务，将用户提交的HQL语句进行编译，解析成对应的Yarn任务或者HDFS操作，从而完成数据的提取、转换、分析。 MetaStore：一个集群内可部署多个MetaStore，负荷分担。提供Hive的元数据服务，负责Hive表的结构和属性信息读、写、维护和修改。提供Thrift接口，供HiveServer、Spark、WebHCat等MetaStore客户端来访问，操作元数据。 WebHCat：一个集群内可部署多个WebHCat，负荷分担。提供Rest接口，通过Rest执行Hive命令，提交MapReduce任务。 Hive客户端：包括人机交互命令行Beeline、提供给JDBC应用的JDBC驱动、提供给Python应用的Python驱动、提供给Mapreduce的HCatalog相关JAR包。 ZooKeeper集群：Zookeeper作为临时节点记录各HiveServer实例的IP地址列表，客户端驱动连接Zookeeper获取该列表，并根据路由机制选取对应的HiveServer实例。 HDFS/HBase集群：Hive表数据存储在HDFS集群中。 MapReduce/Yarn集群：提供分布式计算服务：Hive的大部分数据操作依赖MapReduce，HiveServer的主要功能是将HQL语句转换成MapReduce任务，从而完成对海量数据的处理。 Hive客户端的使用由于Hive有着与SQL几乎一样的查询语句，故本节将不再讲述Hive数据库基础的增删改查命令，如果对命令有所疑问，建议查阅SQL相关资料。本节将着重介绍基于天津现场华为集群的Hive Beeline客户端的使用，以及Hive客户端一些高级特性，开源的Hive组件的使用也能从本文中找到启发。 Hive Beeline客户端使用步骤1、登陆到客户端所在服务器2、安全认证(如果有的话)3、登陆客户端 beeline或spark-sql 4、执行操作命令，Hive操作命令与sql语言几乎一样，一般的增删改查操作均可使用sql语句完成，同时，Hive数据的组织和展示形式，与关系型数据库相似。个别命令举例： 查看数据库：show databases;进入某个数据库：use &lt;database_name&gt;;查看所有表：show tables;查看某一表结构：show create table &lt;table_name&gt;; Hive的“分区剪枝”特性我们知道，在Oracle中，面对大数据量的情况下，一张表无法存储海量数据，而通过以时间分区来切分一张表，则能实现逻辑上的一张表存储海量数据的能力，其实，前面讲到的HBase中也有类似的特性(Region)，而Hive也拥有这样的能力，Hive表中，使用date_partition来实现Hive分区，date_partition会从文件存储层就将数据进行分离，如下图为某张表数据存储在HDFS上的结构： 从图中可以看到，一张表的目录下，还会根据date_partition再分目录切分Hive表的数据文件，这会给Hive表的分区剪枝功能带来实质性的性能提升，而且，在有些查询场景数据量过大的情况下，甚至会导致Hive语句的执行失败，下图展示了未启用分区剪枝的大数据查询场景下出现的报错情况： 未启用分区剪枝，报错原因主要是由于大数据量的查询会使提交的数据超过设置的输入数据限制大小： 已启用分区剪枝： 同时，在这张图中还能看到，Hive的查询是调用了MapReduce，同时，还显示出了执行语句所消耗的CPU性能。 查询时分区剪枝功能的启用：在查询时，如果需要启用分区剪枝功能，需要在查询语句的where子句中，添加 date_partition between &lt;start_partition&gt; and &lt;end_partition&gt; 且必须注意：该条件必须放在where子句中所有条件中的第一个，否则分区剪枝功能将不会启用。 Hive删表操作因Hive基于HDFS建表，Hive中直接drop操作删表会报错。以网吧流程为例，正确的删表方式是如下： 1、查询建表语句 show create table &lt;table_name&gt;找到hive表的location位置 2、hadoop客户端中执行命令删除/&lt;table_name&gt;文件目录即可注意：每个现场的Hive表文件的HDFS目录结构均不一样，删除之前，需要进行确认，以免误删，另外，删表操作生产环境，务必谨慎操作！下图展示了Hive数据在HDFS上的存储结构，其中，1为库名，2为表名，3为分区名，4为实际存储的数据文件，下图展示的是parquet存储格式:","categories":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/"},{"name":"Bigdata","slug":"CS-Concept/Bigdata","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://winyter.github.io/MyBlog/tags/Hive/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://winyter.github.io/MyBlog/tags/Hadoop/"}]},{"title":"Hadoop快速入门系列——篇二：ElasticSearch","slug":"hadoop-elasticsearch","date":"2019-12-26T16:35:11.000Z","updated":"2020-07-25T11:16:10.461Z","comments":true,"path":"2019/12/27/hadoop-elasticsearch/","link":"","permalink":"https://winyter.github.io/MyBlog/2019/12/27/hadoop-elasticsearch/","excerpt":"ES简介ES数据模型","text":"ES简介ES数据模型 Index：即索引，是Elasticsearch中一个逻辑命名空间，指向一个或多个分片，内部Apache Lucene实现索引中数据的读写。索引与关系数据库实例Database相当。一个Elasticsearch实例可以包含多个索引。Type：文档类型，文档类型使得同一个索引中在存储结构不同的文档时，只需要依据文档类型就可以找到对应的参数映射信息，方便文档的存储。相当于数据库中的Table。一个索引对应一个文档类型。Document：文档，是可以被索引的基本单位，特指最顶层结构或根对象序列化成的JSON数据。相当于数据库中的Row。一个类型包含多个文档。Field：字段，组成文档的最小单位。相当于数据库中的Column。每个文档包含多个字段。Mapping：相当于数据库中的schema，用来约束字段的类型，不过 Elasticsearch 的 mapping 可以自动根据数据创建 ES架构EsNode：Elasticsearch节点，一个节点就是一个Elasticsearch实例。EsMaster：主节点，可以临时管理集群级别的一些变更，例如新建或删除索引、增加或移除节点等。主节点不参与文档级别的变更或搜索，在流量增长时，该主节点不会成为集群的瓶颈。primary shard：主分片，索引中的每个文档属于一个单独的主分片，主分片的数量决定了索引最多能存储多少数据。replica shard：即复制分片，它是主分片的一个副本，可以防止硬件故障导致的数据丢失，同时可以提供读请求，比如搜索或者从别的shard取回文档。recovery：代表数据恢复或叫数据重新分布，Elasticsearch在有节点加入或退出时会根据机器的负载对索引分片进行重新分配，挂掉的节点重新启动时也会进行数据恢复。gateway：代表Elasticsearch索引快照的存储方式，默认是先把索引存放到内存中，当内存满了时再持久化到本地硬盘。gateway对索引快照进行存储，当这个Elasticsearch集群关闭再重新启动时就会从gateway中读取索引备份数据。支持多种类型的gateway，有本地文件系统（默认），分布式文件系统，Hadoop的HDFS和amazon的s3云存储服务。transport：代表Elasticsearch内部节点或集群与客户端的交互方式，默认内部是使用tcp协议进行交互，同时它支持http协议（json格式）、thrift、servlet、memcached、zeroMQ等的传输协议（通过插件方式集成）。ZooKeeper：它在Elasticsearch是必须的，提供安全认证信息的存储等功能。shard（分片）：是工作单元(worker unit) 底层的一员，用来分配集群中的数据，它只负责保存索引中所有数据的一小片。 分片是一个独立的Lucene实例，并且它自身也是一个完整的搜索引擎。 文档存储并且被索引在分片中，但是我们的程序并不会直接与它们通信。取而代之，它们直接与索引进行通信的 把分片想象成一个数据的容器。数据被存储在分片中，然后分片又被分配在集群的节点上。当你的集群扩展或者缩小时，elasticsearch 会自动的在节点之间迁移分配分片，以便集群保持均衡 分片分为 主分片(primary shard) 以及 从分片(replica shard) 两种。在你的索引中，每一个文档都属于一个主分片 从分片只是主分片的一个副本，它用于提供数据的冗余副本，在硬件故障时提供数据保护，同时服务于搜索和检索这种只读请求 索引中的主分片的数量在索引创建后就固定下来了，但是从分片的数量可以随时改变。 一个索引默认设置了5个主分片，每个主分片有一个从分片对应 ES环境加载1、登录安装了ES客户端的节点2、登录相应用户3、加载环境变量 eg.source &#x2F;home&#x2F;websrv&#x2F;client&#x2F;hadoopClient&#x2F;bigdata_env 4、执行安全认证（如果使用的是非安全认证的集群，本步骤可以跳过）进入 认证文件所在目录 eg.&#x2F;home&#x2F;websrv&#x2F;keytab&#x2F;hbase 查看认证用户名： klist -kt user.keytab 执行认证： kinit -kt user.keytab &lt;组件业务用户名&gt; --&lt;组件业务用户名&gt;即上一步看到的 认证用户名 eg. kinit -kt user.keytab tjhj_zxsk_jj@HBASE.COM 然后再登录hbase shell即可 拓展：klist ——&gt; 查看认证时间 5、使用curl命令进行操作命令使用方法参见下一节的介绍 ES命令的使用ES 的命令主要是curl命令。ElasticSearch的命令调用是基于http的，提供了丰富的RESTFul API，从功能上来分可以分为3类： (1) 获取集群基础信息； (2) 基本的数据增删改命令；(3) ES搜索/查询命令，以及高级的query、filter、聚合搜索操作；其中，对于ES数据的搜索时本节讲述的重点，也是日常维护工作中使用最频繁的ES命令。ES的搜索命令从命令格式上来划分有两种方式：一是通过RESTfulrequest API传递查询参数，也称“query-string”；另一个是通过发送REST request body，也称作JSON格式。 # query-string格式：curl -XGET ‘192.168.226.133:9200&#x2F;my_index&#x2F;my_type&#x2F;_search?pretty’# JSON格式：curl - XGET &#39;localhost:9200&#x2F;my_index&#x2F;my_type&#x2F;_search?pretty&#39; -d&#39; &#123; &quot;query&quot;: &#123;&quot;match_all&quot;: &#123;&#125; &#125; &#125;&#39; 由于JSON格式更加友好美观，所以目前大部分对ES的查询命令都使用该格式表达，本节也会以这种格式介绍为主，所以在介绍详细命令的使用之前，会先介绍基本的JSON语法原则和基本CURL命令格式。 JSON基本语法原则 数据在键/值对中：JSON 数据的书写格式是：键/值对。键/值对包括字段名称（在双引号中），后面写一个冒号，然后是值：&quot;firstName&quot; : &quot;John&quot; 数据由逗号分隔 花括号保存对象：JSON 对象在花括号中书写，对象可以包含多个键/值对，多个键值对用逗号分隔：&#123; &quot;firstName&quot;:&quot;John&quot; , &quot;lastName&quot;:&quot;Doe&quot; &#125; 方括号保存数组：JSON 数组在方括号中书写，数组可包含多个对象：&#123;&quot;employees&quot;: [&#123; &quot;firstName&quot;:&quot;John&quot; , &quot;lastName&quot;:&quot;Doe&quot; &#125;,&#123; &quot;firstName&quot;:&quot;Anna&quot; , &quot;lastName&quot;:&quot;Smith&quot; &#125;,&#123; &quot;firstName&quot;:&quot;Peter&quot; , &quot;lastName&quot;:&quot;Jones&quot; &#125;]&#125; 基础curl命令curl [options…] &lt;url&gt; 常用操作符： -XGET --指定本次命令为查询请求 -XPUT --指定本次命令为新增修改请求 -XDELETE --指定本次命令为删除请求 ### 上面三个操作符必须选择一个，具体如何选择根据实际的增删改查操作需求 --negotiate --使用HTTP身份认证，如果使用HTTP协议方式连接，该操作符必选 -k --允许不使用证书到SSL站点，天津现场建议选择，其他现场按需选择 -v --显示版本信息 -u --设置服务器的用户名和密码 -H --自定义头信息传递给服务，默认即可 -d --HTTP POST方式传送数据，此处指文档内容以上操作符为常用的curl命令操作符，如果有其它操作符需求，可以在命令行键入：curl –help 获取集群基础信息获取集群健康状态 curl -XGET --negotiate -k -v -u : https:&#x2F;&#x2F;ip:httpport&#x2F;_cluster&#x2F;health?pretty 注意：ip:httpport需要填写ES集群中一个节点的IP地址，不需要填写所有的节点IP，ES集群会自行查询整个集群，httpport是该节点上已安装的任意ES实例的HTTP端口，需要查询确定查询所有索引 curl -XGET --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;_cat&#x2F;indices?v&#39; 查看指定索引的mapping curl -XGET --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;索引名&#x2F;_mapping?pretty&#39; 查看指定索引的setting curl -XGET --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;索引名&#x2F;_settings?pretty&#39; --setting 即该索引的结构 检查文档是否存在 curl -i -XHEAD --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;索引名&#x2F;文档类型&#x2F;索引ID&#39; 基本的数据增删改命令新建索引，并设置副本数 curl -XPUT --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;索引名?pretty&#39; -H &#39;Content-Type: application&#x2F;json&#39; -d&#39; &#123;&quot;settings&quot; : &#123;&quot;number_of_shards&quot; : 分片数量,&quot;number_of_replicas&quot; : 副本数量&#125; &#125;&#39;例如：curl -XPUT --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;1.1.1.1:24148&#x2F;website?pretty&#39; -H &#39;Content-Type: application&#x2F;json&#39; -d&#39; &#123;&quot;settings&quot; : &#123;&quot;number_of_shards&quot; : 3,&quot;number_of_replicas&quot; : 1&#125; &#125;&#39; 写入数据，更新索引 curl -XPOST --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;索引名&#x2F;文档类型&#x2F;索引ID?pretty&#39; -H &#39;Content-Type: application&#x2F;json&#39; -d&#39; &#123; &#125;&#39;例如：curl -XPOST --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;1.1.1.1:24148&#x2F;website&#x2F;blog&#x2F;123?pretty&#39; -H &#39;Content-Type: application&#x2F;json&#39; -d&#39; &#123;&quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Just trying this out...&quot;,&quot;date&quot;: &quot;2014&#x2F;01&#x2F;01&quot;&#125;&#39; 删除文档 curl -XDELETE --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;索引名&#x2F;索引类型&#x2F;索引ID?pretty&#39;例如：curl -XDELETE --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;1.1.1.1:24148&#x2F;website&#x2F;blog&#x2F;123?pretty&#39; ES搜索/查询命令，以及高级的query、filter、聚合搜索操作基础的查询命令查询所有数据 curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;索引名&#x2F;文档类型&#x2F;_search?pretty&#39; -d&#39; &#123;&quot;query&quot; : &#123; &quot;match_all&quot; : &#123;&#125; &#125;&#125;&#39; 根据rowkey查询 curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;索引名&#x2F;文档类型&#x2F;_search?pretty&#39; -d&#39; &#123;&quot;query&quot; : &#123; &quot;match&quot; : &#123;&quot;rowkey&quot;:&quot;KX6ncmYBWgNWo4LeB0VD&quot;&#125; &#125;&#125;&#39;索引名可以用2019*形式，表示查询范围是以2019开头的索引 返回文档的一部分数据 curl -XGET --negotiate -k -v -u : &#39;https:&#x2F;&#x2F;ip:httpport&#x2F;索引名&#x2F;文档类型&#x2F;索引ID?_source&#x3D;文档属性名,文档属性名&amp;pretty&#39; 此外，在搜索时url中可以指定索引名和类型名以减少搜索的范围： /_search：搜索所有索引的所有类型； /my_index/_search：搜索my_index索引的所有类型； /students, my_index /_search：搜索students和my_index索引的所有类型； / my_*/_search：搜索名称以my_开头的所有索引的所有类型； / my_index/customer/_search：搜索my_index索引的customer类型； /_all/customer/_search：搜索所有索引的customer类型 PS：一旦你取回了你的搜索结果，Elasticsearch就完成了使命，它不会维护任何服务器端的资源或者在你的结果中打开游标，这也是RESTFul风格的一个特性，也注定了ES每次返回值默认是有限制的。 由于REST request body的JSON格式从可读性和灵活性来说都给开发者带来太多的美感，所以实际应用中重点使用JSON格式，这种查询语句又被称为DSL。而DSL又因原理不同被划分为查询DSL(query DSL)和过滤DSL(filter DSL)。 DSL格式如果再使用linux的curl命令来书写那真是太累了，建议使用ELK组合中的Kibana来帮助查询ES。下载地址：https://www.elastic.co/downloads/kibana 按照官网的教程：1下载解压2配置config/kibana.yml3执行bin/kibana4浏览器访问http://localhost:5601 从上面的很多命令可以看到，基础的curl命令实际上是使用的query-string格式，但是，该格式仅在简单的命令中有着优势，在面对较为复杂的查询需求，query-string格式就显得心有余而力不足，因此，随着查询需求的越来越高级，curl命令主体的格式也慢慢转变成了JSON格式，这个现象在本段上面的两条查询语句中得到了证实，而下面将会深入讲解curl命令的几个高级搜索特性，这些搜索特性，全都基于JSON格式。 查询DSL — query在查询上下文中，查询会回答这个问题——“这个文档匹不匹配这个查询，它的相关度高么？”虽然与Oracle、MySQL同有数据存储的功能，但ES本质上是一个搜索引擎，搜索引擎意味着它需要去判断查询请求与查询内容之间的“相关度”的问题，ES为了实现“相关度”，引入了“评分”（score）这个概念，而query查询就是实现“评分”这个能力的关键下面会使用一个例子体现query查询的特性，顺带，也会展示ES查询的请求与响应的细节： 请求： curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;sample&#x2F;type&#x2F;_search?pretty&#39; -d&#39; &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;state&quot;: &quot;UT&quot; &#125; &#125;, &quot;from&quot;: 10, &quot;size&quot;: 2, &quot;sort&quot;: [ &#123; &quot;age&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ], &quot;_source&quot;: [&quot;account_number&quot;,&quot;address&quot;,&quot;state&quot;,&quot;age&quot;]&#125;--query 与之相对的是filter，两者的区别后面会详细介绍--match 里面是查询条件，与query匹配使用--size 代表结果取数返回记录数，像limit或rownum的作用。默认为10--from 是标识从第几条记录开始取值。默认为0--sort 标识按什么排序--_source 标识返回集中的字段名，像select后的属性，默认是select * 响应： &#123; &quot;took&quot;: 30, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 20, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;customer&quot;, &quot;_id&quot;: &quot;465&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;account_number&quot;: 465, &quot;address&quot;: &quot;916 Evergreen Avenue&quot;, &quot;state&quot;: &quot;UT&quot;, &quot;age&quot;: 29 &#125;, &quot;sort&quot;: [ 29 ] &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;customer&quot;, &quot;_id&quot;: &quot;758&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;account_number&quot;: 758, &quot;address&quot;: &quot;149 Surf Avenue&quot;, &quot;state&quot;: &quot;UT&quot;, &quot;age&quot;: 28 &#125;, &quot;sort&quot;: [ 28 ] &#125; ] &#125;&#125;简单说返回内容包括2部分。一部分是本次搜索的基本信息：--took 消耗的时间，单位是ms--shards 分片被检索的信息--hits.total 满足搜索条件的记录个数；--hits.hits 返回结果集另一部分是搜索结果集：--index 结果所在索引--type 结果所在类型--id 结果的ID--score 结果的评分（需要了解Lucene中df&#x2F;tf的概念Lucene原理分析）--source 如前面所说select *里的内容 过滤DSL — filter在过滤器上下文中，查询会回答这个问题——“这个文档匹不匹配？” 答案很简单，是或者不是。它不会去计算任何分值，也不会关心返回的排序问题，因此效率会高一点。 过滤上下文 是在使用filter参数时候的执行环境，比如在bool查询中使用Must_not或者filter 另外，经常使用过滤器，ES会自动的缓存过滤器的内容，这对于查询来说，会提高很多性能。 为了便于理解，下面列举一些过滤的情况：创建日期是否在2013-2014年间？status字段是否为published？lat_lon字段是否在某个坐标的10公里范围内？ 从上面的例子中可以看到，过滤器上下文中，查询操作仅判断是否满足查询条件。 最后，使用一个例子，来演示一下filter的使用：搜索年龄在20-25之间，余额在20000-35000之间，地址包含Street，或者邮箱包含schultzmoreno的这样的记录。 curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;sample&#x2F;type&#x2F;_search?pretty&#39; -d&#39; &#123; &quot;query&quot;: &#123; &quot;bool&quot;:&#123; &quot;should&quot;: [ &#123;&quot;match&quot;: &#123; &quot;address&quot;: &quot;Street&quot; &#125;&#125;, &#123;&quot;match&quot;: &#123; &quot;email&quot;: &quot;schultzmoreno&quot; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gte&quot;: 20, &quot;lte&quot;: 25 &#125; &#125; &#125;, &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;balance&quot;: &#123; &quot;gte&quot;: 20000, &quot;lte&quot;: 35000 &#125; &#125; &#125; &#125; &#125;, &quot;from&quot;: 0, &quot;size&quot;: 10&#125; query与filter的区别 query与filter虽然有相对关系，但在实际使用中，并不存在两者不能共用的问题，正如上面的那个查询语句示例，filter嵌套在query里面，在实际使用中，两者的使用仅看查询的需求，如果需要query查询那样的全文搜索、评分功能，那么就使用query，如果需要filter过滤那样的精准判断，那么就使用filter，下面同样以一个例子，展示query与filter的混合使用，以及如何去根据实际需求，组织查询语句： 假设现在我们需要查询一个邮箱数据库，查询收件箱中，邮件内容包含business opportunity 的邮件： 首先，获取邮件内容中有“business opportunity”的： &#123; &quot;match&quot;: &#123; &quot;email&quot;: &quot;business opportunity&quot; &#125; &#125; 然后，限定只在收件箱中查找： &#123; &quot;term&quot;: &#123; &quot;folder&quot;: &quot;inbox&quot; &#125; &#125; 接着，对这两条语句进行拼合，match语句很明显是一个query查询，而team语句则可以看出是一个filter过滤，由于search API中只能包含 query 语句，所以我们需要用 filtered 来同时包含 &quot;query&quot; 和 &quot;filter&quot; 子句： &#123; &quot;filtered&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;email&quot;: &quot;business opportunity&quot; &#125;&#125;, &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;folder&quot;: &quot;inbox&quot; &#125;&#125; &#125; &#125; 最后，我们在最外层加上query的上下文关系以及curl命令主体： curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;sample&#x2F;type&#x2F;_search?pretty&#39; -d&#39;&#123; &quot;query&quot;: &#123; &quot;filtered&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;email&quot;: &quot;business opportunity&quot; &#125;&#125;, &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;folder&quot;: &quot;inbox&quot; &#125;&#125; &#125; &#125; &#125; 至此，一个简单的复合查询命令就已经完成，只需要执行，便能得到想要的结果。 聚合查询Elasticsearch具有称为聚合的功能，允许您对数据生成复杂的分组和分析，像是Oracle中的group by、avg等，而且更强大。 Metric聚合先看运算，跟Oracle类似有sum、max、min、avg、count等，在ES中可以单独算某一种运算外，还提供了一个stats参数，一次请求把以上所有结果都返回出来。 请求： curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;sample&#x2F;type&#x2F;_search?pretty&#39; -d&#39;&#123; &quot;aggs&quot;: &#123; &quot;my_name&quot;: &#123; &quot;stats&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;, &quot;size&quot;: 0&#125;--Aggs 代表是启用聚合功能了，固定套路。--My_name 给聚合返回的结果集起一个别名，单独看不出意义，嵌套聚合的时候体现价值。--Stats 聚合的操作命令，这里是统计的命令，可以换成sum、max、min、avg、term等各种各样ES内置的命令。--Field 指定被聚合的属性名。--Size 指定结果集中要返回多少条被本次聚合命中的document，如果只关心聚合结果不关心命中的记录，size请指定为0。 返回： &#123; &quot;took&quot;: 24, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1000, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;my_name&quot;: &#123; &quot;count&quot;: 1000, &quot;min&quot;: 20, &quot;max&quot;: 40, &quot;avg&quot;: 30.171, &quot;sum&quot;: 30171 &#125; &#125;&#125; Bucket聚合桶子的意思，根据条件把数据按照木桶封装好，有点Oracle中group by的意思。 请求：按照年龄把文档按桶子分分类。 curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;sample&#x2F;type&#x2F;_search?pretty&#39; -d&#39;&#123; &quot;aggs&quot;: &#123; &quot;my_name&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;, &quot;size&quot;: 0&#125; 返回： &#123; &quot;took&quot;: 92, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1000, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;my_name&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 463, &quot;buckets&quot;: [ &#123; &quot;key&quot;: 31, &quot;doc_count&quot;: 61 &#125;, &#123; &quot;key&quot;: 39, &quot;doc_count&quot;: 60 &#125;, &#123; &quot;key&quot;: 26, &quot;doc_count&quot;: 59 &#125;, &#123; &quot;key&quot;: 32, &quot;doc_count&quot;: 52 &#125;, &#123; &quot;key&quot;: 35, &quot;doc_count&quot;: 52 &#125;, &#123; &quot;key&quot;: 36, &quot;doc_count&quot;: 52 &#125;, &#123; &quot;key&quot;: 22, &quot;doc_count&quot;: 51 &#125;, &#123; &quot;key&quot;: 28, &quot;doc_count&quot;: 51 &#125;, &#123; &quot;key&quot;: 33, &quot;doc_count&quot;: 50 &#125;, &#123; &quot;key&quot;: 34, &quot;doc_count&quot;: 49 &#125; ] &#125; &#125;&#125; 每个年龄都有一桶记录在里面，默认是按照桶里数据多少来排序的，可以在field后添加 &quot;order&quot; : { &quot;_term&quot; : &quot;asc&quot;}指定按照内容来排序。 还可以与前面的运算嵌套使用，我想算一下每个年龄存款的平均水平： curl -XGET --negotiate -H &#39;Content-Type: application&#x2F;json&#39; -k -v -u : &#39;https:&#x2F;&#x2F;17.22.142.3:24148&#x2F;sample&#x2F;type&#x2F;_search?pretty&#39; -d&#39;&#123; &quot;aggs&quot;: &#123; &quot;my_name&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;age&quot;, &quot;order&quot;: &#123; &quot;avg_balance&quot;: &quot;desc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_balance&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;balance&quot; &#125; &#125; &#125; &#125; &#125;, &quot;size&quot;: 0&#125; 至此，ES命令的介绍基本结束，本章讲述的命令并非ES搜索命令的全部，ES有着强大的搜索功能，这也意味着ES的查询语句将比较复杂，建议在掌握了本节所列举的这些基础语法后，再上网搜索其他更复杂更强大的命令。","categories":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/"},{"name":"Bigdata","slug":"CS-Concept/Bigdata","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://winyter.github.io/MyBlog/tags/Hadoop/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://winyter.github.io/MyBlog/tags/ElasticSearch/"}]},{"title":"HBase Shell 过滤器 —— HBase Shell Filter","slug":"hbase-shell-filter","date":"2019-12-25T15:29:33.000Z","updated":"2020-07-25T11:16:38.132Z","comments":true,"path":"2019/12/25/hbase-shell-filter/","link":"","permalink":"https://winyter.github.io/MyBlog/2019/12/25/hbase-shell-filter/","excerpt":"转自：https://blog.csdn.net/u014034934/article/details/74330848 &amp; https://blog.csdn.net/liuxiao723846/article/details/73823056 基础查询","text":"转自：https://blog.csdn.net/u014034934/article/details/74330848 &amp; https://blog.csdn.net/liuxiao723846/article/details/73823056 基础查询 hbase(main):046:0&gt; scan &#39;hbaseFilter&#39;ROW COLUMN+CELL row0 column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;age0 row0 column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0 row1 column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1 row1 column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1 row10 column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;age10 row10 column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10 row11 column&#x3D;f2:age, timestamp&#x3D;1499150787905, value&#x3D;age11 row11 column&#x3D;f2:name, timestamp&#x3D;1499150787905, value&#x3D;name11 row12 column&#x3D;f2:age, timestamp&#x3D;1499150787908, value&#x3D;age12 row12 column&#x3D;f2:name, timestamp&#x3D;1499150787908, value&#x3D;name12 row13 column&#x3D;f2:age, timestamp&#x3D;1499150787911, value&#x3D;age13 row13 column&#x3D;f2:name, timestamp&#x3D;1499150787911, value&#x3D;name13 row14 column&#x3D;f2:age, timestamp&#x3D;1499150787913, value&#x3D;age14 row14 column&#x3D;f2:name, timestamp&#x3D;1499150787913, value&#x3D;name14 row15 column&#x3D;f2:age, timestamp&#x3D;1499150787917, value&#x3D;age15 row15 column&#x3D;f2:name, timestamp&#x3D;1499150787917, value&#x3D;name15 row16 column&#x3D;f2:age, timestamp&#x3D;1499150787920, value&#x3D;age16 row16 column&#x3D;f2:name, timestamp&#x3D;1499150787920, value&#x3D;name16 row17 column&#x3D;f2:age, timestamp&#x3D;1499150787923, value&#x3D;age17 row17 column&#x3D;f2:name, timestamp&#x3D;1499150787923, value&#x3D;name17 row18 column&#x3D;f2:age, timestamp&#x3D;1499150787927, value&#x3D;age18 row18 column&#x3D;f2:name, timestamp&#x3D;1499150787927, value&#x3D;name18 row19 column&#x3D;f2:age, timestamp&#x3D;1499150787930, value&#x3D;age19 row19 column&#x3D;f2:name, timestamp&#x3D;1499150787930, value&#x3D;name19 row2 column&#x3D;f:age, timestamp&#x3D;1499150787879, value&#x3D;age2 row2 column&#x3D;f:name, timestamp&#x3D;1499150787879, value&#x3D;name2 row3 column&#x3D;f:age, timestamp&#x3D;1499150787882, value&#x3D;age3 row3 column&#x3D;f:name, timestamp&#x3D;1499150787882, value&#x3D;name3 row4 column&#x3D;f:age, timestamp&#x3D;1499150787885, value&#x3D;age4 row4 column&#x3D;f:name, timestamp&#x3D;1499150787885, value&#x3D;name4 row5 column&#x3D;f:age, timestamp&#x3D;1499150787888, value&#x3D;age5 row5 column&#x3D;f:name, timestamp&#x3D;1499150787888, value&#x3D;name5 row6 column&#x3D;f:age, timestamp&#x3D;1499150787890, value&#x3D;age6 row6 column&#x3D;f:name, timestamp&#x3D;1499150787890, value&#x3D;name6 row7 column&#x3D;f:age, timestamp&#x3D;1499150787893, value&#x3D;age7 row7 column&#x3D;f:name, timestamp&#x3D;1499150787893, value&#x3D;name7 row8 column&#x3D;f:age, timestamp&#x3D;1499150787896, value&#x3D;age8 row8 column&#x3D;f:name, timestamp&#x3D;1499150787896, value&#x3D;name8 row9 column&#x3D;f:age, timestamp&#x3D;1499150787898, value&#x3D;age9 row9 column&#x3D;f:name, timestamp&#x3D;1499150787898, value&#x3D;name9 20 row(s) in 0.1990 seconds 在hbase shell用show_filters命令查看一下可以用什么Filter。hbase(main):009:0&gt; show_filtersColumnPrefixFilter TimestampsFilter PageFilter MultipleColumnPrefixFilter FamilyFilter ColumnPaginationFilter SingleColumnValueFilter RowFilter QualifierFilter ColumnRangeFilter ValueFilter PrefixFilter SingleColumnValueExcludeFilter ColumnCountGetFilter InclusiveStopFilter DependentColumnFilter FirstKeyOnlyFilter KeyOnlyFilter 1.keyOnlyFilter返回的列值全部为空，即只要key，不要value。 import org.apache.hadoop.hbase.filter.KeyOnlyFilter;scan &#39;hbaseFilter&#39;, &#123;FILTER&#x3D;&gt;KeyOnlyFilter.new()&#125;ROW COLUMN+CELL row0 column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D; row0 column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D; row1 column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D; row1 column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D; row10 column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D; 2.FirstKeyOnlyFilter一个rowkey可以有多个version,同一个rowkey的同一个column也会有多个的值, 只拿出key中的第一个column的第一个version。 hbase(main):096:0&gt; import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;hbase(main):097:0* scan &#39;hbaseFilter&#39;,&#123;FILTER&#x3D;&gt;FirstKeyOnlyFilter.new()&#125;ROW COLUMN+CELL row0 column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;age0 row1 column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1 row10 column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;age10 row11 column&#x3D;f2:age, timestamp&#x3D;1499150787905, value&#x3D;age11 row12 column&#x3D;f2:age, timestamp&#x3D;1499150787908, value&#x3D;age12 3.PrefixFilter根据行的前缀过滤行。 hbase(main):106:0&gt; import org.apache.hadoop.hbase.filter.PrefixFilter;hbase(main):107:0* import org.apache.hadoop.hbase.util.Bytes;hbase(main):108:0* scan &#39;hbaseFilter&#39;,&#123;FILTER&#x3D;&gt;PrefixFilter.new(Bytes.toBytes(&#39;row3&#39;))&#125;ROW COLUMN+CELL row3 column&#x3D;f:age, timestamp&#x3D;1499150787882, value&#x3D;age3 row3 column&#x3D;f:name, timestamp&#x3D;1499150787882, value&#x3D;name3 1 row(s) in 0.1670 seconds 4.ColumnPrefixFilter返回满足条件的列 hbase(main):113:0&gt; import org.apache.hadoop.hbase.filter.ColumnPrefixFilter;hbase(main):114:0* import org.apache.hadoop.hbase.util.Bytes;hbase(main):115:0* scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;ColumnPrefixFilter.new(Bytes.toBytes(&#39;n&#39;))&#125;ROW COLUMN+CELL row0 column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0 row1 column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1 row10 column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10 row11 column&#x3D;f2:name, timestamp&#x3D;1499150787905, value&#x3D;name11 row12 column&#x3D;f2:name, timestamp&#x3D;1499150787908, value&#x3D;name12 row13 column&#x3D;f2:name, timestamp&#x3D;1499150787911, value&#x3D;name13 5.multipleColumnPrefixFilter根据列名得前缀过滤，有范围，下面是列名‘a’开始到‘b’结束。 hbase(main):011:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;MultipleColumnPrefixFilter(&#39;a&#39;,&#39;b&#39;)&quot;&#125;ROW COLUMN+CELL row0 column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;age0 row1 column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1 row10 column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;age10 row11 column&#x3D;f2:age, timestamp&#x3D;1499150787905, value&#x3D;age11 row12 column&#x3D;f2:age, timestamp&#x3D;1499150787908, value&#x3D;age12 6.ColumnCountGetFilter返回多少列。 hbase(main):012:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;ColumnCountGetFilter(2)&quot;&#125;ROW COLUMN+CELL row0 column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;age0 row0 column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0 row1 column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1 row1 column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1 row10 column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;age10 row10 column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10 row11 column&#x3D;f2:age, timestamp&#x3D;1499150787905, value&#x3D;age11 row11 column&#x3D;f2:name, timestamp&#x3D;1499150787905, value&#x3D;name11 row12 column&#x3D;f2:age, timestamp&#x3D;1499150787908, value&#x3D;age12 row12 column&#x3D;f2:name, timestamp&#x3D;1499150787908, value&#x3D;name12 7. PageFilter返回多少行 hbase(main):013:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;PageFilter(3)&quot;&#125;ROW COLUMN+CELL row0 column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;age0 row0 column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0 row1 column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1 row1 column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1 row10 column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;age10 row10 column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10 3 row(s) in 0.1680 seconds 8. ColumnPaginationFilter根据limit和offset得到数据 hbase(main):014:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;ColumnPaginationFilter(2,1)&quot;&#125;ROW COLUMN+CELL row0 column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0 row1 column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1 row10 column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10 row11 column&#x3D;f2:name, timestamp&#x3D;1499150787905, value&#x3D;name11 row12 column&#x3D;f2:name, timestamp&#x3D;1499150787908, value&#x3D;name12 row13 column&#x3D;f2:name, timestamp&#x3D;1499150787911, value&#x3D;name13 9. InclusiveStopFilter设置停止的行 hbase(main):015:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;InclusiveStopFilter(&#39;row15&#39;)&quot;&#125;ROW COLUMN+CELL row0 column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;age0 row0 column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0 row1 column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1 row1 column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1 row10 column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;age10 row10 column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10 row11 column&#x3D;f2:age, timestamp&#x3D;1499150787905, value&#x3D;age11 row11 column&#x3D;f2:name, timestamp&#x3D;1499150787905, value&#x3D;name11 row12 column&#x3D;f2:age, timestamp&#x3D;1499150787908, value&#x3D;age12 row12 column&#x3D;f2:name, timestamp&#x3D;1499150787908, value&#x3D;name12 row13 column&#x3D;f2:age, timestamp&#x3D;1499150787911, value&#x3D;age13 row13 column&#x3D;f2:name, timestamp&#x3D;1499150787911, value&#x3D;name13 row14 column&#x3D;f2:age, timestamp&#x3D;1499150787913, value&#x3D;age14 row14 column&#x3D;f2:name, timestamp&#x3D;1499150787913, value&#x3D;name14 row15 column&#x3D;f2:age, timestamp&#x3D;1499150787917, value&#x3D;age15 row15 column&#x3D;f2:name, timestamp&#x3D;1499150787917, value&#x3D;name15 8 row(s) in 0.2250 seconds 10. TimeStampsFilter返回指定时间戳的数据 hbase(main):016:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;TimestampsFilter(1499150787875,1499150787913)&quot;&#125;ROW COLUMN+CELL row1 column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1 row1 column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1 row14 column&#x3D;f2:age, timestamp&#x3D;1499150787913, value&#x3D;age14 row14 column&#x3D;f2:name, timestamp&#x3D;1499150787913, value&#x3D;name14 2 row(s) in 0.0340 seconds 11.RowFilter根据rowkey的值过滤 hbase(main):018:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;RowFilter(&gt;&#x3D;,&#39;binary:row6&#39;)&quot;&#125;ROW COLUMN+CELL row6 column&#x3D;f:age, timestamp&#x3D;1499150787890, value&#x3D;age6 row6 column&#x3D;f:name, timestamp&#x3D;1499150787890, value&#x3D;name6 row7 column&#x3D;f:age, timestamp&#x3D;1499150787893, value&#x3D;age7 row7 column&#x3D;f:name, timestamp&#x3D;1499150787893, value&#x3D;name7 row8 column&#x3D;f:age, timestamp&#x3D;1499150787896, value&#x3D;age8 row8 column&#x3D;f:name, timestamp&#x3D;1499150787896, value&#x3D;name8 row9 column&#x3D;f:age, timestamp&#x3D;1499150787898, value&#x3D;age9 row9 column&#x3D;f:name, timestamp&#x3D;1499150787898, value&#x3D;name9 4 row(s) in 0.2340 seconds 12. FamilyFilter根据列族过滤 hbase(main):020:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;FamilyFilter(&#x3D;,&#39;substring:f&#39;)&quot;&#125;ROW COLUMN+CELL row0 column&#x3D;f:age, timestamp&#x3D;1499150787863, value&#x3D;age0 row0 column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0 row1 column&#x3D;f:age, timestamp&#x3D;1499150787875, value&#x3D;age1 row1 column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1 row10 column&#x3D;f2:age, timestamp&#x3D;1499150787901, value&#x3D;age10 row10 column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10 row11 column&#x3D;f2:age, timestamp&#x3D;1499150787905, value&#x3D;age11 row11 column&#x3D;f2:name, timestamp&#x3D;1499150787905, value&#x3D;name11 13. QualifierFilter根据列名过滤 hbase(main):023:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;QualifierFilter(&#x3D;,&#39;regexstring:n.&#39;)&quot;&#125;ROW COLUMN+CELL row0 column&#x3D;f:name, timestamp&#x3D;1499150787863, value&#x3D;name0 row1 column&#x3D;f:name, timestamp&#x3D;1499150787875, value&#x3D;name1 row10 column&#x3D;f2:name, timestamp&#x3D;1499150787901, value&#x3D;name10 row11 column&#x3D;f2:name, timestamp&#x3D;1499150787905, value&#x3D;name11 row12 column&#x3D;f2:name, timestamp&#x3D;1499150787908, value&#x3D;name12 row13 column&#x3D;f2:name, timestamp&#x3D;1499150787911, value&#x3D;name13 14. ValueFilter根据值过滤，只返回匹配的列 hbase(main):024:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;ValueFilter(&#x3D;,&#39;binary:name3&#39;)&quot;&#125;ROW COLUMN+CELL row3 column&#x3D;f:name, timestamp&#x3D;1499150787882, value&#x3D;name3 1 row(s) in 0.0140 seconds 还有一种值过滤方式，以下返回的是：值包含88的数据 scan &#39;test1&#39;, FILTER&#x3D;&gt;&quot;ValueFilter(&#x3D;,&#39;substring:88&#39;)&quot; ROW COLUMN+CELL user1|ts2 column&#x3D;sf:c1, timestamp&#x3D;1409122354918, value&#x3D;sku188 user2|ts5 column&#x3D;sf:c2, timestamp&#x3D;1409122355030, value&#x3D;sku288 15. SingleColumnValueFilter根据列值返回行 hbase(main):035:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,COLUMN&#x3D;&gt;&#39;f&#39;,FILTER&#x3D;&gt;&quot;SingleColumnValueFilter(&#39;f&#39;,&#39;name&#39;,&gt;&#x3D;,&#39;binary:name3&#39;)&quot;&#125;ROW COLUMN+CELL row3 column&#x3D;f:age, timestamp&#x3D;1499150787882, value&#x3D;age3 row3 column&#x3D;f:name, timestamp&#x3D;1499150787882, value&#x3D;name3 row4 column&#x3D;f:age, timestamp&#x3D;1499150787885, value&#x3D;age4 row4 column&#x3D;f:name, timestamp&#x3D;1499150787885, value&#x3D;name4 row5 column&#x3D;f:age, timestamp&#x3D;1499150787888, value&#x3D;age5 row5 column&#x3D;f:name, timestamp&#x3D;1499150787888, value&#x3D;name5 row6 column&#x3D;f:age, timestamp&#x3D;1499150787890, value&#x3D;age6 row6 column&#x3D;f:name, timestamp&#x3D;1499150787890, value&#x3D;name6 row7 column&#x3D;f:age, timestamp&#x3D;1499150787893, value&#x3D;age7 row7 column&#x3D;f:name, timestamp&#x3D;1499150787893, value&#x3D;name7 row8 column&#x3D;f:age, timestamp&#x3D;1499150787896, value&#x3D;age8 row8 column&#x3D;f:name, timestamp&#x3D;1499150787896, value&#x3D;name8 row9 column&#x3D;f:age, timestamp&#x3D;1499150787898, value&#x3D;age9 row9 column&#x3D;f:name, timestamp&#x3D;1499150787898, value&#x3D;name9 7 row(s) in 0.1520 seconds 16.使用AND相当于FilterList的FilterList.Operator.MUST_PASS_ALL hbase(main):042:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;(FamilyFilter(&#x3D;,&#39;substring:f&#39;)) AND (ValueFilter(&gt;,&#39;binary:name6&#39;))&quot;&#125;ROW COLUMN+CELL row7 column&#x3D;f:name, timestamp&#x3D;1499150787893, value&#x3D;name7 row8 column&#x3D;f:name, timestamp&#x3D;1499150787896, value&#x3D;name8 row9 column&#x3D;f:name, timestamp&#x3D;1499150787898, value&#x3D;name9 3 row(s) in 0.0100 seconds 17.使用OR相当于FilterList的FilterList.Operator.MUST_PASS_ONE。 hbase(main):044:0&gt; scan &#39;hbaseFilter&#39;,&#123;STARTROW&#x3D;&gt;&#39;row0&#39;,STOPROW&#x3D;&gt;&#39;row99&#39;,FILTER&#x3D;&gt;&quot;(FamilyFilter(&#x3D;,&#39;substring:f&#39;)) AND (ValueFilter(&gt;,&#39;binary:name6&#39;)) OR (FamilyFilter(&#x3D;,&#39;substring:f2&#39;)) AND (ValueFilter(&gt;,&#39;binary:name17&#39;))&quot;&#125;ROW COLUMN+CELL row18 column&#x3D;f2:name, timestamp&#x3D;1499150787927, value&#x3D;name18 row19 column&#x3D;f2:name, timestamp&#x3D;1499150787930, value&#x3D;name19 row7 column&#x3D;f:name, timestamp&#x3D;1499150787893, value&#x3D;name7 row8 column&#x3D;f:name, timestamp&#x3D;1499150787896, value&#x3D;name8 row9 column&#x3D;f:name, timestamp&#x3D;1499150787898, value&#x3D;name9 5 row(s) in 0.1450 seconds 18、查询rowkey里面包含ts的import org.apache.hadoop.hbase.filter.CompareFilterimport org.apache.hadoop.hbase.filter.SubstringComparatorimport org.apache.hadoop.hbase.filter.RowFilterscan &#39;test1&#39;, &#123;FILTER &#x3D;&gt; RowFilter.new(CompareFilter::CompareOp.valueOf(&#39;EQUAL&#39;), SubstringComparator.new(&#39;ts&#39;))&#125; ROW COLUMN+CELL user1|ts1 column&#x3D;sf:c1, timestamp&#x3D;1409122354868, value&#x3D;sku1 user1|ts2 column&#x3D;sf:c1, timestamp&#x3D;1409122354918, value&#x3D;sku188 user1|ts3 column&#x3D;sf:s1, timestamp&#x3D;1409122354954, value&#x3D;sku123 user2|ts4 column&#x3D;sf:c1, timestamp&#x3D;1409122354998, value&#x3D;sku2 user2|ts5 column&#x3D;sf:c2, timestamp&#x3D;1409122355030, value&#x3D;sku288 user2|ts6 column&#x3D;sf:s1, timestamp&#x3D;1409122355970, value&#x3D;sku222 此外，该语句还支持正则表达式 import org.apache.hadoop.hbase.filter.RegexStringComparatorimport org.apache.hadoop.hbase.filter.CompareFilterimport org.apache.hadoop.hbase.filter.SubstringComparatorimport org.apache.hadoop.hbase.filter.RowFilterscan &#39;test1&#39;, &#123;FILTER &#x3D;&gt; RowFilter.new(CompareFilter::CompareOp.valueOf(&#39;EQUAL&#39;),RegexStringComparator.new(&#39;^user\\d+\\|ts\\d+$&#39;))&#125; ROW COLUMN+CELL user1|ts1 column&#x3D;sf:c1, timestamp&#x3D;1409122354868, value&#x3D;sku1 user1|ts2 column&#x3D;sf:c1, timestamp&#x3D;1409122354918, value&#x3D;sku188 user1|ts3 column&#x3D;sf:s1, timestamp&#x3D;1409122354954, value&#x3D;sku123 user2|ts4 column&#x3D;sf:c1, timestamp&#x3D;1409122354998, value&#x3D;sku2 user2|ts5 column&#x3D;sf:c2, timestamp&#x3D;1409122355030, value&#x3D;sku288 user2|ts6 column&#x3D;sf:s1, timestamp&#x3D;1409122355970, value&#x3D;sku222 番外、hbase zkcli 的使用hbase zkclils &#x2F;[hbase, zookeeper] [zk: hadoop000:2181(CONNECTED) 1] ls &#x2F;hbase[meta-region-server, backup-masters, table, draining, region-in-transition, running, table-lock, master, namespace, hbaseid, online-snapshot, replication, splitWAL, recovering-regions, rs] [zk: hadoop000:2181(CONNECTED) 2] ls &#x2F;hbase&#x2F;table[member, test1, hbase:meta, hbase:namespace] [zk: hadoop000:2181(CONNECTED) 3] ls &#x2F;hbase&#x2F;table&#x2F;test1[] [zk: hadoop000:2181(CONNECTED) 4] get &#x2F;hbase&#x2F;table&#x2F;test1?master:60000&#125;l$??lPBUFcZxid &#x3D; 0x107ctime &#x3D; Wed Aug 27 14:52:21 HKT 2014mZxid &#x3D; 0x10bmtime &#x3D; Wed Aug 27 14:52:22 HKT 2014pZxid &#x3D; 0x107cversion &#x3D; 0dataVersion &#x3D; 2aclVersion &#x3D; 0ephemeralOwner &#x3D; 0x0dataLength &#x3D; 31numChildren &#x3D; 0","categories":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/"},{"name":"Bigdata","slug":"CS-Concept/Bigdata","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://winyter.github.io/MyBlog/tags/Hadoop/"},{"name":"HBase","slug":"HBase","permalink":"https://winyter.github.io/MyBlog/tags/HBase/"}]},{"title":"Hadoop快速入门系列——篇一：HBase入门","slug":"hadoop-hbase","date":"2019-12-25T15:01:04.000Z","updated":"2020-07-25T11:16:14.864Z","comments":true,"path":"2019/12/25/hadoop-hbase/","link":"","permalink":"https://winyter.github.io/MyBlog/2019/12/25/hadoop-hbase/","excerpt":"HBase概念介绍HBase是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统。HBase适合于存储大表数据（表的规模可以达到数十亿行以及数百万列），并且对大表数据的读、写访问可以达到实时级别。利用Hadoop HDFS（Hadoop Distributed File System）作为其文件存储系统，提供高可靠性、高性能、列存储、可伸缩、实时读写的数据库系统。为Spark和Hadoop MapReduce提供海量数据实时处理能力。利用ZooKeeper作为协同服务。","text":"HBase概念介绍HBase是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统。HBase适合于存储大表数据（表的规模可以达到数十亿行以及数百万列），并且对大表数据的读、写访问可以达到实时级别。利用Hadoop HDFS（Hadoop Distributed File System）作为其文件存储系统，提供高可靠性、高性能、列存储、可伸缩、实时读写的数据库系统。为Spark和Hadoop MapReduce提供海量数据实时处理能力。利用ZooKeeper作为协同服务。 数据模型Rowkey：行键，相当于关系数据库中的主键，行数据的唯一标识Column Family：CF，列族，一个表在水平方向上由一个或多个CF组成，一个CF由任意多个Column组成，Column为CF下的一个标签，所以CF支持动态扩展，而不需要预先定义Column，且不同行之间，列的个数和类型都可以不同Timestamp：每次数据操作是对应的时间戳，该列是实现Hbase一行多版本的关键，一行数据按时间戳区分版本，每个Cell的多版本按时间戳倒序存储Region：可以理解为表分区，随着数据量的增多，Hbase会根据设定好的配置阀值，自动分区Cell：HBase最小的存储单元，由Key和Value组成。Key由row、column family、column qualifier、timestamp、type、MVCC v ersion这6个字段组成。Value就是对应存储的二进制数据对象。 为了便于理解，图1通过将HBase数据模型转化成传统数据库表的实时来表现，但真实的HBase数据模型在表现形式上，还是有所区别的，这一点会在图2有所体现。 图2为实际查询的HBase数据返回结果的截图： HBase的索引依据是行键（Rowkey）、列键（ColumnFamily:Column）、时间戳（Timestamp）。可以看到，同一条rowkey，按照HBase列存储的规则，每一列为一条记录，相同的rowkey代表一行数据，而HBase并没有按照传统数据库行存储的方式，把一条数据的所有内容全部放在一条记录里面，取而代之的，是使用Key-Value的形式，对数据进行列存储，同时，在COLUMN+CELL中，column的值的组织形式是&lt;列族名&gt;:&lt;列名&gt;，列族与列名的组合，共同构建起了key，与之对应的value则包含在cell中，从图中也可以看出来，cell由timestamp和value组成，timestamp是HBase实现数据版本控制的核心，对同一条数据的不同迭代，以不同的timestamp进行标记，而value就是真正的这条数据，指定的这个列（key）所对应的值（value） HBase架构Master：HMaster，RegionServer的管理者，主要控制与Region相关的功能，包括但不限于：表的增删改查、RegionServer的负载均衡、Region分布调整、Region的分裂、RegionServer失效后Region的迁移。在HA模式下，会有主备Master，主Master故障，备用Master会替代。RegionServer：Hbase的数据处理和计算单元，提供表数据的读写等服务，RegionServer一般与HDFS的DataNode部署在一起，实现数据存储功能Zookeeper：为Hbase各进程提供分布式协作服务。各RegionServer将自己的信息注册到Zookeeper，主Master据此感知各个RegionServer的健康状态client：可以通俗的理解为客户端，client与Master进行管理类通信，与RegionServer进行数据操作类通信HDFS：为Hbase提供高可靠的文件存储服务，Hbase的数据全部存在HDFS中 拓展：Region、Store、StoreFlie、ColumnFamily的关系： The HRegionServer opens the region and creates a corresponding HRegion object. When the HRegion is opened it sets up a Store instance for each HColumnFamily for every table as defined by the user beforehand. Each Store instance can, in turn, have one or more StoreFile instances, which are lightweight wrappers around the actual storage file called HFile. A Store also has a MemStore, and the HRegionServer a shared HLog in-stance。 RegionServer打开一个region的时候，会创建一个相应的HRegion对象。当这个HRegion被打开，他会为每一个表中的每一个列簇创建一个Stroe实例，就向用户之前创建的那样。每一个Store实例相应地有一个或者多个StoreFile实例，StoreFile是对真正存储数据的文件(HFile)的轻量级封装。一个Store还会有一个Memstore。每一个HRegionServer中的所有东西会共享一个HLog实例。 结合前面的图看就很清晰了。 From:https://www.cnblogs.com/mrxiaohe/p/5271578.htmlRegionServer数据存储结构Store：一个Region由一个或多个Store组成，每个Store对应一个Column Family。Memstore：一个Store包含一个MemStore，MemStore缓存客户端向Region插入的数据，当RegionServer中的MemStore大小达到配置的容量上限时，RegionServer会将MemStore中的数据“flush”到HDFS中。StoreFile：MemStore的数据flush到HDFS后成为StoreFile，随着数据的插入，一个Store会产生多个StoreFile，当StoreFile的个数达到配置的最大值时，RegionServer会将多个StoreFile合并为一个大的StoreFile。HFile：HFile定义了StoreFile在文件系统中的存储格式，它是当前HBase系统中StoreFile的具体实现。HLog：HLog日志保证了当RegionServer故障的情况下用户写入的数据不丢失，RegionServer的多个Region共享一个相同的HLog。元数据表：用来帮助client定位到具体的Region。元数据表中包括&#39;hbase:meta&#39;表，用来记录用户表的Region信息。 Hbase Shell使用HBase客户端的使用要访问HBase shell，必须导航进入到HBase的主文件夹。 cd &#x2F;usr&#x2F;localhost&#x2F;cd Hbase 可以使用“hbase shell”命令来启动HBase的交互shell，如下图所示。 .&#x2F;bin&#x2F;hbase shell 如果已成功在系统中安装HBase，那么它会给出 HBase shell 提示符，如下图所示。 HBase Shell; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands.Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase ShellVersion 0.94.23, rf42302b28aceaab773b15f234aa8718fff7eea3c, Wed Aug 2700:54:09 UTC 2014hbase(main):001:0&gt; 要退出交互shell命令，在任何时候键入 exit 或使用 &lt;Ctrl + C&gt;。进一步处理检查shell功能之前，使用 list 命令用于列出所有可用命令。list是用来获取所有HBase 表的列表。首先，验证安装HBase在系统中使用如下所示。hbase(main):001:0&gt; list 当输入这个命令，它给出下面的输出。 hbase(main):001:0&gt; listTABLE HBase Shell命令的使用这里仅写出一些简单的HBase Shell命令的使用，HBase Shell Filter的使用可以参考我的另外一篇文章：HBase Shell 过滤器 —— HBase Shell Filter HBase Shell命令是操作HBase数据库的命令集合。 1、查询表中数据：scan &amp; getscan用于查询整表数据，使用方法： scan &#39;table_name&#39; get则是根据指定的表、rowkey、列组合，获取行值或行的指定cell值，使用方法： get &#39;table_name&#39;,&#39;rowkey&#39;,&#39;cf_name:c_name&#39; HBase shell支持通过过滤器Filter对数据进行带条件的查询，由于复杂的过滤查询需要调用HBase Filter的Jave接口，所以这里简单介绍一些常用的不需要调用接口的Filter查询命令，如果想要深入了解，可以上网搜索。1、查询表table1中，所有值为abcde的列的数据： scan ‘table1’, FILTER &#x3D;&gt; “ValueFilter(&#x3D;,’binary:abcde’)” 2、查询表table1中，所有值包含abc的列的数据： scan ‘table1’, FILTER &#x3D;&gt; “ValueFilter(&#x3D;,’subtring:abc’)” 3、查询表table1，列c1中，值包含abc的数据： scan ‘table1’, FILTER &#x3D;&gt; “ColumnPrefixFilter(‘c1’) AND ValueFilter(&#x3D;,’substring:abc’)” 2、统计数据条数：countcount ‘table_name’, {INTERVAL =&gt; 1000, CACHE =&gt; 10}INTERVAL为统计的行数间隔，默认为1000，CACHE为统计的数据缓存,默认为10HBase的count命令与SQL数据库的count统计方法不一样，HBase在统计时，会根据INTERVAL的值对数据进行截断，实际效果如图5所示： 3、插入数据：putput插入数据仅支持cell为单位的数据插入，不支持rowkey为单位的整条数据插入使用方法： put &#39;table_name&#39;,&#39;rowkey&#39;,&#39;cf_name:c_name&#39;,&#39;value&#39; 4、删除数据：delete &amp; deletealldelete：删除指定的cell，使用方法： delete &#39;table_name&#39;,&#39;rowkey&#39;,&#39;cf_name:c_name&#39; deleteall：删除指定行的所有cell，使用方法： deleteall &#39;table_name&#39;,&#39;rowkey&#39; 5、查看表描述：describedescribe可以获取表的描述信息，使用方法： describe &#39;table_name&#39; describe命令会返回指定表的状态(enable/disable)，列族信息，如图6所示： 6、获取所有表名：listlist命令能直接获取所有表名，使用方法： list 7、创建表：create使用方法： create &#39;table_name&#39;,&#39;cf1&#39;,&#39;cf2&#39;,&#39;cf3&#39; 8、启动与停止表：enable &amp; disableHbase引入了表的启动与停止，顾名思义，即在不删除表和表数据的情况下，停用表，以及在不需要新建表和导入表数据的情况下，启用表，增加了数据仓库的灵活性和安全性使用方法： enable &#39;table_name&#39; &amp; disable &#39;table_name&#39; 但在现场环境，该组命令不常用 9、删除表：dropHbase里，表在删除之前，必须保证该表已经停止，否则删除不成功，保障数据安全，使用方法： drop &#39;table_name&#39; 10、更改表结构：alter更改表结构。可以通过alter命令增加、修改、删除列族信息以及表相关的参数值，使用方法举例： alter &#39;test&#39;, &#123;NAME &#x3D;&gt; &#39;cf3&#39;, METHOD &#x3D;&gt; &#39;delete&#39;&#125;。","categories":[{"name":"CS Concept","slug":"CS-Concept","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/"},{"name":"Bigdata","slug":"CS-Concept/Bigdata","permalink":"https://winyter.github.io/MyBlog/categories/CS-Concept/Bigdata/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://winyter.github.io/MyBlog/tags/Hadoop/"},{"name":"HBase","slug":"HBase","permalink":"https://winyter.github.io/MyBlog/tags/HBase/"}]},{"title":"网站推荐（持续更新哟）","slug":"webpage-recommend","date":"2019-12-22T16:14:43.000Z","updated":"2020-07-25T11:17:37.290Z","comments":true,"path":"2019/12/23/webpage-recommend/","link":"","permalink":"https://winyter.github.io/MyBlog/2019/12/23/webpage-recommend/","excerpt":"网站推荐（持续更新哟）首先声明：本文推荐的所有网站，均是我多年来的心血收藏，当然以后我这个博客做的好的话，不排除帮真正好的网站推一推（如果帮推的网站我会注明的），但不管怎样，这里的推荐一定是纯公益的，也一定是符合我的推荐审美的：无毒、无诱导、页面清爽、内容积极的。同时，我也会在我的推荐语中，言明它的优点、缺点，以供大家选择。","text":"网站推荐（持续更新哟）首先声明：本文推荐的所有网站，均是我多年来的心血收藏，当然以后我这个博客做的好的话，不排除帮真正好的网站推一推（如果帮推的网站我会注明的），但不管怎样，这里的推荐一定是纯公益的，也一定是符合我的推荐审美的：无毒、无诱导、页面清爽、内容积极的。同时，我也会在我的推荐语中，言明它的优点、缺点，以供大家选择。 编程语言类廖雪峰的Python教程：这是廖雪峰老师的Python教程，基于Python3，全面的讲解了Python的知识点，是很多人的Python启蒙教程。Anaconda离线包下载：Anaconda作为Python包管理工具，其官网集成了Python绝大部分包，如果你Anaconda所处的环境无法连接互联网，那么你可能需要使用这个网站，下载所需的包，然后在你的环境中使用“conda install”命令部署。 云与大数据类华为大数据官方论坛 &amp; 华为云官方论坛：这两个论坛算是与我工作相关，所以才关注了，华为FusionInsight大数据集群和FusionSphere云，算是国内比较出色的政企云，尤其在目前的政府领域，而这两个论坛是华为这个产品的官方论坛，其中的知识较为全面，能帮助有掌握华为大数据和云平台知识需求的童鞋快速上手，当然如果你本身的工作没有这方面的需求，这两个论坛对你价值可能不大，毕竟这两个论坛产品针对性很强，比较偏产品业务应用方向，而非通用知识技能。HBase教程：来自民间的HBase教程(工具书)，优势在于没有高谈阔论，简单粗暴，直接面向日常使用，按照日常使用需求，给出相应的方法，同时，方法讲解也非常直接：第一步、第二步......，非常适合运维人员或者开发人员临场查询使用。如果你是HBase小白，甚至不知道HBase原理，看不懂HBase术语，那么我建议你先系统学习HBase知识后，再来阅读使用该教程。Apache软件基金官网：如果说南迦巴瓦是很多藏传佛教徒心中的圣地，那么这个网站，就是很多IT从业者心中的南迦巴瓦。用Apache官网的一句话来描述其地位，就是：“THE WORLD&#39;S LARGEST OPEN SOURCE FOUNDATION”，你在点开这个网站后，可以拉到首页最底下，有其下辖管理运营的所有IT技术，其中包括：Tomcat、HTTP Server、Hadoop这些几乎家喻户晓的技术协议，不用疑惑，这些技术协议均由Apache基金会进行管理运营。尤其在大数据时代，整个Hadoop项目，是Apache顶级项目之一，也因此，这个网站有着全世界最官方的Hadoop技术资讯。当然，吹归吹，这个网站，缺点还是有的：内容分散(由于需要服务于各类使用Hadoop的人，所以官方文档写的大而全，导致你可能仅寻找一个命令，却花费了你数个小时)、全英文(这大概是你的问题，而不是网站的问题，哈哈)。如果你能解决英文网页的问题，那么这个网站将是你全面了解大数据最好的地方。 工具类在线经纬度查询：这个网站针对性较强，受众比较窄，如果你发现你看不懂下面的文字，那么这个网站基本是不适合你的。这个网站用来对经纬度地址进行查询，并提供多种坐标系的转换，能把经纬度数据转换为地址，能解析你在地图上点击的地方的经纬度，同时，还支持批量经纬度转换查询功能。 软件资源类以下推荐的网站，都是无毒、无全家桶、无诱导下载、几乎无广告的网站，宗旨是方便大家，大家用的好，可以悄悄的收藏起来，如果你有一颗感恩的心，也希望你在觉得这个网站不错的时候，可以打个赏，支持一下博主，毕竟，这些几乎完全是公益的资源网站，背后还是需要较多运营费用的。 达牛帮帮你：资源分享博客，胜在免费、干净、无广告、无毒，目前支持的软件不多，但都是大家平时使用率极高的软件，所以还是很有用的，目前支持的软件：Office系列、Windows系列镜像、Adobe Ps Ai Pr系列、CAD Revit Maya 3D系列，思维脑图系列。但需要注意，这个网站提供的均是官方下载通道，且没有“小工具”哟，而且没有安装教程，部分安装较为复杂的软件，可能需要你自行上网搜索安装方法。适合乐于使用正版的童鞋或者装机大牛使用，如果你不想花钱买正版，也不会自己动手找资源破解，那这个网站可能不太适合你。 胡萝卜周博客：资源分享博客，胜在软件多、教程多、小工具多、无广告、无毒、免费。如果你的软件比较偏门，你可以来这里试一试，你可以把这里当做华军、西西，但没有诱导下载、没有广告、没有病毒、没有全家桶，而且他家有微信公众号的，如果喜欢使用微信来获取软件或者装机方面的咨询的话，还是比较合适的关注的。同时，他家还特别的对软件所在的操作系统进行了分类，目前支持有Windows、macOS、Android三个平台，算是很好很贴心的一个功能了。但和华军西西一样，由于软件较多，所有你想要的软件你可能得去搜索一下，挑选一下，即使这个软件是家喻户晓的。 CentOS系统包官方下载：CentOS官网的下载目录，如果你需要看源码，或者需要离线安装，这是最合适的网站。 人工智能 &amp; 机器学习类算法类必备算法：这篇文章名为：&quot;Top Algorithms/Data Structures/Concepts every computer science student should know&quot;，是必备算法的合集。","categories":[{"name":"Source","slug":"Source","permalink":"https://winyter.github.io/MyBlog/categories/Source/"}],"tags":[]},{"title":"PostgreSQL表间复制数据","slug":"postgreSQL-copy-data-between-two-tables","date":"2019-12-22T16:08:01.000Z","updated":"2020-07-25T11:17:19.876Z","comments":true,"path":"2019/12/23/postgreSQL-copy-data-between-two-tables/","link":"","permalink":"https://winyter.github.io/MyBlog/2019/12/23/postgreSQL-copy-data-between-two-tables/","excerpt":"表间复制即从源表复制到目标表，根据目标表是否存在，分为两种语句： 1、目标表不存在（即目标表需要新建）的情况语句： SELECT column1, column2, ... into Table2 from Table1 该语句将数据从Table1复制到Table2，前提是Table2不存在，语句执行时会自动创建Table2，Table2相关字段的约束规则会继承Table1。另外，该语句还支持选择Table1中需要复制的字段。","text":"表间复制即从源表复制到目标表，根据目标表是否存在，分为两种语句： 1、目标表不存在（即目标表需要新建）的情况语句： SELECT column1, column2, ... into Table2 from Table1 该语句将数据从Table1复制到Table2，前提是Table2不存在，语句执行时会自动创建Table2，Table2相关字段的约束规则会继承Table1。另外，该语句还支持选择Table1中需要复制的字段。 2、目标表已存在的情况语句： Insert into Table2(field1,field2,…) select value1,value2,… from Table1 该语句将Table1的数据复制到Table2，Table1和Table2均支持自选字段，只需要value字段和field字段一一对应即可，但需要注意字段约束规则，如果对应字段的约束规则冲突，语句运行可能失败。","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Postgre","slug":"CS-Soft/Server/Postgre","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Postgre/"}],"tags":[{"name":"Postgre","slug":"Postgre","permalink":"https://winyter.github.io/MyBlog/tags/Postgre/"},{"name":"SQL","slug":"SQL","permalink":"https://winyter.github.io/MyBlog/tags/SQL/"},{"name":"Database","slug":"Database","permalink":"https://winyter.github.io/MyBlog/tags/Database/"}]},{"title":"Shell脚本实现FTP文件增量传输","slug":"ftp-increment-download-with-shell","date":"2019-12-19T11:29:51.000Z","updated":"2020-07-25T11:16:00.770Z","comments":true,"path":"2019/12/19/ftp-increment-download-with-shell/","link":"","permalink":"https://winyter.github.io/MyBlog/2019/12/19/ftp-increment-download-with-shell/","excerpt":"#Shell脚本实现FTP文件增量传输 环境：CentOS 7.3使用语言：shellFrom：shell脚本中的实时ftp传输实例 前言该脚本基本思路是定时读取FTP远程目录上的文件名列表，然后将本次列表与上次列表对比，获取增量文件列表，然后按照此列表下载文件。","text":"#Shell脚本实现FTP文件增量传输 环境：CentOS 7.3使用语言：shellFrom：shell脚本中的实时ftp传输实例 前言该脚本基本思路是定时读取FTP远程目录上的文件名列表，然后将本次列表与上次列表对比，获取增量文件列表，然后按照此列表下载文件。 获取当前文件列表#!/bin/bashcd &lt;本机目录&gt; # 进入本机目录，你需要指定一个目录，用来存放文件列表记录ftp -n -v &lt;远程FTP IP地址&gt; &lt;&lt;! user &lt;用户名&gt; &lt;密码&gt; prompt off # 关闭交互模式 nlist &lt;文件名正则表达式&gt; list_origin.txt # 获取本次文件列表，生成list_origin.txt文件 close bye! 这时，在本机目录下，会产生一个list_origin.txt文件，这个文件就是本次运行读取的当前文件列表。 对比文件列表# 将当前文件列表与前次文件列表比对，获得增量文件列表comm -3 list_origin.txt list_last.txt &gt; list_mid.txt# 统计增量文件数量num=\"`cat list_mid.txt | wc -l`\"# 将本次文件列表以覆盖方式写入前次文件列表cat list_origin.txt &gt; list_last.txt 这个步骤就是增量传输思路的核心，利用comm命令，将本次列表与前次列表比对的结果写入增量列表，供下一步FTP下载文件使用 下载增量文件for ((i = 1; i &lt;= $num; i++));doftp -n -v &lt;远程FTP IP地址&gt; &lt;&lt;! user &lt;用户名&gt; &lt;密码&gt; binary # 使用二进制传输 passive # 进入被动传输 prompt off # 关闭交互模式 cd &lt;远程文件目录&gt; lcd &lt;本地数据文件存储目录&gt; # 这个目录可以和上面的“本地目录”不一样，且建议不一样 get `cat list_mid.txt | tail -n +$i | head -n 1` close bye!done 这段代码即是实时下载文件，使用for循环，逐行下载增量文件列表中的文件，至此，就完成了增量传输脚本的所有内容。 设置定时任务crontab -e 写入定时任务 *&#x2F;5 * * * * nohup &#x2F;bin&#x2F;bash &lt;脚本路径&gt; &gt;&gt; &lt;脚本日志路径&gt; 2&gt;&amp;1 &amp; 总结这个脚本简单粗暴，适用于非严谨场合下的快速部署。该脚本稳定性较好，实测生产环境一年的运行中，未出现问题。但由于使用for循环，每次下载一个文件都需要打开关闭一次FTP，所以，这个脚本性能一般，而且日志生成量较大，大量文件传输的情况下，需要再搭配日志清除的定时任务，基于此，也建议设置脚本执行的定时任务时，定时时间间隔尽可能小一些。另外，三个文件列表文件，尤其是list_last.txt这个前次列表文件，切勿删除，否则，将会丢失所有增量传输进度。","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Language","slug":"CS-Soft/Language","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Language/"},{"name":"Shell","slug":"CS-Soft/Language/Shell","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Language/Shell/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"https://winyter.github.io/MyBlog/tags/Shell/"},{"name":"FTP","slug":"FTP","permalink":"https://winyter.github.io/MyBlog/tags/FTP/"},{"name":"Code","slug":"Code","permalink":"https://winyter.github.io/MyBlog/tags/Code/"}]},{"title":"Nginx快速部署指南","slug":"Nginx-install-guide","date":"2019-12-18T10:04:44.000Z","updated":"2020-07-25T11:05:52.671Z","comments":true,"path":"2019/12/18/Nginx-install-guide/","link":"","permalink":"https://winyter.github.io/MyBlog/2019/12/18/Nginx-install-guide/","excerpt":"转自：菜鸟教程 （在其基础上做了一些修改） 什么是NginxNginx(&quot;engine x&quot;)是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理 服务器，也是一个 IMAP/POP3/SMTP 代理服务器。 在高连接并发的情况下，Nginx是Apache服务器不错的替代品。","text":"转自：菜鸟教程 （在其基础上做了一些修改） 什么是NginxNginx(&quot;engine x&quot;)是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理 服务器，也是一个 IMAP/POP3/SMTP 代理服务器。 在高连接并发的情况下，Nginx是Apache服务器不错的替代品。 Nginx安装系统平台：CentOS 7.3 x86_64 一、安装编译工具及库文件yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel 二、新建Nginx用户基于环境安全，不建议使用root用户部署Nginx，尤其是生产环境。 useradd -g users nginx -m -d &#x2F;home&#x2F;nginxpasswd nginx # 设置nginx用户密码su - nginx 三、安装PCREPCRE 作用是让 Nginx 支持 Rewrite 功能。 下载PCRE安装包：wget http:&#x2F;&#x2F;downloads.sourceforge.net&#x2F;project&#x2F;pcre&#x2F;pcre&#x2F;8.35&#x2F;pcre-8.35.tar.gz 解压安装包：tar zxvf pcre-8.35.tar.gz 进入安装包目录：cd pcre-8.35 编译安装.&#x2F;configuremake &amp;&amp; make install 安装完成，查看pcre版本：pcre-config --version 四、安装Nginx 下载Nignxwget http:&#x2F;&#x2F;nginx.org&#x2F;download&#x2F;nginx-1.6.2.tar.gz 解压安装包tar zxvf nginx-1.6.2.tar.gz 进入安装包目录cd nginx-1.6.2 编译安装.&#x2F;configure --prefix&#x3D;&#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx --with-http_stub_status_module --with-http_ssl_module --with-pcre&#x3D;&#x2F;home&#x2F;nginx&#x2F;pcre-8.35makemake install 查看Nginx版本&#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;sbin&#x2F;nginx -v 至此，Nginx安装完成 Nginx配置 配置nginx.conf，将/home/nginx/webserver/nginx/conf/nginx.conf替换为以下内容:user nginx users; # user &lt;username&gt; &lt;groupname&gt;worker_processes 2; #设置值和CPU核心数一致error_log &#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;logs&#x2F;nginx_error.log crit; #日志位置和日志级别pid &#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;nginx.pid;#Specifies the value for maximum file descriptors that can be opened by this process.worker_rlimit_nofile 65535;events&#123; use epoll; worker_connections 65535;&#125;http&#123; include mime.types; default_type application&#x2F;octet-stream; log_format main &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39; &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39; &#39;&quot;$http_user_agent&quot; $http_x_forwarded_for&#39;; #charset gb2312; server_names_hash_bucket_size 128; client_header_buffer_size 32k; large_client_header_buffers 4 32k; client_max_body_size 8m; sendfile on; tcp_nopush on; keepalive_timeout 60; tcp_nodelay on; fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.0; gzip_comp_level 2; gzip_types text&#x2F;plain application&#x2F;x-javascript text&#x2F;css application&#x2F;xml; gzip_vary on; #limit_zone crawler $binary_remote_addr 10m; #下面是server虚拟主机的配置 server &#123; listen 80;#监听端口 server_name localhost;#域名 index index.html index.htm index.php; root &#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;html;#站点目录 location ~ .*\\.(php|php5)?$ &#123; #fastcgi_pass unix:&#x2F;tmp&#x2F;php-cgi.sock; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; &#125; location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf|ico)$ &#123; expires 30d; # access_log off; &#125; location ~ .*\\.(js|css)?$ &#123; expires 15d; # access_log off; &#125; access_log off; &#125;&#125; 检查配置文件nginx.conf的正确性：&#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;sbin&#x2F;nginx -t 启动Nginx&#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;sbin&#x2F;nginx 访问站点从浏览器访问我们配置的站点IP，如果出现了：Welcome to nginx。就表示部署成功。 Nginx其他常用命令&#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;sbin&#x2F;nginx -s reload # 重新载入配置文件&#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;sbin&#x2F;nginx -s reopen # 重启 Nginx&#x2F;home&#x2F;nginx&#x2F;webserver&#x2F;nginx&#x2F;sbin&#x2F;nginx -s stop # 停止 Nginx","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Server","slug":"CS-Soft/Server","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/"},{"name":"Nginx","slug":"CS-Soft/Server/Nginx","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Server/Nginx/"}],"tags":[{"name":"Tools","slug":"Tools","permalink":"https://winyter.github.io/MyBlog/tags/Tools/"},{"name":"Nginx","slug":"Nginx","permalink":"https://winyter.github.io/MyBlog/tags/Nginx/"}]},{"title":"Hexo部署指南","slug":"Hexo_install_guide","date":"2019-12-16T11:25:37.000Z","updated":"2020-07-25T11:05:03.349Z","comments":true,"path":"2019/12/16/Hexo_install_guide/","link":"","permalink":"https://winyter.github.io/MyBlog/2019/12/16/Hexo_install_guide/","excerpt":"什么是Hexo“Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。”","text":"什么是Hexo“Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。” 这段话，是Hexo中文官网中关于什么是Hexo的解释。 实际上，Hexo首先是一个博客框架，和WordPress等博客框架是一样的作用——给用户写博客，而使用博客框架，可以免除我们每写一篇文章还需要自己手动构建一个网页，这些博客框架能够自动将你写成的文章转化为一个网页。 这些博客框架之间的区别，就在于使用的技术、呈现的效果上有所区别，Hexo采用了Markdown解析文章（也可以更换其他渲染引擎，但非常不建议），Hexo主打简洁、扁平的现代网页风格，所以，这也吸引了越来越多的人，使用Hexo构建自己的博客。 作为一个没使用过其他博客框架的人，我也没有能力点评这些博客框架的优劣，但对我来说，选择哪个框架只是其次，重要的，是我写在其中的文章和知识，是否足够优秀，足够出彩，而这也希望给正在纠结选择哪个博客框架的你们一点idea。 写在动手前下面，我会开始讲述如何一步一步从0开始，搭建自己的博客平台，并让她帮助你分享你的心情。 你可以打开这篇文章，按照文章中的步骤，一边阅读，一边部署，而基于这样的阅读方式，在开始正式动手前，我希望你能确定几件事情： 你是否有一个GitHub账户，并且有GitHub基础的操作知识，Hexo本身以及很多Hexo主题的开发者，都把GitHub作为他们源代码的分享地，为了顺利的使用Hexo，你可能需要在使用Hexo之前，建立一个GitHub账户，并且掌握其基础的操作。由于本文主要讲述Hexo的使用，关于GitHub的使用，可能需要你通过下面的两个链接来了解。 GitHub官网地址：你可以在这里注册、登录、使用GitHub。 GitHub教程：这是廖雪峰老师的GitHub教程，非常详实，也非常好用，你可以在这里学到几乎所有关于GitHub的基础知识。 你是否拥有一个线上环境（云服务器、连接了互联网的服务器......），这并不是必须的，Hexo可以部署在GitHub上，但如果你出于各种原因，想要部署在自己的线上环境中，那么你可能需要“整”一个线上环境，本文中会有线上环境的部署步骤，但是如何获取并搭建一个线上环境，这需要你自己的努力。 你是想在 Windows / Linux / MacOS 上部署你的本地或线上环境？本文会涉及Linux和MacOS操作系统中的部署（Windows会在后期补充上），你在阅读时，可能需要按照你的操作系统，选取相应的步骤执行，否则，你的部署会遇到问题。 最后，需要声明，本文灵感来源于诸多网上公开博客和网址，在这里感谢各位大牛的分享，下面会放出部分链接，文中如果有侵犯了您版权的地方，可以与我联系，我会在核实后第一时间更改。 Hexo官方文档(中文)：这是Hexo官方的中文文档，Hexo的中文文档写的比较好，可以直接阅读中文文档，如果你需要阅读英文文档，可以点击这里。 https://www.jianshu.com/p/1c888a6b8297?utm_source=oschina-app https://blog.csdn.net/u010075335/article/details/82861322 本地环境部署Hexo需要部署一个本地环境，这允许你在这个环境中进行调试、修改，直到页面的展示效果满足你上线的要求。 环境准备 Nodejs (官方建议使用 Node.js 10.0 及以上版本) GitNodejs安装Mac用户：推荐直接在官网下载最新LTS安装包直接安装，如果有需要，也可以使用官网推荐的Homebrew和MacPorts安装 Linux用户：推荐使用Source Code方式安装，下面也会讲解Source Code安装步骤： 前往官网，下载最新的LTS for Linux Source Code 包。或者使用wget获取安装包：wget https:&#x2F;&#x2F;npm.taobao.org&#x2F;mirrors&#x2F;node&#x2F;v10.14.1&#x2F;node-v10.14.1-linux-x64.tar.gz 然后放到你想安装的文件夹中。 解压安装包：tar -zxvf node-v8.0.0-linux-x64.tar.gz 配置环境变量。需要注意：你如果需要配置全局环境变量，需要修改系统的环境变量文件：vim &#x2F;etc&#x2F;profile 如果只想在本用户下(非root用户)安装，则只需修改用户根目录下的环境变量文件，下面以CentOS为例 (其他操作系统的环境变量文件，可以自行查询) ：vim ~&#x2F;.bash_profile 在文件末尾添加：export NODE_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;node export PATH&#x3D;$NODE_HOME&#x2F;bin:$PATH 生效配置文件：source ~&#x2F;.bash_profile 测试安装效果：npm -vnode -v nodejs和npm的安装有很多方法，上面写的方法不一定是你最喜欢的，也不一定是最方便的，但是最通用的，由于Linux Distributions较多，每个发布版本安装软件的方式不尽相同，如果有需要，你可以自行网上搜索安装方式。Git的安装Git需要在本地安装一个客户端，详细的安装方法不再赘述，可以参照廖雪峰老师的教程，亦可参照Hexo官方文档中的安装方法，当然，你也可以自行上网搜索。 安装HexoLinux与MacOS都适用于以下的安装步骤。 全局安装 npm install -g hexo-cli 仅当前用户下安装 npm install hexo echo &#39;PATH&#x3D;&quot;$PATH:.&#x2F;node_modules&#x2F;.bin&quot;&#39; &gt;&gt; ~&#x2F;.profile 需要注意，上面的~/.profile文件不一定适用于所有系统，请视自己系统内的环境变量文件而定。 建站安装完成后，你需要选择部署Hexo本地站点的文件夹，执行： hexo init cd npm install 完成后，folder文件夹下，会有如下目录结构： .├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes _config.yml这是网站的配置文件，绝大部分配置均可在这个文件中设置，但需要注意，如果你使用了第三方的主题，部分主题下，也会有_config.yml文件，这时，这两个同名文件是有区别的，根目录下的_config.yml文件是全局配置文件，服务于整个网站，而主题下的_config.yml文件仅用来设置本主题下的一些配置，仅服务于该主题，而且这个配置文件内的配置项，基本不会和全局_config.yml文件中的配置项重合。 package.json应用程序的信息。非Hexo高级操作，不需要修改这个文件，如果需要修改，可以参考官方文档和网上资料。 scaffolds“模版 文件夹。当您新建文章时，Hexo 会根据 scaffold 来建立文件。 Hexo的模板是指在新建的文章文件中默认填充的内容。例如，如果您修改scaffold/post.md中的Front-matter内容，那么每次新建一篇文章时都会包含这个修改。” 以上是官方文档中对该文件夹作用的解释，如非高级操作，该文件夹一般也不需要打开进行修改。 source存放用户资源的地方。一般默认情况下，你写的文章在发布之前，都会存在在这个文件夹下的_post目录中，当你发布网站时，Hexo会根据source目录中的相关资源，生成静态页面，存放到public文件夹下，供页面展示。 themes主题文件夹。Hexo 会根据主题来生成静态页面。这个文件夹也是你在下载了第三方Hexo主题后，这些文件的存放处。 选择主题Hexo有丰富的主题提供选择，如果你有需要，请点击官网主题页面，进行挑选。 选择完成后，下载这个主题的所有文件，将这些文件，存放到themes文件夹下。 配置在选择完主题后，你需要配置你的网站和主题，以更好的服务于你，这时候，你主要会对全局_config.yml文件和主题的_config.yml文件进行修改。 全局_config.yml修改方法全局_config.yml文件的各个配置项，在官网都有详细的说明，建议先通读官方文档中关于配置项的说明后，再着手开始配置，这里给出文档链接，本文中就不再过多赘述，仅简单讲解一些关键配置。language ：网站语言配置项，该项与你选择的主题有密切关系，例如，如果你的主题不支持中文，那么你将不能使你的网站显示中文。配置此项的时候建议你进入主题的语言资源目录（一般在你主题根目录下的language文件夹下），查看支持的语言文件，再进行该项的配置。url &amp; root：这两项是息息相关的，但在本地站点部署阶段，这两个配置不需要进行配置，因为它们仅与线上站点有关，此处不详细讲解这两个配置，后面在讲解线上站点配置时，会进行说明。 主题_config.yml修改方法你下载的主题中，基本都会带有一个Readme文件，你需要详细阅读该文件，该文件中一般会注明主题的_config.yml文件如何配置，由于每个主题的配置方法，不尽相同，这里不再进行讲解。需要注意，部分主题可能会需要你根据需求特殊修改全局_config.yml文件，请根据Readme指示，进行操作。 发布本地站点在你配置完你的站点后，你需要在本地发布你的站点，发布的过程就是Hexo根据你的主题、资源自动生成静态页面，供你查看。发布本地站点你需要执行： hexo generate在官方文档中，对这个命令解释为生成静态文件，我在此处将其理解为发布本地站点，这并没有冲突。 启动本地服务这是为了你能在本地的浏览器中查看你的站点，这能方便的让你对站点进行调试、操作，而不会影响线上站点。 hexo server启动服务器。默认情况下，访问网址为： http://localhost:4000/。这时，你就可以通过这个网址，访问你的本地站点了。 线上环境部署当你完成了你本地站点的调试后，你就需要开始部署你的线上站点，毕竟，开通博客，最终是为了分享。 选择线上平台首先，你需要选择线上平台，目前主流有两种方式：GitHub部署和自建站点部署，本文也会对这两种主流方式的部署进行讲解。 GitHub部署GitHub提供了一个名为GitHub Pages的网页服务，它允许每个GitHub账户能够部署一个基于GitHub的站点服务，每个账户仅能部署一个站点，且所有站点源代码会被公开，当然，仅能提供基础的访问量服务，但好处在于完全免费，这一特性使得这个服务很适合部署个人博客站点。而本文也将带你一起，学习如何在GitHub上免费部署你的Hexo站点。需要注意，官方文档中，建议使用Travis CI进行GitHub部署，但实际上大可不必如此麻烦。 第一步：新建Repository你需要在你的GitHub上，新建一个Repository，在命名时，有两种选择方式，这两种方式取决于你喜欢哪种URL。 如果你喜欢形如：https://yourname.github.io，那么，你需要将这个Repo的名称命名为：&lt;your_github_name&gt;.github.io，请务必保证你的命名没有错误字符，否则站点无法部署成功。 如果你喜欢形如：https://yourname.github.io/xxx，那么，你可以对这个Repo任意命名，最后只需要在https://yourname.github.io后面加上你命名的名称即可。（官网中，将这个方法叫做Project page） 第二步：配置全局_config.yml文件如果你采用了上面的第一个Repo命名方式，那么，你需要确保以下配置： url: https://&lt;your_github_name&gt;.github.io/ root: /如果你采用了第二种Repo命名方式，那么，你需要确保以下配置： url: https://&lt;your_github_name&gt;.github.io/&lt;your_repo_name&gt; root: /&lt;your_repo_name&gt;接着，你需要修改以下配置项，从而使你能直接使用Hexo命令，将站点部署到GitHub上： deploy: type: git repo: &lt;your_repo_url&gt; branch: &lt;your_branch_name&gt;repo的链接地址你可以使用ssh形式或者https形式，在你repo页面中的左上角，绿色的clone or download按钮中可以查看地址。 第三步：上线站点在上线站点前，首先需要确保你的本地GitHub仓库已经建好。配置GitHub仓库： cd &lt;your_hexo_dir&gt; git init git remote add origin &lt;your_repo_url&gt;检查git省略文件，需要保证public不会被上传到GitHub： vi .gitignore上线站点： hexo deploy此时，如果之前的操作均没有问题的话，稍等几分钟，你就能访问你的线上站点了。 自建站点部署前提你需要拥有一个连接了互联网的服务器，具有独立的IP地址。可以租用一个云服务器。 本文使用了Nginx作为代理服务，如果你想使用其他工具，可以参考官方文档或者上网搜索。 安装Nginx点击这里，查看Nginx快速部署指南。需要注意，如果你给你的服务器配置了域名，你需要修改nginx.conf文件中的以下内容： server_name localhost; #域名将localhost修改为你的域名。 配置本地站点进入站点目录，打开全局_config.yml文件： vi _config.yml修改以下配置： url: 1.1.1.1 #填写你的IP地址或域名 root: / #填写网站根目录，如无特殊情况，默认 &quot;/&quot; 即可 permalink: :year/:month/:day/:title/ permalink_defaults: pretty_urls: trailing_index: false # Set to false to remove trailing index.html from permalinks生成静态文件： hexo generate上传静态网页文件拷贝public目录下的内容，上传到线上服务器的~/webserver/nginx/html目录下，浏览器输入访问地址，页面即可显示。 写作使用Nginx部署时，有几率出现一个小问题：中文命名的文章，由于Nginx可能不识别中文，使得含有中文的文件无法被Nginx解析，导致页面报404错误。解决方法：将文章文件名修改为英文即可，在你新建文章时，有两种方法实现这个效果： 在hexo new &quot;title_name&quot;时，将title_name命名为英文，然后打开文章文件编辑时，将文件抬头的title属性修改为你想命名的中文名； hexo new &quot;title_name&quot;时，正常命名，然后使用mv命令，将文件名修改为中文。 Hexo使用Hexo有很多高级操作，本文仅介绍基础的写作，如果对其他操作感兴趣，可以访问官方文档，其中有详实的操作指导. 写作hexo new [layout] &lt;title&gt;您可以在命令中指定文章的布局（layout），默认为 post，可以通过修改 _config.yml 中的 default_layout 参数来指定默认布局。 如果你没有修改配置的话，一般默认会新建一个名为title.md的文件在source/_post文件夹中，这时，你需要编辑这个文件，使你的文章能够上传到站点： vim &lt;title&gt;.md在你打开站点后，你可以看到抬头有三个配置项可以配置： title: Hexo部署指南 date: 2019-12-16 19:25:37 tags:一般只需要修改tags:即可，这个配置设置的是这篇文章的标签信息（如果你的主题中启用了标签服务的话）。 配置完标签后，你就可以将你的文章放在下方，保存后，发布到本地站点： hexo generate确认文章没有问题后，发布到线上站点： hexo deploy","categories":[{"name":"CS Soft","slug":"CS-Soft","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/"},{"name":"Client","slug":"CS-Soft/Client","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Client/"},{"name":"Hexo","slug":"CS-Soft/Client/Hexo","permalink":"https://winyter.github.io/MyBlog/categories/CS-Soft/Client/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://winyter.github.io/MyBlog/tags/Hexo/"},{"name":"Tools","slug":"Tools","permalink":"https://winyter.github.io/MyBlog/tags/Tools/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-12-11T19:09:49.000Z","updated":"2020-04-19T07:37:21.965Z","comments":true,"path":"2019/12/12/hello-world/","link":"","permalink":"https://winyter.github.io/MyBlog/2019/12/12/hello-world/","excerpt":"","text":"Hello World 这是我的博客，欢迎大家来潜水，我会在这里分享我的心得、心声、心爱","categories":[],"tags":[]}]}